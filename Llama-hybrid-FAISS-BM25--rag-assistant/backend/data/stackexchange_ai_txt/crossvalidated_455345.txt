[site]: crossvalidated
[post_id]: 455345
[parent_id]: 455334
[tags]: 
PCA is related to SVD , so your general question is answered in the Using principal component analysis (PCA) for feature selection thread. In the paper, the authors give following rationale for feature selection: Feature selection can facilitate data visualization/understanding and improve prediction performance by reducing redundant input dimension (Guyon and Elisseeff 2003). In our study, we used the combination of several methods (listed below) to select the useful features for prediction. Those may be valid arguments in general, but do not seem to be reasonable for their setting. First of all, going down from 14 to 8 features, for me, does not sound like a great improvement in terms of ease of interpretability and visualization, since 14 is already a small number of features. Second, they've used neural network, that is already a "black box" model that is harder to interpret, same with random forest where it is not feasible to draw all the trees in the forest and interpret them directly. In cases of both neural networks and random forests, there are indirect methods for measuring feature interpretability, that would work the same, and be equally interpretable, no matter if they used 8 or 14 features. Third, neither random forest, nor neural networks have problems with redundant features. Both of the algorithms can deal by itself with "selecting" the appropriate features. I'm not saying that feature selection is for no use with those algorithms, but that is unlikely the case if you have only 14 variables.
