[site]: crossvalidated
[post_id]: 521400
[parent_id]: 521385
[tags]: 
In general, there is no closed-form solution for a logistic regression that corresponds to the OLS estimator of regression coefficients, as you seem to wish. See this thread for discussion. From that perspective, there isn't an answer to your question. There is, however, a closed-form solution when all predictors are categorical . If the design matrix $X$ is furthermore coded in a particular way, with orthogonal binary variables, then there is a solution close to the form you seek. This presentation follows Lipovetsky , Journal of Applied Statistics, 42: 37-49 (2015). First, the design matrix $X$ needs to represent each combination of predictor categories as a distinct binary predictor.* Second, it's simplest to start by treating all such distinct binary predictors as having separate coefficients, omitting an intercept, then transforming back to the usual form. Each row of the design matrix then has a single non-zero entry of 1, in the column representing its particular combination of predictor values. In that form, $X'Y$ is a vector of counts, for each binary predictor $j$ , of cases having $Y=1$ , or $N_{j1}$ .** The matrix $X'X$ is diagonal, with element $jj$ being the total number of cases having binary predictor $j$ regardless of the value of $Y$ , or $(N_{j0} + N_{j1})$ . Then the elements of the vector $(X'X)^{-1}X'Y$ are the maximum-likelihood estimated probabilities of $Y=1$ for each binary predictor $j$ , or $$\hat p_j = \frac{N_{j1}}{N_{j0} + N_{j1}},$$ the fraction of cases having $Y=1$ when the predictor is $j$ . In this form, $\text{logit}\left((X'X)^{-1}X'Y\right)$ (done element-wise) gives coefficients $c_j$ representing the log-odds of $Y=1$ for each predictor $j$ . As Lipovetsky puts it, this: ... model without intercept presents the absolute impact of each category on the binary output, while a related model with intercept presents the relative impact of each category versus one taken as the reference. To transform to the latter standard representation, choose the last predictor combination $K$ as the reference, set the intercept $b_0 = c_K$ , and write the other logistic regression coefficients as $b_j = c_j - c_K$ . Lipovetsky then shows how to derive standard errors for coefficients and deviances. *I'm not sure if this will work if all possible combinations of predictor categories aren't present in the data set. ** Note that $\text{logit}(X'Y)$ as in your first suggestion wouldn't make sense, as those values are counts rather than probabilities.
