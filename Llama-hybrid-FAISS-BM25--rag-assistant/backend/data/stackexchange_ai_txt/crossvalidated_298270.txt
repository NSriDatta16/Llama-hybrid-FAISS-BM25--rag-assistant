[site]: crossvalidated
[post_id]: 298270
[parent_id]: 
[tags]: 
error propagation of oscillation removal of time series

I have a time series that that has the form: $$ F(t) = C + \sum_i A_i\sin(2\pi f_i t +p_i) $$ Where $C$ is a constant, $A_i,f_i,p_i$ are amplitude, frequency, and phase of the $i^{th}$ oscillation, and $t$ is time. I have performed spectral analysis to calculate the values of $(A_i,f_i,p_i)$ as well as their uncertainties $(\delta A_i,\delta f_i,\delta p_i)$. I now want to subtract off these sine wave oscillations from my time series. each $F_i$ value possesses its own $1\sigma$ uncertainty. My question is -- how do I perform error propagation of my subtracted time series at this stage? The only equation I know for error propagation is the basic one: $$ \delta F = \sqrt{\sum^N_j\left(\frac{\partial F}{\partial x_j}\delta x_j\right)^2} $$ where $x_j$ is the $j^{th}$ parameter with $N$ total parameters. The problem I am having with this method of error propagation is the partial derivative with respect to $f_i$. That error term looks like: $$ (A_i\cos(2\pi f_i t+p_i)*2\pi t\delta f)^2 $$ The equation yields a $t$ value outside of the cosine term, meaning that error will just grow and grow as I progress this calculation through the time series. I do not expect that to be an accurate representation of uncertainty in my subtracted time series. My only decent idea is to apply a modulus to the time value so that it follows the $0$ to $2\pi$ cyclicality of its parent oscillation. However, this still does not match my intuition of how errors propagate. Is there a general rule for dealing with time in error propagation? Thanks for you help. NOTE: These errors are extremely tiny and do not matter much. I really just need something acceptable, I do not need the most rigorous statistical analysis available.
