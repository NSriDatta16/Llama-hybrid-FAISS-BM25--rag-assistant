[site]: datascience
[post_id]: 64218
[parent_id]: 
[tags]: 
Why might trees work so much better than boosting classifiers?

I am predicting 10 classes label encoded using scikit-learn with 6 factors, 1.2M cases. DecisionTreeClassifier RandomForestClassifier ExtraTreesClassifier give accuracies (and precision and recall) of 0.9 AdaBoostClassifier GradientBoostingClassifier give accuracies of 0.2 Any pointers on the huge discrepancy? (I am doing gridsearchcv). Code: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) def output_metrics(): from sklearn.metrics import accuracy_score, precision_score, recall_score print("Accuracy:",accuracy_score(y_test, y_pred)) print('Precision', precision_score(y_test, y_pred, average=None).mean()) print('Recall', recall_score(y_test, y_pred, average=None).mean()) from sklearn.ensemble import AdaBoostClassifier from sklearn.model_selection import GridSearchCV tree_para = { 'n_estimators': [16, 32] } clf = GridSearchCV(AdaBoostClassifier(), tree_para, cv=5) model= clf.fit(X_train, y_train) y_pred = clf.predict(X_test) output_metrics()
