[site]: crossvalidated
[post_id]: 44650
[parent_id]: 
[tags]: 
A simpler way to calculate Exponentially Weighted Moving Average?

Proposed Method: Given a time series $x_i$, I want to compute a weighted moving average with an averaging window of $N$ points, where the weightings favour more recent values over older values. In choosing the weights, I am using the familiar fact that a geometric series converges to 1, i.e. $\sum (\frac{1}{2})^k$, provided infinitely many terms are taken. To get a discrete number of weights that sum to unity, I am simply taking the first $N$ terms of the geometric series $(\frac{1}{2})^k$, and then normalising by their sum. When $N=4$, for example, this gives the non-normalised weights 0.0625 0.1250 0.2500 0.5000 which, after normalising by their sum, gives 0.0667 0.1333 0.2667 0.5333 The moving average is then simply the sum of the product of the most recent 4 values against these normalised weights. This method generalises in the obvious way to moving windows of length $N$, and seems computationally easy as well. Question: Is there any reason not to use this simple way to calculate a weighted moving average using 'exponential weights'? I ask because the Wikipedia entry for EWMA seems more complicated. Which makes me wonder whether the textbook definition of EWMA perhaps has some statistical properties that the above simple definition does not? Or are they in fact equivalent?
