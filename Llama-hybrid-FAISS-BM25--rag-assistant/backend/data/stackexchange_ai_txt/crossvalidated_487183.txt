[site]: crossvalidated
[post_id]: 487183
[parent_id]: 487173
[tags]: 
To fit so much data, you have to use subsamples, for instance tensorflow you sub-sample at each step (using only one batch) and algorithmically speaking you only load one batch at a time in memory it is why it works. Most of the time this is done using a generator instead of the dataset right away. Your problem is that you always load the whole dataset in memory. To use sub-samples without loading the whole dataset with Random forest, I don't think it is doable using scikit-learn without re-coding part of the library. On the other hand, you can use xgboost and manually do the training part. Here is an example in classification, you can adapt the loss to get an example in regression. import numpy as np import xgboost as xgb from sklearn.datasets import make_blobs import pandas as pd # Construct dataset in 1D, dumped in a csv for illustration purpose X, y = make_blobs(centers= [[0,0], [1,2]],n_samples=10020) df = pd.DataFrame() df['feature1']=X[:,0] df['feature2']=X[:,1] df['label'] = y.ravel() features = ['feature1','feature2'] df.to_csv('big_dataset.csv') # Construct a generator from a csv file. Read chunck of 1000 lines gen_data = pd.read_csv('big_dataset.csv', chunksize=1000) class make_model(): def __init__(self,param,num_round=300): self.param=param self.num_round=num_round def fit(self,gen_data): iteration = 0 for df in gen_data: dtrain = xgb.DMatrix(np.array(df[features]), label=df['label']) if iteration ==0: model = xgb.Booster(self.param, [dtrain]) model = xgb.train(self.param,dtrain,num_boost_round=1, xgb_model=model) iteration += 1 self.model_=model def predict(self,X): dtest=xgb.DMatrix(X) return self.model_.predict(dtest)>0.5 # use argmax in non-binary classification parameters = {'max_depth':5, "booster":"gbtree"} # parameters to tune, see xgboost doc. Can be used to make boosted trees or Random Forests. model = make_model(parameters) model.fit(gen_data) xgb.plot_importance(model.model_)
