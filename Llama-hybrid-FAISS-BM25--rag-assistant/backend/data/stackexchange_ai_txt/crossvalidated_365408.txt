[site]: crossvalidated
[post_id]: 365408
[parent_id]: 365288
[tags]: 
Your analysis would be much better if it involved a common cross-validation or validation schema. Nevertheless, you are on the right track: Since random forests are typically quite strong (especially in large p small n situation) even without tuning, getting a bad out-of-bag score is often a sign that something is fishy about other methods with much better results (e.g. OLS with backward elimination and an R-squared of 90%). Let's demonstrate with a classic example based on completely random data. So everything above 0% R-squared (adjusted) would come from overfit. # Random data n So we can clearly see that (adjusted) R-squared of OLS followed by variable selection is super promising 68% (yeah, let's publish!). On the other hand, the out-of-bag R-squared of the random forest is close to 0%. Regarding the many comments below the OP: OOB scores are not as "valid" as cross-validation scores, but if the observations are independent and not too few trees are run, they are often sufficiently reliable. There are different defaults for "mtry" in a random forest in regression settings. One is $p/3$, another might be $\sqrt{p}$. In different real world situations, usually using values like this provides close to optimal cross-validation scores. The value chosen in the OP is somewhere in the middle, so it can't be too bad. Take-home message: Stepwise variable selection is a very naive and bad solution to a large p small n problem. In this example, the original OLS (before any variable selection) yields a negative adjusted R-squared, which is completely fine. The problem is the variable selection part, not the OLS part.
