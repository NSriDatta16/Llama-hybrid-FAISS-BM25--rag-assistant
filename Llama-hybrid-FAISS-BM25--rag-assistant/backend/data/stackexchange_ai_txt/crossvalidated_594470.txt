[site]: crossvalidated
[post_id]: 594470
[parent_id]: 
[tags]: 
Figure out the number of samples needed to increase the precision of an estimate

Suppose I am calculating the average rating for several products that were rated by different people. For example, assuming that if I choose 5 random people and calculate average ratings 100 times then the variance of each average is of course higher than if I chose 10 random people. I would like to figure out how many people I would need to obtain this average with as high precision as possible. Therefore, I believe that I can do a bootstrapping experiment where I increase the number of people whose ratings I have averaged and demonstrate that the variance decreases as I include more and more people when calculating the average(s). This would simply amount to choosing N ratings with replacement. Taking their average. Calculating the variance across bootstrap iteration. I'm then left of a plot of variance vs. number of samples. Does it make sense to use this plot to gain insight on how many samples I should use in order to increase the precision of my estimate? Importantly, I may only have 3 people give a rating but I want to infer that I potentially need 5 people. How would I go about inferring that? Is there any similar work and references associated with it that I can look at?
