[site]: crossvalidated
[post_id]: 275766
[parent_id]: 273101
[tags]: 
I try to answer my own question. Lets start with the selection of the model: Should I use an SVM, Neural Network (NN) or Random Forest (RF)? To answer this question, the following should be done: For each of the three different classifiers, define a parameter grid of the hyperparameters, which should be analyzed For each of the three classifiers, do a separate nested cross validation during which the hyperparameters are tuned The selection of hyperparameters in the inner loop will vary from fold to fold. Therefore this is not a way to select any hyperparameters Instead, use only the average score of the outer loops to get an unbiased estimate of the model under consideration (whereby model means here SVM, NN or RF) For each model (again: SVM, NN or RF) one of these values is retrieved from the nested CV. This value is then an unbiased metric to tell which model works best on the task. Select this model. Now a model is selected and I can tell whether I want to choose an SVM, NN and RF. Notice that the choice of hyperparameters was not made until now! Lets assume from now on that the Random Forest yielded the highest score so that we want to use it as our model. Which hyperparameters should I use for my model, (i.e. Random Forest)? To answer this, do the following: Use the grid of hyperparameters for the RandomForest defined before. Loop over the hyperparameters (or combinations of hyperparameters): for params in hyperparameters : Do k-fold cross validation (single, not nested!) during which we train the RF with params Measure then mean score, i.e. the average score over all k folds The hyperparameters with the highest mean score are the ones which should be chosen for the final model Now the hyperparameters of the Ranfom Forest are determined. We have two score values for the Random Forest: (i) the one from the nested cross validation where (possibly and almost certainly) different hyperparameters were considered in each inner fold and (ii) the score of the single cross validation during which we found our best hyperparameters for the RF. The first one only told us which model to choose whereas the second one tells us how well the model including fixed hyperparameters performs. A final model used for prediction of new unseen data would then be the Random Forest with the selected hyperparameters trained on all data. There is just one small thing left open: I would think that the second score value (the one from the single CV) is a proper measure of how well our final model (including hyper parameters) works on new data. Is this correct? And could one say that in general the nested cv score of this model is either a) higher b) lower or c) equal to the non nested score? A comment on this would be highly appreciated.
