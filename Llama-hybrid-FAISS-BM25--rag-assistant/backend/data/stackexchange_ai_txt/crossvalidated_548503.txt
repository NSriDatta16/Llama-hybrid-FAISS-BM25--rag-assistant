[site]: crossvalidated
[post_id]: 548503
[parent_id]: 541448
[tags]: 
The mask is typically a square matrix with a upper-right triangle of ones (or True in the pytorch implementation), and the rest is filled with zeros. 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 The ones will mask/put to zero the attention coefficients on those positions (read below). The shape of this matrix, a upper-right triangle, allows the transformer to avoid a look ahead bias (google this term together with time series), that is the current inputs can attend/pay attention to themselves and to past input data, but not to future input data. Otherwise the algorithm would be cheating. How does the mask work? Look at the last piece of code that you show in your question. That -1e9 is a very low number simulating minus infinity. Therefore, mask * -1e9 becomes a matrix with zeroes and minus infinity on what you want to mask. scaled_attention_logits becomes a matrix with a upper-right triangle of very low numbers. Thereafter the architecture includes softmax, which puts to zero all minus infinity, effectively masking the upper-right triangle. A doubt that I am having and that I am going to ask on a different post is how come the residual connection on the attention block does not cause any look ahead bias.
