[site]: crossvalidated
[post_id]: 266298
[parent_id]: 265859
[tags]: 
We never "accept the null hypothesis" (without also giving consideration to power and minimum relevant effect size). With a single hypothesis test, we pose a state of nature, $H_{0}$, and then answer some variation of the question "how unlikely are we to have observed the data underlying our test statistic, assuming $H_{0}$ (and our distributional assumption) is true?" We will then reject or fail to reject our $H_{0}$ based on a preferred Type I error rate, and draw a conclusion that is always about $H_{A}$â€¦ that is we found evidence to conclude $H_{A}$, or we did not find evidence to conclude $H_{A}$. We do not accept $H_{0}$ because we did not look for evidence for it. Absence of evidence (e.g., of a difference), is not the same thing as evidence of absence (e.g., of a difference). . This is true for one-sided tests, just as it is for two-sided tests: we only look for evidence in favor of $H_{A}$ and find it, or do not find it. If we only pose a single $H_{0}$ (without giving serious attention to both minimum relevant effect size, and statistical power), we are effectively making an a priori commitment to confirmation bias , because we have not looked for evidence for $H_{0}$, only evidence for $H_{A}$. Of course, we can (and, dare I say, should ) pose null hypotheses for and against a position ( relevance tests that combine tests for difference ($H_{0}^{+}$) with tests for equivalence ($H^{-}_{0}$) do just this). It seems to me that there is no reason why you cannot combine inference from a one-sided test for inferiority with a one-sided test for non-inferiority to provide evidence (or lack of evidence) in both directions simultaneously. Of course, if one is considering power and effect size, and one fails to reject $H_{0}$, but knows that there is (a) some minimum relevant effect size $\delta$, and (b) that their data are powerful enough to detect it for a given test, then one can interpret that as evidence of $H_{0}$.
