[site]: datascience
[post_id]: 73050
[parent_id]: 
[tags]: 
Implementing sklearn's FeatureHasher on Unseen Data

For a little bit of background I have been working on a binary classification of health insurance claims and am implementing sklearn's FeatureHasher to vectorize categorical features, many of which are particularly high in cardinality with a high count of unique factor levels and sklearn's FeatureHasher has been a useful tool to encode all of this information. I have trained an XGBoost classifier on my initial training set which was transformed such that each categorical feature was hashed into log(x) dimensions (x being the number of unique factor levels within in each feature). I am planning to use the trained model to make predictions on new/unseen claims data which will be scored on a daily basis. I will be hashing the daily extract of claims data with identical hashing dimensions used in the training set, such that there are no disparities between the number of features the model was trained upon and the number of features within the daily extract, allowing me to use the trained model to make predictions on incoming claims data. I am wondering how sklearn's FeatureHasher encodes unseen data. For example, if I had ten factor levels in a specific categorical feature within my original training set and only three factor levels within the same categorical feature in an incoming daily extract of claims data (likely to be a much lower volume of claims data as compared to the training set), does FeatureHasher encode this information in the same way such that each factor level can be identified and mapped back to its original value? Conversely, will hashing encode factor levels within categorical features differently between training and test sets if the number of factor levels within a given feature are not equal to what the model was trained upon? If so would this cause issues with how the model interprets the factor levels of a given categorical feature within a daily extract of claims causing the model to confuse or misinterpret factor levels from how they were originally encoded in the training set? Any feedback would be greatly appreciated! P.S. I have looked into using CatBoost gradient boosting (catboost.ai/docs/concepts/python-installation.html) as an alternative but unfortunately am using a platform that does not support this and thus will not be an option for me
