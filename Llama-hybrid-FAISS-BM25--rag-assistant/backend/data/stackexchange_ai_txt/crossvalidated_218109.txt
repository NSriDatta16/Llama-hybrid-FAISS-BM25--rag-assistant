[site]: crossvalidated
[post_id]: 218109
[parent_id]: 217762
[tags]: 
As @Heisenberg pointed out in the comments, before doing any model training/evaluation/selection, you should create a hold-out test set, which you in the end use once to test your single, final, ready-trained model. You could do this e.g. as follows: Create a training and hold-out test partition. Using the training partition, do repeated cross validation for training and evaluating different model types, model parametrizations, and combinations of features $^1$: This gives you multiple performance measures for each model type, model parametrization, and combination of features, from which you can derive how well exactly this configuration is suited for your task, and what its performance spread might be. Chose a model type, parametrization, and combination of features from those results, then train this model using the complete training partition. Test the obtained model on the test partition once to assure its performance is correct. $^1$ The deletion of features you explained seems to be close to backwards feature selection, e.g. recursive feature elimination : this starts with all features and removes features until a peak is reached in the average repeated cross validation performance. This means it re-evaluates the model each time. The big difference to what you mentioned is that this does not involve the test data at all: if you overfit, you do it just on the training data and will at last notice on the test data. As you are using R : the caret package has all this ready-made in their ?rfe implementation, so its easy to use, just takes a while to evaluate all the options. PS: in your example, it could theoretically be that you initially overfitted using all features, then by removing some of them decreased variance at the cost of (some little) bias. But much more likely you just overfitted your test data as well, as you used it like training data in an optimization process. UPDATE If you did not use any test data in training and only used test data to ensure that your final model actually has a reasonable performance, using such features, which are labeled "insignificant" by your feature importance metrics, is legit. This could e.g. be the case with some "weak" features, which only in their combination will solve a problem well - for example, a binary classification over two features $A$ and $B$, that are organized as as $AND$ or $OR$: B 0 1 B 0 1 A +---- A +---- 0 | 0 0 0 | 0 1 1 | 0 1 1 | 1 1 Each of those features might end up being labeled insignificant, but their combination enables the problem to be solved - so using them in your model is reasonable.
