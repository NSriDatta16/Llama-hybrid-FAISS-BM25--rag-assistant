[site]: crossvalidated
[post_id]: 589304
[parent_id]: 
[tags]: 
Neural networks - what does learning rate exactly mean and is it applied over batches or epochs?

I'm playing with different learning rates and batch sizes for a neural network. What I want to understand is a more technical definition of what learning rate is. I understand that it is, in some way, a "step size" for the adjustments made to the network parameters based on the data it sees. However, I'd like to understand to a bit more depth. If the learning rate is 0.001, what exactly does that mean? And is this value "applied" over batches, or epochs? Thank you!
