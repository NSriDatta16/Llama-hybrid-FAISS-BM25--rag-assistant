[site]: crossvalidated
[post_id]: 468361
[parent_id]: 
[tags]: 
Bellmans equation and existence of optimal policy for MDPs

I'm trying to understand the proof of existence of an optimal policy from this question Why is there always at least one policy that is better than or equal to all other policies? by Lovelris. - First, here's my attempt at a proof, which is shorter than the linked question, and so perhaps there's something wrong, or it proves something different, or I might be using a wrong definition at some point in my proof. I think it explains the first step in the linked proof in more detail, but I don't really see why the later steps are needed. I would appreciate if someone checked my proof, and maybe compared it to the linked one. Suppose we have a discounted MDP, and assume for simplicity that we have a finite amount of states, actions, and rewards, that are only dependent on the state. We would like to show that there exists an optimal decision (stationary) policy $\pi^*$ , i.e. it is true that $V^{\pi^*}(s) = V^*(s)$ for all states $s \in S$ , where $V^*(s) = \max_{\pi} V^{\pi}(s)$ . Here $\max_{\pi} V^{\pi}(s)$ is clearly well defined and exists (though the $\pi$ can be different for different states). Consider any $s \in S$ . Then using Bellman's equation for $V^{\pi}(s)$ we have \begin{align*} V^* (s) &= \max_{\pi} V^{\pi}(s)\\ &= max_{a \in A} \: max_{\pi, \pi(s) = a} [r(s) + \gamma \sum_{s'} P_{s,a}(s') V^{\pi}(s')]\\ &\leq max_{a \in A} [r(s) + \gamma \sum_{s'} P_{s,a}(s') V^{*}(s')]\\ \end{align*} Define $\tilde \pi$ as the policy that maximizes the above inequality, i.e. $$ \tilde \pi (s) = \arg \max_a [r(s) + \gamma \sum_{s'} P_{s,a}(s') V^{*}(s')]$$ Then we have $$ V^* \leq [r(s) + \gamma \sum_{s'} P_{s,\tilde \pi (s)}(s') V^{*}(s')]$$ Denoting the vector $V^*$ for $(V^*(s_1),...,V^*(s_n))$ , and similarly for $V^{\tilde \pi}$ and $r$ , and denoting the matrix corresponding to $\gamma P_{s,\tilde \pi(s)}$ by $B$ , we have that $$ V^* \leq r + BV^* \; i.e. (I-B)V^* \leq r$$ $$ V^{\tilde \pi} = r + BV^{\tilde \pi} \; i.e. (I-B)V^{\tilde \pi} = r$$ and so $(I-B)(V^{\tilde \pi} - V^*) \geq 0$ . Because $(I-B)^{-1}$ exists and $u \geq 0$ implies $(I-B)^{-1} (u) \geq u \geq 0$ ( intuitively $(I-B)^{-1}(u)$ is really just the expected payoff given policy $\tilde \pi$ and reward vector $u$ , so clearly if $u \geq 0$ , we have that the expected payoff is non negative), and so $(V^{\tilde \pi} - V^*) \geq 0$ , which shows that $V^{\tilde \pi} = V^*$ - The basic idea is quite simple and is maybe easier to see in the case of $V^\pi(s) . Here denote $\pi' = \pi$ for all states except $s$ , for which $\pi'(s) = a$ . Symbolically we then have $v = r + Av and $v' = r + Bv'$ , where $A$ is the matrix corresponding to the policy of $\pi$ and $B$ to that of $\pi'$ , and $v'$ is just the solution to the equation. In this scenario $B$ is $A$ with one row exchanged. Note - here $v means that $v \leq r + Bv$ in all components, and $v in at least in component. So again as above, we have that $(I-B)(v' - v) > 0$ , and so $v' > v$ , meaning that if $V^\pi(s) , then there exists a policy $\pi'$ such that $V^{\pi'}(s) > V^{\pi}(s)$ . By an argument of similar type, if we now observe how the other components of $V^{\pi'}$ look, I think we can also show that $V^{\pi'}(s') \geq V^{\pi}(s')$ for all the other states too (we can "ignore" the first column of $A$ because we're assuming the $V^{\pi'}(s_1)$ is fixed). So if $V^\pi(s) , then changing the action in this state, will only improve the resulting values. But I'm not too sure about this.
