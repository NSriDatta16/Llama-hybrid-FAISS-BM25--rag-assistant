[site]: crossvalidated
[post_id]: 136674
[parent_id]: 
[tags]: 
transform of predictor variables

Assume I have a linear model like this: $$ y_i = \beta_1 x_{i1} + \cdots + \beta_{ip} x_{ip} + \varepsilon_i \hspace{1cm} i = 1,\dots,n $$ I know that if $y_i$ must be greater than zero, I should fit against $\log(y_i)$ rather than $y_i$. If $y_i$ must be between $[0,1]$ (like the case in logistic regression), I should use a logistic link function. On the other hand, from my experience, I found that if $x_{ij}$ must be greater than zero. Usually I will get a better fit if I first log-transform it, i.e. I use $\log(x_{ij})$ rather than $x_{ij}$. Are there any theory for this experience? Moreover, Let's say: (1) If I know $x_{ij}$ is an indicator variable (must be either $0$ or $1$). Shall I use an inverse of logistic function to transform it first? (2) How about if I know $x_{ij}$ is on the interval $[a,b]$?
