[site]: crossvalidated
[post_id]: 524239
[parent_id]: 524236
[tags]: 
Only in response to: I'm looking for a reference that talks about the relationship between the variance of the gradient and convergence of SGD. Whilst the Robbins-Monro stochastic approximation paper is the seminal reference, its scope is broader than that of stochastic gradient descent. There are more contemporary papers that cover stochastic approximation specifically in context of stochastic gradient descent. For some theoretical results concerning the gradient variance and convergence, an appropriate starting point might be: Bottou, L., Curtis, F., Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. SIAM Review Vol 60 Issue number 2 pp 223-311. DOI:10.1137/16M1080173 The bibliography of that paper and other papers by Leon Bottou and associates may also yield fruit.
