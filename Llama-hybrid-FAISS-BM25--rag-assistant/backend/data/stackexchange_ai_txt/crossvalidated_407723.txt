[site]: crossvalidated
[post_id]: 407723
[parent_id]: 407638
[tags]: 
Keep in mind the goal of regression through this simple example: Let's say you want to determine the relationship between the price of homes in the state you live in and the size of the home. And you are clever, so you also include a control variable which indicates whether the home is in a rural or urban district of the state. To study this problem, you randomly pick 311 homes in your state and record those three measurements for each. Now the question that regression is trying to answer is as follows: given the relationships between the variables observed (which we obtain through the coef. column of the output), are these relationships likely to be simply a coincidence of the particular 311 homes we randomly picked, or is it likely that regardless of which homes ended up in the sample, you would still be likely to observe the same relationships. (slight tangent on what we mean by significance) How do we define what "likely" means in this context? This is where the p > |t| column comes in. In general, you will often see a significance level of 5% being used. That is, you want your p > |t| column to be less than .05 and if it is you call the variable "statistically significant". The exact reason for this starts to get technical, but the general idea is as follows: suppose there is no relationship between the things we are studying. If we were to repeatedly take random samples and perform the regression, how often would the regression indicate there was a significant relationship between the things? The answer is around 5% of the time. OK, back to the concept of control variables. In this scenario, the reason for including a control variable is rather obvious-- real estate prices are highly influenced by location so by including a location control variable you will greatly increase your understanding of the relationship between price and size. In fact, by not including a control variable, it is possible that you completely miss the true relationship between your variables of interest. Consider this situation: Let's say you have a population of interest and you want to know the relationship between Y and X. In this population, people are 50-50 split between membership in some group (this group membership is our control variable). Suppose that if people belong to the group, the coefficient between Y and X is a large positive number and if not it is large negative number. If we ran a regression without including this group membership we would find that our regression would tell us there is no relationship between Y and X! The large positive association that exists for people in the group would be completely negated by the large negative association between Y and X that people not in the group have. But it can get even worse! Look at https://en.wikipedia.org/wiki/Simpson%27s_paradox . Not only can relationships be negated by not including the correct control variables, doing so may give you relationships that are the opposite of what they are in reality. You think Y and X are positively correlated, but in reality after proper controls they are negatively correlated. This is why we sometimes see 2 studies on the same subject that come to opposite conclusions! Now, back to your questions. How can I explain the relationship between the controlling variables and my other significant independent variables? What do the insignificant controlling variables say about the significant independent variables? Given your results, it would appear that location (I am assuming this is what the categorical variable BO represents) has no impact on the relationship between your dependent and independent variables. Essentially, because the p > |t| is so large, it is highly likely that another random sample of data on these variables would produce entirely different coefficients for Oppland Oslo and so on. Which is to say, it would not at all be unusual to see that another sample produced a slightly positive coefficient for Oppland instead of a slightly negative one. Because the p > |t| values are so large for this variable it should almost certainly be excluded and instead you should just note that you attempted to control for location but found no relationship. Now AB is a different story because one level is highly significant while the others are not. I would guess the improvement you see (according to r^2 and adj r^2) in the bigger model is almost entirely because of the >= 10 ganger level of AB. One thing you could do here is instead turn the variable into a binary variable and see what happens. But since this is an ordinal categorical variable (there is a natural way to order the levels) sometimes slightly non-significant levels are kept if the coefficients "make sense". To specifically answer your question: holding everything else constant we would expect an observation with 10+ ganger to have a higher LOJ than one with And how do I know if my significant independent variables are "good" or not, when all the controlling variables are insignificant? It could just be the case that X and Y have a highly causative relationship and that not many other things influence the relationship between them very much. Or you just picked bad control variables. It really doesn't mean much on its own. It can be a cause for concern, though. Like if you are doing the house price study and it comes out that location isn't significant I would be very concerned about the data quality. This question really gets down to the heart of the issues with regression. Why do we see studies that say x causes cancer, and then another that says the exact opposite? Because the whole thing only works when we have included all variables relevant to the problem. In nutrition studies, as an example, this is incredibly difficult. Because in reality we can never know for certain that we haven't missed an incredibly influential lurking variable that, if included, would change all the results.
