[site]: datascience
[post_id]: 58824
[parent_id]: 58822
[tags]: 
from sklearn.metrics import f1_score y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] f1_score(y_true, y_pred, average='weighted') From documentation Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html
