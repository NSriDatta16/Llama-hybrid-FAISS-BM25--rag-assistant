[site]: crossvalidated
[post_id]: 615290
[parent_id]: 
[tags]: 
If feature importance is only computed based on training set, does it mean one should never compute shap values on test set?

If feature importance is only calculated from the training set according to here , does it mean one should never compute shap values on test set? What would it mean if I compute shap values from test set ? For instance, if i have the following code, clf_opt is a random forest estimator and the code runs without any error. What do the shap values of X_test represent in this case? (Note that the validation set (not test set) is part of X_train which goes into K fold cross validation) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) ...# perform K fold cross validation explainer = shap.TreeExplainer(clf_opt,X_train) # training set is background data shap_values = explainer.shap_values(X_test) # test set is foreground data I am trying to understand what is the right way to compute shap values for random forest. Most online examples just fit random forest on the entire dataset X without train_test_split and compute shap values on the X without the use of background. I don't know what is the right way to look at shapley values. Should I copmute shapley values only on training set never on test set? explainer = shap.TreeExplainer(clf_opt,X_train) shap_values = explainer.shap_values(X_train) OR (without providing background data) explainer = shap.TreeExplainer(clf_opt) shap_values = explainer.shap_values(X_train) OR explainer = shap.TreeExplainer(clf_opt, X_train) shap_values = explainer.shap_values(X_test) OR explainer = shap.TreeExplainer(clf_opt) shap_values = explainer.shap_values(X_test) I am very lost. Would really appreciate some guidance here.
