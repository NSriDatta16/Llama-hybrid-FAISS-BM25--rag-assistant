[site]: crossvalidated
[post_id]: 624232
[parent_id]: 
[tags]: 
Should we (under- or over-) sample when training a ML model, if we care about edge cases?

I know this question has been somehow reiterated in multiple ways, but I have not yet found an answer that would explicitly apply to my case. I wish to train a classification model to predict who is most likely to perform action X. Morever, I wish to use the output of the classifier (not 0 and 1, but the number between 0 and 1) the model outputs) to rank the customers from the most to the least likely, thus not caring about thresholds etc. However, I have a very small amount of positive samples - depending on the specifics, sometimes 0.1% of population, sometimes 1%. On one hand, I read that on Stack and similar forums resampling in general might not even make sense, on the other hand I am thinking whether without any resampling the model will, figuratively speaking, pay enough attention to those edge cases and what type of customers are in the "positive" group. What do you as the more statistically-inclined people think about the topic? EDIT: As the original question might've been imprecise, let me give an example. Right now, 1% of people bought product X. We want to market product X to our customer base, but instead of doing a random/semi-random send, we want to target people who the model predicts are most likely to buy it.
