[site]: crossvalidated
[post_id]: 597831
[parent_id]: 587148
[tags]: 
You have answered your own question here (note my emphasis): When the token is replaced with , it makes sense to me that the model learns how to use the surrounding tokens to create a contextual representation of what the token is likely to be. In other words, the contextual representation of a masked token is a function not only of its embedding but also the embeddings of the other tokens in the input, masked or otherwise. Thus, although the loss is only computed for the masked tokens, its gradient flows to all the token embeddings, i.e. they're updated during training. So, the model is "incentivized" to also produce good embeddings for the unmasked tokens.
