[site]: crossvalidated
[post_id]: 470632
[parent_id]: 470619
[tags]: 
Yes, the same holds true. Note that omitted variable bias is an epistemic, not a technical problem . It means you lack a meaningful interpretation of results from a statistical model, e.g. coefficients, because the selection of variables you allowed the model to use was misleading. It has nothing to do with the technical side of your model, e.g., which estimator you use, which link functions you use (logit, in your case), which fitting algorithm etc. Coefficients will almost always change in some way when you add or remove variables from your model. Whether this is a bias really depends on the question you are out to answer and your expert knowledge on the topic. This links to the broader problem of confounding and the assumptions you need to make to interpret your result from a causal perspective. If that's your aim, leaving out important confounding variables from your model can totally mess up your conclusions, as in typical 'spurious associations': https://www.tylervigen.com/spurious-correlations But note that you can also use too many variables , from an espistemic perspective (besides the technical and estimation issues that arise in crowded models), for example when conditioning on a collider ! A good introduction into these problems is chapter 5+6 in: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. Besides such theoretical approaches for the inclusion of variables into models, there are also data-driven approaches, which can be useful depending on what your goal is ([causal / statistical] inference? prediction?), a really good primer is: Heinze, G., & Dunkler, D. (2017). Five myths about variable selection. Transplant International, 30(1), 6-10.
