[site]: crossvalidated
[post_id]: 576579
[parent_id]: 576568
[tags]: 
Generally, if your objective is to maximize predictive performance, you should choose the hyperparameters that maximize your expected predictive performance (eg. by choosing the parameters which lead to maximal performance on holdout data). It's not common to set hyperparameters by hand unless you have some specific goal in mind. If you see significantly better performance with min_data_in_leaf:30 than min_data_in_leaf:100 , go with 30, all other things being equal. No need to overthink it. To your thoughts... 1. I should stick to tuning other parameters that control overfitting and keep min_data_in_leaf fairly low Never really a bad idea to experiment with more hyperparameters. And in this case, if you know that certain quirks of your dataset will lead to a low value of min_data_in_leaf performing well, it might make extra sense to experiment with other types of regularization. The LightGBM Parameters Tuning page lists around eight other hyperparameters that can reduce overfitting. But again, this should be done in the context of maximizing performance on holdout data. If more regularization leads to a worse predictive model, why are you adding it? 2. Maybe I need to do more feature engineering? (I've tried, unsuccessfully) That would not solve the problem with min_data_in_leaf . If you only have 40 observations where date='Dec 30' you're only going to have 40 observations where is_dec_30=True .
