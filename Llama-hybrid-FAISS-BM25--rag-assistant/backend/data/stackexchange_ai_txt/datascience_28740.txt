[site]: datascience
[post_id]: 28740
[parent_id]: 
[tags]: 
Do we need to increase training data size when increasing dropouts?

I am using a fully connected feed forward neural network built using keras for text classification. It consists of 3 hidden layer. I am planning to add a dropout layer after each hidden layer to prevent overfitting. While tuning the dropout rate, I am increasing the value from 0.2 -> 0.3 -> 0.4 -> 0.5. I want to know if I should increase the training data size to have a more accurate comparison. What I mean is suppose I am having training data of size 1 million for a dropout rate 0.2. Should I increase the training data size to 1.5 million for dropout rate 0.3? Calculation: 1000000 * 0.8 * 0.8 * 0.8 = 512000 (for dropout ratio 0.2) 1500000 * 0.7 * 0.7 * 0.7 = 514500 (for dropout ratio 0.3)
