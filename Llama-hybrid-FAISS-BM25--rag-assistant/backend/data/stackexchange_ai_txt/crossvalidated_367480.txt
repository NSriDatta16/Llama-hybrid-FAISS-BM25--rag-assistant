[site]: crossvalidated
[post_id]: 367480
[parent_id]: 367450
[tags]: 
With such a large dataset it can make sense to set aside a separate testing set, but your proposed one-trait-at-a-time approach doesn't take advantage of the scale of your data and might risk missing some important information about how the traits are related together to outcome. With 100,000 cases having 10% deaths, you have 10,000 events to use in your analysis. The usual rule of thumb for evaluating predictors without overfitting a survival model is to allow at least 10-20 events per predictor. So you could in principle consider evaluating up to 500 predictors. Just evaluating one predictor at a time is not a good approach for any type of regression model, as it ignores potential combined contributions to outcome. See this page for discussion in the context of linear regression with continuous variables. The issues, however, are similar with categorical predictors. The regression coefficients in a multiple regression model including all predictors of interest then specify the relation of each predictor to outcome when all the other predictors are held constant , which is typically of more interest than single-predictor relations to outcome. A single-predictor relation to outcome can depend more on the distribution of its values among cases than on any intrinsic relation to outcome. Besides covariates like age and sex, you have only 10 traits/predictors that you wish to examine. At the least you should use a multiple regression model that includes all of the traits and covariates at once. To look at how the 10 traits work together in more detail, you could consider evaluating all their 2-way interactions, of which there are only 45. This would show whether the effect of each single trait on outcome depends on the status of each of the other traits. A multiple regression model including all at once the other covariates, the 10 traits, and all the 2-way interactions among traits, and two-way interactions of the traits with covariates would only mean about 60 predictors.* With respect to splitting the data, the danger is that limiting the number of cases used to build the model might cut down on the quality of the model, as it throws away information from the other cases. I don't have practical experience with such large data sets, but a 50/50 split as you propose seems like too large a test set. My sense is that using 80% for building the model and 20% for testing the model might give a better model with adequate testing. Random sampling would be a wise choice, as it avoids any bias that might be associated with how the data were collected and formatted. If you know that there are some traits (or combinations of traits, if you choose to examine interactions) that occur infrequently, you might want to ensure that there is even representation between training and test sets by setting aside such cases and doing the random 80/20 (or other) split on such cases separately from the rest of the data. However you split the data, though, avoid the one-trait-at-a-time approach and use a multiple regression instead. *If you do choose to evaluate the large numbers of interactions among the binary traits, there is a danger that you might face a situation similar to the perfect separation seen in logistic regression. In that case you could use a penalized regression method like ridge regression, as noted on the page linked in this footnote.
