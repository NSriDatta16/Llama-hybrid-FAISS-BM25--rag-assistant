[site]: datascience
[post_id]: 92362
[parent_id]: 18414
[tags]: 
Based upon Andrew Ng's Deep Learning Specialisation Course 2, here're a few things to be kept in mind: Use mini-batch gradient descent if you have a large training set. Else for a small training set, use batch gradient descent. Mini-batch sizes are often chosen as a power of 2, i.e., 16,32,64,128,256 etc. Now, while choosing a proper size for mini-batch gradient descent, make sure that the minibatch fits in the CPU/GPU. 32 is generally a good choice To know more, you can read this: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size
