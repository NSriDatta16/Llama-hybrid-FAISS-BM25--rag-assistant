[site]: stackoverflow
[post_id]: 4333674
[parent_id]: 4333094
[tags]: 
I think that R's data pre-processing capability--i.e., everything from extracting data from its source and just before the analytics steps--has improved substantially in the past three years (the length of time i have been using R). I use python daily and have for the past seven years or so--its text-processing capabilities are superb--and still i wouldn't hesitate for a moment to use R for the type of task you mention. A couple of provisos though. First, i would suggest looking very closely at a couple of the external packages for the set of tasks in your Q--in particular, hash (python-like key-value data structure), and stringr (consists mostly of wrappers over the less user-friendly string manipulation functions in the the base library) Both stringr and hash are available on CRAN . > library(hash) > dx = hash(k1=453, k2=67, k3=913) > dx$k1 [1] 453 > dx = hash(keys=letters[1:5], values=1:5) > dx containing 5 key-value pair(s). a : 1 b : 2 c : 3 d : 4 e : 5 > dx[a] containing 1 key-value pair(s). a : 1 > library(stringr) > astring = 'onetwothree456seveneight' > ptn = '[0-9]{3,}' > a = str_extract_all(astring, ptn) > a [[1]] [2] "456" It seems also that there is a large subset of R users for whom text processing and text analytics comprise a significant portion of their day-to-day work--as evidenced by CRAN's Natural Language Processing Task View (one of about 20 such informal domain-oriented Package collections). Within that Task View is the package tm , a package dedicated to functions for text mining. Included in tm are optimized functions for processing tasks such as the one mentioned in your Q. In addition, R has an excellent selection of packages for working interactively on reasonably large datasets (e.g., > 1 GB) often without the need to set up a parallel processing infrastructure (but which can certainly exploit a cluster if it's available). The most impressive of these in my opinion are the set of packages under the rubric " The Bigmemory Project " (CRAN) by Michael Kane and John Emerson at Yale; this Project subsumes bigmemory, biganalytics, synchronicity, bigtabulate, and bigalgebra. In sum, the techniques behind these Packages include: (i) allocating the data to shared memory, which enables coordination of shared access by separate concurrent processes to a single copy of the data; (ii) file-backed data structures (which i believe, but i am not certain, is synonymous with a memory-mapped file structure, and which works enabling very fast access from disk using pointers thus avoiding the RAM limit on available file size). Still, quite a few functions and data structures in R's standard library make it easier to work interactively with data approaching ordinary RAM limits. For instance, .RData, a native binary format, is about as simple as possible to use (the commands are save and load ) and it has excellent compression: > library(ElemStatLearn) > data(spam) > format(object.size(spam), big.mark=',') [1] "2,344,384" # a 2.34 MB data file > save(spam, file='test.RData') This file, 'test.RData' is only 176 KB , greater than 10-fold compression.
