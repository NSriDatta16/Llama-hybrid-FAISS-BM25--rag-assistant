[site]: crossvalidated
[post_id]: 561533
[parent_id]: 561460
[tags]: 
A popular post about 'stacking' can be found here: Ensemble Learning: Why is Model Stacking Effective? . The author said that 'stacking' can easily improve the performance with respect to the individual base learners. But I found it not that easy, at least in my project. Many posts said the key for stacking to work consists in many different base learners are used. From my understanding, it is like the Fourier series in which a complex function can be approximated by a series of triangular base functions. It required that the base triangular fucntions are different in frequency, in other words, they represent different characteristics of the objective functions. In model stacking, we also required the base learners to be different that can capture different features of the underlying complex functions. Actually, I calculated the simple Pearson correlation of the two methods A and B in the question, and found the correlation coefficient to be only 0.2. I am not sure whether it indicates being different enough for stacking. If they are not that different, or they both underestimate or overestiamte the result on average, then the stacking can never improve the performance whatever meta-learner was used?
