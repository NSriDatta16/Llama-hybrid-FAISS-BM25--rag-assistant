[site]: datascience
[post_id]: 104084
[parent_id]: 104081
[tags]: 
XGBoost, LightGBM, and CatBoost are almost always better than Random Forest in accuracy and they are similar to Random Forest. LightGBM is also much faster to calculate. They all work best if the time series features are added to the data. Lags, moving averages, standard deviations of different number of periods help improve accuracy significantly. Just look at M5 Forecasting Accuracy Wallmart Kaggle competition ( https://www.kaggle.com/c/m5-forecasting-accuracy ). The first 5 places are 4 LightGBM models and 1 DeepAR model. 1st place solution: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163684 All places: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163414 The first place and, if I remember correctly, all other first places with LightGBM models, had: lags moving averages moving standard deviations features based on price About 10 years ago the winning models were Random Forest for classification and different time series algorithms like Exponential Smoothing for time series. But now not any one of the winning solution uses Random Forest. And the boosting algorithms gain significantly from feature engineering. In my opinion this kind of feature engineering was a new development several years ago that allowed the tree algorithms to beat simple and even more complicated usual time series algorithms when you have additional external data (like prices). This should also apply to Random Forest. I am working full-time as a data scientist at a consulting company making demand forecasting to improve supply chain processes. I was involved with about 6 clients to create demand forecasts and I was once competing with a competitor for the best accuracy and won.
