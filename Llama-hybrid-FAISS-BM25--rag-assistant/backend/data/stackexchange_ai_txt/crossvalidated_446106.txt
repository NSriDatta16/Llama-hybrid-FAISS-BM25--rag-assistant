[site]: crossvalidated
[post_id]: 446106
[parent_id]: 446098
[tags]: 
You have data $X \sim \mathcal{B}(\theta)$ , and you observe $(X_1,\dots,X_n)$ , $X_o = \sum X_i$ , which means that for $k\in \{0,\dots,n\}$ , $$ \mathbb P(X_o = k \mid \theta) = \binom{n}{k} \theta^k(1-\theta)^{n-k} $$ From a bayesian perspective you also have a prior distribution on $\theta$ which is a Beta distribution $p(\alpha,\beta)$ . Then you get the posterior distribution of $\theta$ , \begin{align*} p(\theta \mid X_o) &\propto p(\alpha,\beta) \mathbb P(X_o \mid \theta) \\ &\propto p(\alpha,\beta) \theta^{X_o} (1-\theta)^{n-X_o}, \end{align*} and you can derive the maximum a posteriori, $$ \hat \theta= \arg\max_{\theta} p( \theta \mid X_o) $$ Now if you want to compute the probability of getting 1 head out of 5 (new and independent) coin tosses, given that the probability of head is $\hat \theta$ , you can simply put $\hat \theta$ in the likelihood $\mathbb P(X \mid \theta)$ : $$ \mathbb P(X=1 \mid \hat \theta ) = \binom{5}{1} \hat \theta(1-\hat \theta)^4 $$ But from a Bayesian point of view it may be more relevant to consider all possible values of $\theta$ , and not just a point estimate like $\hat \theta$ , and rather compute $$ \mathbb P(X= 1 \mid X_o) = \int_\Theta P(X = 1 \mid \theta) p(\theta \mid X_o)d\theta $$
