[site]: datascience
[post_id]: 78487
[parent_id]: 78450
[tags]: 
We calculate the Loss only once per batch(average for each data point) and only at the output layer not at every Neuron . $\hspace{5cm}$ Loss = Loss_fn(y_true, y_pred) In other words, this Loss_fn is a function of all the weight/bias and we want to know the Gradient w.r.t each weight/bias. We can't get this directly for each parameter as each layer is dependent on the previous layer. Output is something similar to - f( g( h( k(x) ) ) ) This is where you apply Chain rule to get the Gradient till the first layer in a backward fashion. $\hspace{2cm}$ $\hspace{3cm}$ [Image Credit - https://leonardoaraujosantos.gitbook.io/ ] For the last layer, We can get the Gradient directly as for this layer we have got both the output error and input( output of the last layer from forward pass )
