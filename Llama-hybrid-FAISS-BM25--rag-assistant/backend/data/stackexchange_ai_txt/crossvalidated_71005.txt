[site]: crossvalidated
[post_id]: 71005
[parent_id]: 
[tags]: 
$R^2$ from a regression of two trend-stationary processes, $Y_t$ and $X_t$

In Estimation and Inference in Econometrics, by Davidson and MacKinnon, p.671, they claim that $R^2$ from a regression of $Y_t$ on $X_t$, where both time series are trend stationary, tends to 1 as $n$ tends to infinity. Can anyone please provide a mathematical answer to this? The models for $Y_t$ and $X_t$ are: $Y_t = \delta_0 + \delta_1t + u_t$ and $X_t = \gamma_0 + \gamma_1t + v_t$, where $u_t$ and $v_t$ are stationary.
