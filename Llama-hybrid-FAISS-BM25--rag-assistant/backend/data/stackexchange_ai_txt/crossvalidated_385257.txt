[site]: crossvalidated
[post_id]: 385257
[parent_id]: 
[tags]: 
Why the bias distribution in the last layer is always close to symmetric?

I am trying to train a very simple model, the first layer is full connection, while the output layer output 2 values to represent different categories. def inference(self, features_pl): tensor_dict = {} # Define a scope for reusing the variables with tf.variable_scope("cnn"): layer1_output = tf.layers.dense(features_pl, 512, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(mean=0.01, stddev=0.001), bias_initializer=tf.zeros_initializer(), name="layer1") logits = tf.layers.dense(layer1_output, 2, activation=None, kernel_initializer=tf.truncated_normal_initializer(mean=0.01, stddev=0.001), bias_initializer=tf.zeros_initializer(), name="output_layer") I found the output layer's value (kernel + bias) are always looks symmetric. I had printed the weight in the model, the sum of kernel and bias in very close to the initial value while we can see the distribution changed a lot in the tensorboard. Did anyone here has the similar experience about this issue?
