[site]: datascience
[post_id]: 9068
[parent_id]: 869
[tags]: 
Both the answers from @Emre and @Madison May make good points about the issue at hand. The problem is one of representing your string as a feature vector for input to the NN. First, the problem depends on the size of the string you want to process. Long strings containing may tokens (usually words) are often called documents in this setting. There are separate methods for dealing with individual tokens/words. There are a number of ways to represent documents. Many of them make the bag-of-words assumption. The simplest types represent the document as a vector of the counts of words, or term frequency (tf). In order to eliminate the effects of document length, usually people prefer to normalize by the number of documents a term shows up in, document frequency ( tf-idf ). Another approach is topic modeling, which learns a latent lower-dimensional representation of the data. LDA and LSI/LSA are typical choices, but it's important to remember this is unsupervised. The representation learned will not necessarily be ideal for whatever supervised learning you're doing with your NN. If you want to do topic modeling, you might also try supervised topic models . For individual words, you can use word2vec , which leverages NNs to embed words into an arbitrary-sized space. Similarity between two word vectors in this learned space tends to correspond to semantic similarity. A more recently pioneered approach is that of paragraph vectors , which first learns a word2vec-like word model, then builds on that representation to learn a distributed representation of sets of words (documents of any size). This has shown state-of-the-art results in many applications. When using NNs in NLP, people often use different architectures, like Recurrent Neural Nets (like Long Short Term Memory networks). In some cases people have even used Convolutional Neural Networks on text.
