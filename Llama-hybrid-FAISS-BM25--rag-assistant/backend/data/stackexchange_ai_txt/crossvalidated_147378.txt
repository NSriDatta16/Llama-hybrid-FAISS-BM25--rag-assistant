[site]: crossvalidated
[post_id]: 147378
[parent_id]: 94697
[tags]: 
First, notice that you can fix the variance of the hidden units to 1, since the weight matrix will scale them arbitrarily. Then: If you learn the variance of each visible unit, you get factor analysis If the variance of the visible units is tied, you get is PPCA (of the demeaned data) If you fix the variance of the visible units to a small value, you get pure PCA. In the last two cases, the weight matrix, $W$ will correspond to the leading eigenvectors of the data correlation matrix, up to a rotation. Stacking several layers is not equivalent to having a larger layer. The distribution of all the units is still jointly Gaussian, but the connectivity restricts the covariance matrix to a certain structure.
