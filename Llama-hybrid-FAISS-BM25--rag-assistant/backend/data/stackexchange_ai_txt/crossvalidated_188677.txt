[site]: crossvalidated
[post_id]: 188677
[parent_id]: 
[tags]: 
Can the Operating Characteristic for the LRT be derived from minimizing Bayes Risk $ \varphi(f) = \alpha P_F - \beta P_D + \gamma$?

For fun I was reading some notes on Operating Characteristics and it said (paraphrased with notation definition): As Fig. 1 suggests (omitted from question), good detection probability $P_D = P(\hat{H}(y) = H_1 \mid H = H_1)$ is generally obtained at the expense of high false alarm probability $P_F = P(\hat{H}(y) = H_1 \mid H = H_0)$ and so choosing a (Likely Ratio Test LRT) threshold $\eta$ for a particular problem involves making an acceptable tradeoff... From this perspective, the Bayesian Hypothesis test (i.e. choosing a decision rule that minimizing Bayes risk $\varphi(f) = \mathbb{E}_{H,y}[C(f(y), H)] $) represents a particular tradeoff, and corresponds to a single point on this curve. To obtain this tradeoff, we effectively selected as our objective function a linear combination of $P_D$ and $P_F$. More specifically, we minimize: $$ \varphi(f) = \alpha P_F - \beta P_D + \gamma$$ over all possible decision rules, where the choice of $\alpha$ and $\beta$ is in turn, determined by the cost assignment $C_{ij}$'s (where i corresponds to the chosen hypothesis by the decision rule and j corresponds to the correct hypothesis) and the a priori probabilities $P_m$'s. and claims for the Binary case that the following is true: $$\alpha = (C_{10} - C_{00})P_0,$$ $$\beta=(C_{01} - C_{11})P_1,$$ $$ \gamma = (C_{00} + C_{01})P_1$$ My first question is, why do we minimize: $$ \varphi(f) = \alpha P_F - \beta P_D + \gamma$$ that doesn't quite makes sense to me. Why do we want to minimize that? Is it because we just want to minimize the expected risk? (Also, why does that equation express the intrinsic trade off between $P_D$ and $P_F$?)From the notation, I infer that one can re-write Bayes Risk and write it in that form, however, I've been unable to do that. Here is what I have tried: $$ \varphi(f) = \mathbb{E}_{H,y}[C(f(y), H)] = \sum_{H_j,y} C(f(y), H_j) P(y,H_j)$$ $$ \varphi(f) = \sum_{H_j} \sum_{H_i} \sum_{y : f(y) = H_i} C(f(y) = H_i, H_j) P(y \mid H_j) P(H_j)$$ $$ =\sum_{H_j,H_i} C(H_i, H_j) P(f(y) = H_i \mid H_j) P(H_j)$$ for the binary case we have (were $P_i = P(H_i)$, $C_{ij} = C(f(y) = H_i, H_j)$, $P(H_i \mid H_j) = P(f(y) = H_i \mid H = H_j)$): $$ \varphi(f) = C_{00} P(H_0 \mid H_0)P_0 + C_{11} P(H_1 \mid H_1)P_1 + C_{10} P(H_1 \mid H_0)P_0 + C_{01} P(H_0 \mid H_1)P_1$$ $$ \varphi(f) = C_{00} P(H_0 \mid H_0)P_0 + C_{11} P_DP_1 + C_{10} P_F P_0 + C_{01} P(H_0 \mid H_1)P_1$$ and after this step, is where I get stuck. The reason I am having trouble advance is because, the form the text suggests as an answer does not even have the terms $P(f(y) = H_0 \mid H = H_0) = P(H_0 \mid H_0)$ and $ P(f(y) = H_0 \mid H = H_1) = P(H_0 \mid H_1)$ anywhere. Which makes me suspect that there might be a typo on the text and $\gamma$ should actually be something else involving $P(H_0 \mid H_1)$ or $P(f(y) = H_0 \mid H = H_0)$. Even if that were true, I can't explain myself how Bayes Risk has a minus sign (as in $\alpha P_F - \beta P_D + \gamma$) even if there were true, unless it was cancelled out somewhere again in the $\gamma$ by some hidden terms. Someone know where the typo is or if there is a way of proceeding from the location I got stuck?
