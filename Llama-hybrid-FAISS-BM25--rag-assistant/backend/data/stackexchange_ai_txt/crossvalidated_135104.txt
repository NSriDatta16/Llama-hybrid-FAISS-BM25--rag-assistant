[site]: crossvalidated
[post_id]: 135104
[parent_id]: 135100
[tags]: 
$p(f|\mathbf{x})$ is actually the prior for Gaussian process not the likelihood. Gaussian process by definition is assuming a particular prior over functions (represented as random variable $f|\mathbf{x}$) not parameters ($\mathbf{w}$). When the model is $y = f(x) + \epsilon$ where usually $\epsilon$ is considered to the noise in observation $y$, the likelihood (of $f$) is $p(y | f)$, the prior (of $f$) is $p(f|\mathbf{x})$ and the posterior (of $f$) is $p(f|y)$. $f$ is called the latent/hiden variable meaning that we cannot observe its values. Therefore, we are usually not interested in $p(f|y)$. $y$ is the variable representing observation samples/training data. By the assumption of linear regression model, $f$ is a linear function. Therefore, it can be expressed as $\mathbf{w}^T\mathbf{x}$. Now, we want to estimate parameters $\mathbf{w}$ using a Bayesian approach. The likelihood (of parameters) would be $p(y|\mathbf{w})$, the posterior (of parameters) is $p(\mathbf{w}|y)$ and the prior (of parameters) is $p(\mathbf{w})$. You are trying to be prove assuming $\mathbf{w}\sim\mathcal{N}(\mathbf{0}, \Sigma)$ and linearity of $f$ entails Gaussian process prior assumption. You are proving when the Gaussian process assumption meets the linear regression assumption then the two priors (one over functions, the other over parameters) are equivalent. Mean is not necessarily zero though. That is to simplify algebra.
