[site]: crossvalidated
[post_id]: 210758
[parent_id]: 
[tags]: 
Is there any point to Reverse Engineering the Fisher Information Matrix from an Inverse Covariance Matrix?

Would there be any advantage in deriving a Fisher Information Matrix backwards from an inverse covariance matrix? I've discovered that this is much easier to do on the SQL Server platform I use than calculating Fisher Information in the usual direction, which is not really conducive to SQL solutions; the whole realm of parameter estimation and Maximum Likelihood Estimation (MLE) seems better suited to ordinary computer languages (such as VB.Net or C# on my platform) or packages like R. I would nevertheless like to leverage SQL in my workflow though if there's any practical benefit in doing so, since it is trivial to derive large covariance matrices and invert them in a matter of seconds, on large datasets consisting of tens of millions of rows. Datasets closer to "Big Data" size are the norm, whereas those in ordinary statistical studies (particularly in medicine) seem to run somewhere between a few dozen to a few thousand. I'm basically approaching the question from an unusual vantage point, due to the idiosyncrasies of the platform I use, which allow me to easily fill in the entries of the Fisher Information in reverse with hard numbers, based on the weight of millions or even billions of records. I've learned from sources like David Wittman's Fisher Information for Beginners that covariance matrices are sometimes constructed for Bayesian priors, then inverted into Fisher Information Matrices and added to other Fisher Information Matrices in order to capture more information in experimental design; the resulting matrix is then inverted in the normal direction to derive the covariance estimates and Cramér-Rao Lower Bound and all that. What I am doing is something different: taking covariance figures calculated from millions of rows and inverting them, which gives me hard numbers to reverse engineer the Fisher Matrix from. By removing some power operations and canceling out some terms, I can basically arrive at the derivatives of both ℒ and the variables. What I need to know is whether the hard numbers this exercise spits out lend themselves to misinterpretation or are even merely useless gibberish. Information measures of all kinds are riddled with all sorts of subtle caveats in interpretation, so I don't want to make any assumptions, especially since I'm still a newbie at Fisher Information. I know from threads like this one at Quora that the entries of an inverse covariance matrix have value in and of themselves, in the form of partial correlations. The CrossValidated thread What Does the Inverse of Covariance Matrix Say About Data? (Intuitively) also discusses many other useful properties and mentions in passing that it's equivalent to the Fisher Matrix. How to interpret an inverse covariance or precision matrix? is another useful one that gets into partial correlations. My question(s) would be: Is there any point to doing this? My main concern would be that if we can derive exact covariances already, there might not be any sense in working backwards to finding something equivalent to likelihood estimates. Why do estimation if we already have certainties? Would the numbers returned have any validity (as opposed to usefulness, which is a different matter)? What meaning and mathematical properties would the entries of a Fisher Information Matrix reverse-engineered in this way have? Would there be any difference from ordinary Fisher Information? This would be a crucial consideration. A tangential question of less relevance: Do any of the considerations above change if the millions or billions of rows I'm plugging in are merely a sample of some unknown full population of billions or trillions? What if the proportional difference between the two is quite narrow - say, for example, the full population is only two or three times the size of our "Big Data"-sized sample? In the database server world, it seems to be more common to work with datasets that are proportionally closer or even identical to the full population. If these are dumb questions, I'll accept an answer that explains why. I've never seen these considerations addressed in the literature I've skimmed over the past few years, so either my approach is so ludicrous that it goes without saying that it wouldn't work, or there's some merit to it, but only in the kind of unusual use cases I run into all the time.
