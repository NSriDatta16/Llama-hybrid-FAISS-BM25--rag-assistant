[site]: datascience
[post_id]: 117061
[parent_id]: 117051
[tags]: 
See How to deal with perfect separation in logistic regression? Perfect separation tells you that there is a combination of predictor variables that perfectly predicts if someone is going to default on their loans. For example, an income of less than \$X, a monthly expenditure of more than \$Y, and owning Z might perfectly predict that someone will default, and vice versa. This generally implies that there's something wrong with your classification model, because well-off people default on their loans all the time because of human error, and financially insecure people can still pay their interest payments on time. Here are some suggestions from my experience: How are you defining credit risk? This sounds obvious, but is your definition based on a quantitative score you're calculating from your predictor variables? If so, try adjusting the weights and/or the threshold. Also, are you predicting credit risk for individuals or firms? Using predictor variables that are known to be correlated well with credit risk and validated by academic research can help you to build a more empirically reliable model. Do you have multicollinear variables? Multicollinearity refers to predictor variables that are strongly correlated with one another. I've seen pairs of variables with an R^2 of more than 0.99. This tends to distort your coefficients and output because they effectively measure the same construct. I don't think there's much of a consensus on which variable to remove because it's a qualitative decision, so you could select the more interpretable variable or the variable that's better supported by past models. Did you standardize your variables? Use a standard scaler on your numerical variables. I've worked with models that took a company's stock market capitalization (which can go up to a few billion dollars) and the ratio between the company's net profit and total assets on hand (which tends to range from 0 to 5). In cases like these, standardization ensures that the distribution of your predictor variables remains the same while ensuring that the range of possible values are relatively constant between variables.
