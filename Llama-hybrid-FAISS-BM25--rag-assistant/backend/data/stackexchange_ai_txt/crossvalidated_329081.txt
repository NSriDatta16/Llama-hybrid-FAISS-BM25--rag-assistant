[site]: crossvalidated
[post_id]: 329081
[parent_id]: 328630
[tags]: 
Thanks everybody for the great ongoing discussion. The crux of the matter seems to be that minimum-norm OLS is effectively performing shrinkage that is similar to the ridge regression. This seems to occur whenever $p\gg n$. Ironically, adding pure noise predictors can even be used as a very weird form or regularization. Part I. Demonstration with artificial data and analytical CV @Jonny (+1) came up with a really simple artificial example that I will slightly adapt here. $X$ of $n\times p$ size and $y$ are generated such that all variables are Gaussian with unit variance, and correlation between each predictor and the response is $\rho$. I will fix $\rho=.2$. I will use leave-one-out CV because there is analytical expression for the squared error: it is known as PRESS , "predicted sum of squares". $$\text{PRESS} = \sum_i \left( \frac{e_i}{1-H_{ii}}\right)^2,$$ where $e_i$ are residuals $$e = y - \hat y = y - Hy,$$ and $H$ is the hat matrix $$H = X (X^\top X + \lambda I)^{-1} X^\top=U\frac{S^2}{S^2+\lambda} U^\top$$ in terms of SVD $X=USV^\top$. This allows to replicate @Jonny's results without using glmnet and without actually performing cross-validation (I am plotting the ratio of PRESS to the sum of squares of $y$): This analytical approach allows to compute the limit at $\lambda\to 0$. Simply plugging in $\lambda=0$ into the PRESS formula does not work: when $n The trick is to do Taylor expansion of the hat matrix when $\lambda\to 0$: $$H=U\frac{1}{1+\lambda/S^2} U^\top\approx U(1-\lambda/S^2) U^\top = I - \lambda US^{-2}U^\top = I-\lambda G^{-1}.$$ Here I introduced Gram matrix $G=XX^\top = US^2U^\top$. We are almost done: $$\text{PRESS} = \sum_i\Big( \frac{\lambda [G^{-1}y]_i}{\lambda G^{-1}_{ii}}\Big)^2 = \sum_i\Big( \frac{ [G^{-1}y]_i}{G^{-1}_{ii}}\Big)^2.$$ Lambda got canceled out, so here we have the limiting value. I plotted it with a big black dot on the figure above (on the panels where $p>n$), and it matches perfectly. Update Feb 21. The above formula is exact, but we can gain some insight by doing further approximations. It looks like $G^{-1}$ has approximately equal values on the diagonal even if $S$ has very unequal values (probably because $U$ mixes up all the eigenvalues pretty well). So for each $i$ we have that $G^{-1}_{ii}\approx \langle S^{-2} \rangle$ where angular brackets denote averaging. Using this approximation, we can rewrite: $$\text{PRESS}\approx \Big\lVert \frac{S^{-2}}{\langle S^{-2} \rangle}U^\top y\Big\rVert^2.$$ This approximation is shown on the figure above with red open circles. Whether this will be larger or smaller than $\lVert y \rVert^2 = \lVert U^\top y \rVert^2$ depends on the singular values $S$. In this simulation $y$ is correlated with the first PC of $X$ so $U_1^\top y$ is large and all other terms are small. (In my real data, $y$ is also well predicted by the leading PCs.) Now, in the $p\gg n$ case, if the columns of $X$ are sufficiently random, then all singular values will be rather close to each other (rows approximately orthogonal). The "main" term $U_1^\top y$ will be multiplied by a factor less than 1. The terms towards the end will get multiplied by factors larger than 1 but not much larger. Overall the norm decreases. In contrast, in the $p\gtrsim n$ case, there will be some very small singular values. After inversion they will become large factors that will increase the overall norm. [This argument is very hand-wavy; I hope it can be made more precise.] As a sanity check, if I swap the order of singular values by S = diag(flipud(diag(S))); then the predicted MSE is above $1$ everywhere on the 2nd and the 3rd panels. figure('Position', [100 100 1000 300]) ps = [10, 100, 1000]; for pnum = 1:length(ps) rng(42) n = 80; p = ps(pnum); rho = .2; y = randn(n,1); X = repmat(y, [1 p])*rho + randn(n,p)*sqrt(1-rho^2); lambdas = exp(-10:.1:20); press = zeros(size(lambdas)); [U,S,V] = svd(X, 'econ'); % S = diag(flipud(diag(S))); % sanity check for i = 1:length(lambdas) H = U * diag(diag(S).^2./(diag(S).^2 + lambdas(i))) * U'; e = y - H*y; press(i) = sum((e ./ (1-diag(H))).^2); end subplot(1, length(ps), pnum) plot(log(lambdas), press/sum(y.^2)) hold on title(['p = ' num2str(p)]) plot(xlim, [1 1], 'k--') if p > n Ginv = U * diag(diag(S).^-2) * U'; press0 = sum((Ginv*y ./ diag(Ginv)).^2); plot(log(lambdas(1)), press0/sum(y.^2), 'ko', 'MarkerFaceColor', [0,0,0]); press0approx = sum((diag(diag(S).^-2/mean(diag(S).^-2)) * U' * y).^2); plot(log(lambdas(1)), press0approx/sum(y.^2), 'ro'); end end Part II. Adding pure noise predictors as a form of regularization Good arguments were made by @Jonny, @Benoit, @Paul, @Dikran, and others that increasing the number of predictors will shrink the minimum-norm OLS solution. Indeed, once $p>n$, any new predictor can only decrease the norm of the minimum-norm solution. So adding predictors will push the norm down, somewhat similar to how ridge regression is penalizing the norm. So can this be used as a regularization strategy? We start with $n=80$ and $p=40$ and then keep adding $q$ pure noise predictors as a regularization attempt. I will do LOOCV and compare it with LOOCV for the ridge (computed as above). Note that after obtaining $\hat\beta$ on the $p+q$ predictors, I am "truncating" it at $p$ because I am only interested in the original predictors. IT WORKS!!! In fact, one does not need to "truncate" the beta; even if I use the full beta and the full $p+q$ predictors, I can get good performance (dashed line on the right subplot). This I think mimics my actual data in the question: only few predictors are truly predicting $y$, most of them are pure noise, and they serve as a regularization. In this regime additional ridge regularization does not help at all. rng(42) n = 80; p = 40; rho = .2; y = randn(n,1); X = repmat(y, [1 p])*rho + randn(n,p)*sqrt(1-rho^2); lambdas = exp(-10:.1:20); press = zeros(size(lambdas)); [U,S,V] = svd(X, 'econ'); for i = 1:length(lambdas) H = U * diag(diag(S).^2./(diag(S).^2 + lambdas(i))) * U'; e = y - H*y; press(i) = sum((e ./ (1-diag(H))).^2); end figure('Position', [100 100 1000 300]) subplot(121) plot(log(lambdas), press/sum(y.^2)) hold on xlabel('Ridge penalty (log)') plot(xlim, [1 1], 'k--') title('Ridge regression (n=80, p=40)') ylim([0 2]) ps = [0 20 40 60 80 100 200 300 400 500 1000]; error = zeros(n, length(ps)); error_trunc = zeros(n, length(ps)); for fold = 1:n indtrain = setdiff(1:n, fold); for pi = 1:length(ps) XX = [X randn(n,ps(pi))]; if size(XX,2)
