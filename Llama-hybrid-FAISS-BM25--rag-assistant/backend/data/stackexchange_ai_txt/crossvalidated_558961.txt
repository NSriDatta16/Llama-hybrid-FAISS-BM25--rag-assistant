[site]: crossvalidated
[post_id]: 558961
[parent_id]: 557769
[tags]: 
Stanford University lists three types of transfer learning. Take a pretrained ConvNet on ImageNet, remove the last fully-connected layer (this layerâ€™s outputs are the class scores for a different task), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. Replace and retrain the classifier on top of the ConvNet on the new dataset, and also fine-tune the weights of the pretrained network by continuing the backpropagation. You can fine-tune all the layers of the ConvNet or keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. Fine-tune a published pre-trained model. Note that the last type really only differs in that the pre-trained model is typically large and publicly available. When to use which approach depends on how much data you have and how different it is from the pre-trained model (for more details see the link). \begin{array} {|l|l|l|} \hline \bf \text{Amount of data}& \bf \text{Similarity of data}& \bf \text{Procedure} \\ \hline Low & Low & \text{Unfreeze more layers of pretrained model}\\ \hline Low & High & \text{Unfreeze fewer layers of pretrained model}\\ \hline High & Low & \text{Initialize with pretrained weights and train completely} \\ \hline High & High & \text{Fine-tune completely} \\ \hline \end{array} CS231n Convolutional Neural Networks for Visual Recognition
