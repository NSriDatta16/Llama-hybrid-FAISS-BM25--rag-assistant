[site]: crossvalidated
[post_id]: 363407
[parent_id]: 362593
[tags]: 
After a bit of research it appears there is no "right" way to do this. You can get close by assigning probabilities to the word's rank on a list but the two methods I tried were not accurate enough for a good comparison. Use 1/rank normalized to 1.0. Est perplexity = 132. Actual = 100 Use the unigram prob curve. Est perplexity = 156. Actual = 100 There is a paper Estimation of gap between current language models and human performance where they ran into a similar need. The authors used 2 different measurement criteria, mean-log-rank (Rank) and percent-words-correct (Top1). In the paper, they ran these metrics against a number of different language models and calculated perplexity too. Table #2 of the paper is a few dozen models with perplexity measured along with the two new metrics. There is a reasonably good correlation between all three metrics so, using the table, you can estimate perplexity (PPL), given "Rank" or "Top1".
