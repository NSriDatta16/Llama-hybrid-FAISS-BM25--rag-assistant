[site]: crossvalidated
[post_id]: 159070
[parent_id]: 
[tags]: 
Curse of dimensionality: kNN classifier

I am reading Kevin Murphy's book: Machine Learning-A probabilistic Perspective. In the first chapter the author is explaining the curse of dimensionality and there is a part which i do not understand. As an example, the author states: Consider the inputs are uniformly distributed along a D-dimensional unit cube. Suppose we estimate the density of class labels by growing a hyper cube around x until it contains the desired fraction $f$ of the data points. The expected edge length of this cube is $e_D(f) = f^{\frac{1}{D}}$. It is the last formula that I cannot get my head around. it seems that if you want to cover say 10% of the points than the edge length should be 0.1 along each dimension? I know my reasoning is wrong but I cannot understand why.
