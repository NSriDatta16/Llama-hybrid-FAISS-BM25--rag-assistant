[site]: crossvalidated
[post_id]: 298494
[parent_id]: 298485
[tags]: 
In the context of applied research, effect sizes are necessary for readers to interpret the practical significance (as opposed to statistical significance) of the findings. In general, p-values are far more sensitive to sample size than effect sizes are. If an experiment measures an effect size accurately (i.e. it is sufficiently close to the population parameter it is estimating) but yields a non-significant p-value then, all things being equal, increasing the sample size will result in the same effect size but a lower p-value. This can be demonstrated with power analyses or simulations. In light of this, it is possible to achieve highly significant p-values for effect sizes that have no practical significance. In contrast, study designs with low power can produce non-significant p-values for effect sizes of great practical importance. It is difficult to discuss the concepts of statistical significance vis-a-vis effect size without a specific real-world application. As an example, consider an experiment that evaluates the effect of a new studying method on students' grade point average (GPA). I would argue that an effect size of 0.01 grade points has little practical significance (i.e. 2.50 compared to 2.51). Assuming a sample size of 2,000 students in both treatment and control groups, and a population standard deviation of 0.5 grade points: set.seed(12345) control.data treatment sample mean = 2.51 control sample mean = 2.50 effect size = 2.51 - 2.50 = 0.01 p = 0.53 Increasing the sample size to 20,000 students and holding everything else constant yields a significant p-value: set.seed(12345) control.data treatment sample mean = 2.51 control sample mean = 2.50 effect size = 2.51 - 2.50 = 0.01 p = 0.044 Obviously it's no trivial thing to increase the sample size by an order of magnitude! However, I think we can all agree that the practical improvement offered by this study method is negligible. If we relied solely on the p-value then we might believe otherwise in the n=20,000 case. Personally I advocate for reporting both p-values and effect sizes. And bonus points for t- or F-statistics, degrees of freedom and model diagnostics!
