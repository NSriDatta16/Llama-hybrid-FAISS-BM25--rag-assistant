[site]: datascience
[post_id]: 101995
[parent_id]: 101986
[tags]: 
There are several ways to find the relationship between vector representations in NLP, such as the cosine distance (you can check this for instance to apply it as a quick proof of concept) or L2 distance, which aim to find the relationship between such vectors in the vectors space they lay in. Nevertheless, to associate the geometric distance to semantic similarity, it is interesting to apply word embeddings , with which you get more lower-dimensional vectors directly learned from your data ( you can check for it ).
