[site]: stackoverflow
[post_id]: 3426421
[parent_id]: 3420265
[tags]: 
Okay, if you are doing neural networks most likely you will NOT need to take just a general derivative of some arbitrary function. Which is what you would need a general Calculus library for. Backprop requires you to use the derivative of your activation function. USUALLY, your activation function is going to be the sigmoid function or the hyperbolic tan function. Both of which you can just get the derivative of from Wikipedia and simply provide that function to your neural network training. You do not need to actually solve the derivative each time. There are other common activation functions, but there is really only a handful that is actually used. Just look up the derivative and make use of which one you want. Most neural network frameworks just build the regular activation function and derivative into some sort of a base class you use. Here are some of the most common ones: https://web.archive.org/web/20101105231126/http://www.heatonresearch.com/online/programming-neural-networks-encog-java/chapter-3/page2.html
