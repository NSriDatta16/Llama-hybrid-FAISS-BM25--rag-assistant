[site]: crossvalidated
[post_id]: 483638
[parent_id]: 483309
[tags]: 
Discriminator loss is always the same I think you're misreading the contex here. The two training schemes proposed by one particular paper used the same discriminator loss, but there are certainly many more different discriminator losses out there. What I don't get is that instead of using a single neuron with sigmoid and binary crossentropy , why do we use the equation given above? It is binary cross-entropy. I think you're confusing the mathematical description -- "we want to find the optimal function $D$ which maximizes...", versus the implementation side "we choose $D$ to be a neural network, and use sigmoid activation on the last layer". Although the mathematical description can be very suggestive about how to implement, and vice versa, they can be written differently without any conflict.
