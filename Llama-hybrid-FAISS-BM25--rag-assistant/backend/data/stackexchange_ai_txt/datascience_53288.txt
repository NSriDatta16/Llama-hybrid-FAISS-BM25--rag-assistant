[site]: datascience
[post_id]: 53288
[parent_id]: 
[tags]: 
Do I use class weights to penalize false negatives or threshold optimization to improve recall?

I built a Random Forest model for a binary classification problem.Both the classes in the target variable are balanced. My main class of interest is 'class 1'. False negatives are more costly to me, so it would make more sense to reduce false negatives by optimizing the recall. I read online that I can assign weights to my classes to penalize false negatives ( basically misclassification of class1) or optimize the threshold using a precision-recall curve to improve the recall for class 1. Im my case ( where both classes are almost balances) which method would be better?Can I still assign class weights to penalize false negatives?If yes, how do I determine what weights to apply to each class?or optimizing the threshold is better? Here is my classification report:
