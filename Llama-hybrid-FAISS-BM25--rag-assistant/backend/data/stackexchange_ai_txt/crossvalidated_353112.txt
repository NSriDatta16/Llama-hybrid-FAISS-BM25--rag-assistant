[site]: crossvalidated
[post_id]: 353112
[parent_id]: 
[tags]: 
Competing methods for estimating autocorrelation time

I'm testing several methods for obtaining the integrated autocorrelation time (IAT) of some synthetic data using Python. These methods are: emcee: the built-in method in this MCMC package. ISE: Initial sequence estimators as defined in Thompson (2010) (with a criteria for selecting the number of lags taken from the emcee docs ) BM: Batch means as defined in Thompson (2010). AR(p): AR process as defined in Thompson (2010). ACOR: an adapted version of the Goodman code. mESS: Multivariate effective sample size from Vats et al. (2015) , converted into IAT dividing it by the sample size (ie: $IAT=N/mESS$) The synthetic data is generated by simply adding some normal noise to a sine wave, with different values of the standard deviation $\sigma$ where a larger $\sigma$ means less correlated data: $$\mathrm{(heavily\; correlated\; data\; with\; \sigma=0.1)}$$ $$\mathrm{(mostly\; uncorrelated\; data\; with\; \sigma=5)}$$ The result looks like this: This looks reasonable as the IAT tends to converge to the expected minimum value of 1 for uncorrelated data for all methods. What I can not understand is that weird spike for the AR(p) method for low $\sigma$ values. I use statsmodels' Autoregressive AR(p) model to obtain both the number of lags and the fitted coefficients needed to estimate the IAT following Eq (7) in Thompson (2010). My question therefore is: is this behaviour for the AR(p) method expected, or should I be looking for some bug in my code? The Python code is a bit long so I don't paste it here. It can be found in this repo .
