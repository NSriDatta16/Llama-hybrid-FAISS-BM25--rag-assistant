[site]: crossvalidated
[post_id]: 298563
[parent_id]: 
[tags]: 
How to stabilize training in multi-objective reinforcement learning?

I am trying to train an agent to maximize multi-objectives. If I just add up rewards from different objectives, my problem is that the agent maximizes the 'easy' objective, at the expense of the hard one. How can I adaptively penalize the easiness with which the agent realize an objective?
