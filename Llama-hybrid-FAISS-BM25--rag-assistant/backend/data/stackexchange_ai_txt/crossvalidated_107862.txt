[site]: crossvalidated
[post_id]: 107862
[parent_id]: 107845
[tags]: 
Using PCA for feature selection (removing non-predictive features) is an extremely expensive way to do it. PCA algos are often O(n^3). Rather a much better and more efficient approach would be to use a measure of inter-dependence between the feature and the class - for this Mutual Information tends to perform very well, furthermore it's the only measure of dependence that a) fully generalizes and b) actually has a good philosophical foundation based on Kullback-Leibler divergence. For example, we compute (using maximum likelihood probability approx with some smoothing) MI-above-expected = MI(F, C) - E_{X, N}[MI(X, C)] where the second term is the 'expected mutual information given N examples'. We then take the top M features after sorting by MI-above-expected. The reason why one would want to use PCA is if one expects that many of the features are in fact dependent. This would be particularly handy for Naive Bayes where independence is assumed. Now the datasets I've worked with have always been far too large to use PCA, so I don't use PCA and we have to use more sophisticated methods. But if your dataset is small, and you don't have the time to investigate more sophisticated methods, then by all means go ahead and apply an out-of-box PCA.
