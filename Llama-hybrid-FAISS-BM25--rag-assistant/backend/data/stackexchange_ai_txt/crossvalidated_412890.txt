[site]: crossvalidated
[post_id]: 412890
[parent_id]: 
[tags]: 
Is it redundant to normalize test data during Mahalanobis calculation?

I have a training set which is a 12-column dataframe that I'm using to generate a covariance matrix (and possibly a PCA model). My test set is a single 12-column vector. All 12 variables have the same units. I want to just see if this vector is an outlier by calculating the Mahalanobis Distance (MD), and comparing that to the average MD values in my training data. In Python, it looks somewhat like this: cov = training_df.cov() mean_vector = training_df.mean() std_vector=training_df.std() MD= np.dot(np.dot((test_vector - mean_vector),np.linalg.inv(cov)),(test_vector-mean_vector).T) My question is - I have to use the training set to calculate the mean vector that I use for the MD. If I normalize my training data before getting the covariance, I'm subtracting the same "mean_vector" from the data set (and dividing by std). And if I do this, then I need to normalize the test vector to be consistent. So then instead of just raw data, it's actually (test_vector-mean_vector)/(std_vector). SO, since (test_vector-mean_vector) is a big part of the MD calculation, is it redundant to normalize it first before doing this? If I'm normalizing, instead of just raw numbers: (test_vector-mean_vector) the term in my MD is now: ((test_vector-mean_vector)/std_vector)-mean_vector in other words, I'm subtracting the mean twice. I've been having a little trouble wrapping my head around this for a couple days now. Just want to make sure I'm understanding this correctly. My thought, as of now, is that I should NOT normalize anything.
