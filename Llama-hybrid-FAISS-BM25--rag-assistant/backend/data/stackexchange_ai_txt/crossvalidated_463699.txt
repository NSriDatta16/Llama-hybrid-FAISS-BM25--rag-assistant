[site]: crossvalidated
[post_id]: 463699
[parent_id]: 
[tags]: 
Literary author classification from books content

Some background: I'm pretty much a newbie in NLP and in machine learning in general, I'm currently following some courses in my university about these topics. I'm working on my first ml project using basic scikit learn tools and some deep learning with Keras. I'm trying to build something that's able to classify the author of a book (just fiction books for the moment) by looking at its text. At first, I just worked with authors that had at least 30 books. In my dataset, that was ~2000 books written by ~30 authors in total, and that went pretty great, so I decided to raise the stakes. By lowering the threshold to 10 books, the database grew to ~400 authors and ~9000 books. The way I classified the books is as follows: I removed punctuation, newlines, extra spaces, and stopwords from every text, then I extracted the features with sklearn CountVectorizer and TfidfVectorizer. With this approach, the F1 score was 0.95 for the 2000 books db and 0.62 for the 9000 books db. The training with the larger dataset was much slower, so I decided to get 5000 random words from each book and use only those. This decreased the training time from 20 minutes to only 2 minutes, and enabled me to try something different. There wasn't a big difference in the various metrics, so I decided to try to remove the tfidf features. Using only CountVectorizer I got a 0.87 F1 score. I'm not sure why this is happening, my expectation was that the tfidf would make things easier and therefore better my score. Do you have any clue on why removing it improved the score instead? Are there other strategies, maybe better suited to work with long texts?
