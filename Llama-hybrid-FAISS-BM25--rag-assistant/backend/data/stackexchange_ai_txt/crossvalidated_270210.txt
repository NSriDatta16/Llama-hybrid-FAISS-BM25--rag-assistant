[site]: crossvalidated
[post_id]: 270210
[parent_id]: 
[tags]: 
Analyzing impact of a training program when performance increases have diminishing returns

I am testing a hypothesis that a training program impacts employee performance. My dataset consists of employee performance as measured by a unit output (e.g. 150 units, 201 units, etc.) in two consecutive periods (before (p0) and after (p1) training), as well as a binary flag of whether the employee received the training. The improvement in performance from p0 to p1 based on training v. no training is what I'm interested in. My issue with the data stems from two facts: 1) Whether the employee received the training is at the manager's discretion and therefore partially influenced by the employee performance in p0, ie lower performing employees tended to receive the training to correct performance. 2) Performance in output of units can only rise so high, and someone with a lower performance in p0 (say, 100 units) has much more room for potential improvement than someone already crushing it with 300 units (in fact it's likely that their performance will decrease in p1 because it would be difficult to repeat a 300unit period. Together, these two facts mean that on average, employees who receive training have more potential for gain than employees who don't. How can I account for the non-randomness of the "treatment" application and the phenomenon of (p1-p0) being dependent on starting position p0?
