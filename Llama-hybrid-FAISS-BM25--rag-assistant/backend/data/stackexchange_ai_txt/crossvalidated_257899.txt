[site]: crossvalidated
[post_id]: 257899
[parent_id]: 
[tags]: 
polar analog of cartesian cross-correlation function

Background: The cross-correlation function, wrapped in frequency domain convolution, is used in particle image velocimetry to allow sub-pixel metrology. It is also used in convolutional neural networks and deep learning, and has this feature: it is translation invariant. It is not rotation invariant. It is not skew invariant. It is not expansion invariant. A homogeneous affine transform allows simultaneous translation, rotation, and skew while allowing previously parallel lines to remain parallel and such. Question: Is there an analog to the classic Cartesian cross correlation that allows not only translation invariance, but rotation, skew, and scale invariance? That is a tall order. Let's simplify it substantially. Is there an analog to the classic Cartesian cross correlation that allows both translation invariance and rotation invariance? If so, how is it derived? Note: I have an idea about how to do this, but as I work through my idea, I'm hoping to see what is "known in the field". Edit: @whuber - I think of this in terms of images because my first exposure was PIV where cross-correlation is used to find new locations of the same particle in fluid flow a small time later. Translation invariance allows best guess for new location of an ensemble of particles. The particles are nonuniform, and non-identical. In a short time they can translate slightly, but in general they do not rotate much. If I was using a 360 degree camera, but instead of tracking stars using time-lapse like this What if I took two snapshots at relatively distant times in the same night? I wouldn't have streak-paths connecting the stars. If all I had was a mostly uniform translation then I could map stars in the first to stars in the second. The translation is not uniform. I could break the photo up into many small parcels and look for new locations there, but the poles have substantial rotation, and anything outside one-at-a-time cross correlation is likely to have poor results. the brightness of the stars changes over time. Near dawn/dusk they are going to be less bright. When darkness is complete the stars will be cleaner, but their images can show up having larger apparent size on the "film". I could hand-wave or ignore it, but that is nonzero scaling. Scaling invariance would help. A transform that had translation, rotation, skew, and scale invariance would be useful for less compute-intense tracking of the particles around the "vortex centers". Here is an example of the use of scale invariance. If we take this image, and try to find the transformation of the small apple to make the large one, where is the center? If they were at the same scale, then the transform would be a center-to-center displacement. They aren't. The small apple is the big apple, scaled and translated, then the stem is reflected. The mapping can be found if I take the small one, cut it out, and then work with two images. What if I don't know what an apple is? What if I am looking at brush-strokes in "the starry night" instead of apples, and I want to have rotation and scale adjusted "eigen-brushstrokes" directly from the images without invoking curvelets? What if I want to determine the transforms from this image, without knowing what a "truck" or "wheel" is? I could use an optimization search on the transform, but what if instead of searching there was a frequency domain method, an analog of a convolution, but one that did the same job in n*log(n) time?
