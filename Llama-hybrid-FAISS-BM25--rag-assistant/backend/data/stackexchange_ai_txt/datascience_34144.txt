[site]: datascience
[post_id]: 34144
[parent_id]: 34110
[tags]: 
If I understand the problem, you have text that is pre-sliced into some unit of interest (your chapter), that requires pre-processing to convert into a form suitable for further work. You've done some of this pre-processing by hand and want to use it as a training set for a model to infer the rules that should be applied to accomplish the same result on raw text. Because I don't know what you intend to do with the scrubbed text, I'm going to assume that it's destined for natural language processing to extract some semantic content. Here's what you'd normally do. Convert unicode encoding to UTF8 to get rid of the diacritical marks and other characters that aren't part of your target language Tokenize the text into a list of strings of words and punctuation marks Strip (or optionally convert to words if short) numbers Strip excess white space Optionally strip stop words (such as the, of, them ) Lemminize (remove inflections, such as by changing plurals to singular) This gets you to the point where you're ready to do linguistic analysis (which, as I say, I only assume is your goal). Fortunately, there are packages, such as NLTK in Python that do this for you without the need for regex. Writing a model to infer that from a training set of clean text might not be the best way to go, if getting to clean is your goal, rather than extending the frontiers of machine learning. Could you post a MWE (Minimal Working Example) of a chunk of raw text and it's corresponding manually scrubbed version. It may turn out to be a quicker means to your end to apply a combination of off-the-shelf tools to get to where you want to be.
