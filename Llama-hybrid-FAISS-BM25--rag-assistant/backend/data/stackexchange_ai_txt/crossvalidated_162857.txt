[site]: crossvalidated
[post_id]: 162857
[parent_id]: 
[tags]: 
Managing high autocorrelation in MCMC

I'm building a rather complex hierarchical Bayesian model for a meta-analysis using R and JAGS. Simplifying a bit, the two key levels of the model have $$ y_{ij} = \alpha_j + \epsilon_i$$ $$\alpha_j = \sum_h \gamma_{h(j)} + \epsilon_j$$ where $y_{ij}$ is the $i$th observation of the endpoint (in this case, GM vs. non-GM crop yields) in study $j$, $\alpha_j$ is the effect for study $j$, the $\gamma$s are effects for various study-level variables (the economic development status of the country where the study was done, crop species, study method, etc.) indexed by a family of functions $h$, and the $\epsilon$s are error terms. Note that the $\gamma$s are not coefficients on dummy variables. Instead, there are distinct $\gamma$ variables for different study-level values. For example, there is $\gamma_{developing}$ for developing countries and $\gamma_{developed}$ for developed countries. I'm primarily interested in estimating the values of the $\gamma$s. This means that dropping study-level variables from the model is not a good option. There's high correlation among several of the study-level variables, and I think this is producing large autocorrelations in my MCMC chains. This diagnostic plot illustrates the chain trajectories (left) and the resulting autocorrelation (right): As a consequence of the autocorrelation, I'm getting effective sample sizes of 60-120 from 4 chains of 10,000 samples each. I have two questions, one clearly objective and the other more subjective. Other than thinning, adding more chains, and running the sampler for longer, what techniques can I use to manage this autocorrelation problem? By "manage" I mean "produce reasonably good estimates in a reasonable amount of time." In terms of computing power, I'm running these models on a MacBook Pro. How serious is this degree of autocorrelation? Discussions both here and on John Kruschke's blog suggest that, if we just run the model long enough, "the clumpy autocorrelation has probably all been averaged out" (Kruschke) and so it's not really a big deal. Here's the JAGS code for the model that produced the plot above, just in case anyone is interested enough to wade through the details: model { for (i in 1:n) { # Study finding = study effect + noise # tau = precision (1/variance) # nu = normality parameter (higher = more Gaussian) y[i] ~ dt(alpha[study[i]], tau[study[i]], nu) } nu
