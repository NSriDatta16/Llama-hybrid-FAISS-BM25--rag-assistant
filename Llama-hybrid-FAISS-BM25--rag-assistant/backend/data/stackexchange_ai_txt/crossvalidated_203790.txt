[site]: crossvalidated
[post_id]: 203790
[parent_id]: 203711
[tags]: 
A large motivation in restricting the number of predictors available to each learner in a random forest is to encourage variance between the trees. Because each tree has the same starting point, tricks like row and column subsampling are necessary to ensure that you don't have the same tree multiple times. This isn't nearly as big a problem for boosting, where trees are built residually to each other. Each tree gets a new, adjusted starting point for which a new, different tree structure will be optimal. Subsampling by rows and columns still increases variance between trees and allows your model to converge faster with boosting, but it is not essential. $p/3$ or $\sqrt p$ seems like it would be too low for most boosting problems. Interacted signal will be harder to find if a pair of variables have such a small chance of existing together in the same tree. I have seen occasional and marginal gains in predictive power at around $3/4$.
