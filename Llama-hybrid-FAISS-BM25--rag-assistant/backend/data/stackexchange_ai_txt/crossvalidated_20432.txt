[site]: crossvalidated
[post_id]: 20432
[parent_id]: 20428
[tags]: 
No--not formally, at least. The logic of significance testing goes like this: a) We posit that if chance -- some random process -- were at work, we would see results that could be described by some distribution such as a bell curve, a flat "rectangular" distribution, a "binomial" (this or that) distribution, etc., depending on the situation. b) Given a set of results we obtain, we ask how unusual it would be for the random process to produce such results. In your example, you have identified no underlying distribution with which to compare the results you obtained. On the other hand, and less formally, there is a tradition of using the guesses of large numbers of people in order to estimate the correct amount of something. By this tradition, the guesses of hundreds of people, averaged out, should approximate an amount such as the number of jelly beans in a bowl. Perhaps someone can locate a good reference for this, or some cleverer examples.
