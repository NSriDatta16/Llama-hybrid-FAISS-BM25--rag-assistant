[site]: crossvalidated
[post_id]: 513643
[parent_id]: 
[tags]: 
How can I directly compare GBM and XGBoost?

I'm struggling to wrap my head around how GBM and XGBoost are mathematically related. Specifically, I don't know how to relate the steps described in the XGBoost vignette to the steps of the gradient tree boosting algorithm described here . I get that an important difference is that XGBoost minimizes a regularized objective while GBM simply minimizes the loss function. But does XGBoost otherwise have the same basic structure, i.e. does it at each iteration compute the negative gradients of the objective function for the previous iteration, fit a tree to them and then add the leaf predictions of this tree to the predictions from the previous iteration?
