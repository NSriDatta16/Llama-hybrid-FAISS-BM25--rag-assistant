[site]: crossvalidated
[post_id]: 169182
[parent_id]: 169148
[tags]: 
It turns out you've stumbled on one of the fundamentally hard, important, and well studied questions of Bayesian statistics. I know of two good options. The first is to use a conjugate prior . A prior can be conjugate to the distribution from which your data is being drawn, meaning the prior comes from a family of distributions, and the posterior will also be in that family. For instance the Beta distribution is conjugate to the Bernoulli, so if you use a Beta prior on Bernoulli data, you'll end up with an easily update Beta posterior. The second is know as Markov Chain Monte Carlo or MCMC . MCMC methods allow for approximately sampling from a distribution where one has the relative probability (or density in the continuous case) of events but not the absolute. Meaning you have a non-normalized PMF or PDF. This is useful because it allows you to avoid the often more difficult bottom integral that comes from the Bayes Law update. (If that last sentence doesn't make sense let me know and I'll explain). There are packages that allow you to set up generative models and approximately sample from posterior distributions, such as PyMC for Python.
