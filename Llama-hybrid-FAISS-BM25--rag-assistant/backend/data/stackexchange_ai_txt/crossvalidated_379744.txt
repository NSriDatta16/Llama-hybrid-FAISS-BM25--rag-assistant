[site]: crossvalidated
[post_id]: 379744
[parent_id]: 
[tags]: 
Comparison between SHAP (Shapley Additive Explanation) and LIME (Local Interpretable Model-Agnostic Explanations)

I am reading up about two popular post hoc model interpretability techniques: LIME and SHAP I am having trouble understanding the key difference in these two techniques. To quote Scott Lundberg , the brains behind SHAP: SHAP values come with the black box local estimation advantages of LIME, but also come with theoretical guarantees about consistency and local accuracy from game theory (attributes from other methods we unified) I am having some trouble understanding what this ' theoretical guarantees about consistency and local accuracy from game theory ' is. Since SHAP was developed after LIME, I assume it fills on some gaps which LIME fails to address. What are those? Christoph Molnar's book in a chapter on Shapley Estimation states: The difference between the prediction and the average prediction is fairly distributed among the features values of the instance - the shapley efficiency property. This property sets the Shapley value apart from other methods like LIME. LIME does not guarantee to perfectly distribute the effects. It might make the Shapley value the only method to deliver a full explanation Reading this, I get a sense that SHAP is not a local but a glocal explanation of the data point. I could be wrong here and need some insight into what this above quote means. To summarize my ask: LIME produces Local explanations. How are SHAP's explanations different from LIME's?
