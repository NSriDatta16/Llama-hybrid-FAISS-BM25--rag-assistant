[site]: datascience
[post_id]: 49499
[parent_id]: 
[tags]: 
Embedding variable length "multi-hot-encoded" features

How can I implement an embedding layer in Keras that takes in an input that could have a variable length? For instance, if the vocabulary was 10-long I could have inputs like: [1] [2,4,5] [7,3] But Keras embedding does not accept variable length inputs. I guess it is possible for me to zero-pad so that it looks like: [1,0,0] [2,4,5] [7,3,0] But this would make some of the inputs quite long (in my particular case, up to length ~500 vectors in a vocab of maybe ~10000). If only one data example has length 500 it forces even ones of length 1 to be zero padded to 500. Is there a better way to handle variable length inputs?
