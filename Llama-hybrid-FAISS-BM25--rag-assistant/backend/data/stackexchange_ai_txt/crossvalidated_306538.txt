[site]: crossvalidated
[post_id]: 306538
[parent_id]: 
[tags]: 
How can I maximize statistical power to test group size?

Suppose I'm ranking college students against their peers, and I want to study whether the size of the peer group affects the impact of ranking. For example, if students are ranked in groups of 20, do they work harder than if they are in groups of 100? I also want to study whether the impact varies across subgroups: for example, whether top students are affected differently from average students by peer group size, etc. Here's what I want to do: At the start of Semester 2, I randomly assign students to groups of either 20 or 100. I display a student's Semester 1 GPA alongside the rest of those in their group. Students don't see any other GPAs besides those in their peer group (assume they don't talk to others). I observe their Semester 2 GPA. To determine impact, I calculate whether the average GPA change is higher in groups of 20, compared to groups of 100. To determine whether top students are differently affected from average students, I look at whether students who were in the top quartile in semester 1 had a different average GPA change compared to those who were originally in the second and third quartiles. What I'm less sure is group assignment. To maximize power, should the number of groups of 20 be equal to the number of groups of 100? Or should there be more groups of 20? I have the intuition that there should be more groups of 20, but can't formalize it.
