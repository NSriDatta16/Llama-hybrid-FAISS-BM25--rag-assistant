[site]: datascience
[post_id]: 82383
[parent_id]: 
[tags]: 
vanishing gradient and gradient zero

There is a well known problem vanishing gradient in BackPropagation training of Feedforward Neural Network (FNN) (here we don't consider the vanishing gradient of Recurrent Neural Network). I don't understand why vanishing gradient does not mean the zero gradient namely the optimal solution we want? I saw some answer said vanishing gradient is not exactly the zero gradient, just means the update of gradient is very slow. However, the stopping rule of gradient decent is just the unchange of parameter within $\epsilon.$ So can anyone give me a clear answer?
