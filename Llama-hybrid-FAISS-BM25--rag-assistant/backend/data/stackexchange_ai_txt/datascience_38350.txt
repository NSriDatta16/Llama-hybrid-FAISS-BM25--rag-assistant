[site]: datascience
[post_id]: 38350
[parent_id]: 
[tags]: 
Which design pattern is better for data pipelines: batches or one at a time?

I come from a software engineering background and have a firm knowledge of best design patterns in that world, but with data science I feel like I'm making elementary design pattern mistakes. One thing I've noticed is that I often write code like this: mediumInputArray => someFilterFn => someMapFn => someOtherMapFn => someOtherFilterFn => someFinalFn All of those functions in that pipeline accept arrays, so method 2 doesn't start on the batch until method 1 finishes with all the array elements, and so one. It seems often mediumInputArray evolves into hugeInputArray and I need to shard things to run on many machines. At that point dealing with batches becomes brittle and cumbersome. Is it a better design pattern to just write a pipeline that operates on one atomic entity, and then have 1 job run from start to end? What would be some design patterns that are related to this? Is there a good book on advanced data science design patterns? Thanks!
