[site]: crossvalidated
[post_id]: 475485
[parent_id]: 473564
[tags]: 
Both formulations are equivalent and compatible with standard models of contextual multi-armed bandits, where you assume that you have information available about the environment that does not depend directly of your arm choices. This context, however, can be informative and thus predictive of what rewards / regret you may face based on what arm you pull. Examples Let's say every morning you need to decide whether or not to bring an umbrella to work. Say your goal is to avoid getting wet while minimizing how long it takes you to get ready to leave the house over time (you could model this as a e.g. weighted combination of two objectives). Let's consider the first model. The weather forecast ( context ) can help you decide whether or not you should carry an umbrella with you, but it's safe to assume that your choice (the arm you pull) of carrying an umbrella or not won't change the weather forecast over time. Now let's consider the second model. You could carry with you an umbrella or a raincoat. Context here could include information about the arms themselves (e.g. where each of these items are in your house on that day), but again, your choice wouldn't change the weather forecast. Relationship between those definitions When the 2nd definition that you quote says "A context vector $x_{t,a}$ summarizes information of both the user $u_t$ and arm $a$ " that's simply stating that context can be represented (understood) as a vector with per-arm features. This does not require you to assume that these features are only informative about a particular arm, or how the agent is supposed to leverage them, so the definition is equivalent to the 1st one. The difference between them is a matter of representation and does not change the problem at hand. Note that vectorial and scalar representation (encodings) of information are equivalent here. The agent can still be assumed to see the same information at the same time, use this information however it wants, and it still needs to decide which arm to pull. For example, for all you know, all features but one could be zero always, and the problem still allows you to use the non-zero feature left to choose which arm to pull. As stated, the agent can choose to re-represent, process and encode the information provided however it sees fit. Context and State If you assume instead that your actions can influence some information that you have about the environment or your arms, and you want to leverage this information to optimize the problem, it's more common to call that information state instead of just context. In the above example (e.g. 2nd model), carrying an umbrella to work may mean you could leave it in the car that night accidentally, so the following day your context would be impacted by this choice. Another example here would be a description of the screen that you see when playing a video game. This description would depend on your joystick actions (arm choices) over time. In this case you may end up with more general Markov decision processes for modeling the problem, and e.g. reinforcement learning to learn to optimize your objective.
