In probability theory, Slutsky’s theorem extends some properties of algebraic operations on convergent sequences of real numbers to sequences of random variables.

The theorem was named after Eugen Slutsky. Slutsky's theorem is also attributed to Harald Cramér.

Statement
Let  be sequences of scalar/vector/matrix random elements.
If   converges in distribution to a random element  and  converges in probability to a constant , then

 
 
    provided that c is invertible,

where  denotes convergence in distribution.

Notes:
 The requirement that Yn converges to a constant is important — if it were to converge to a non-degenerate random variable, the theorem would be no longer valid. For example, let  and .  The sum  for all values of n.  Moreover, , but  does not converge in distribution to , where , , and  and  are independent.
 The theorem remains valid if we replace all convergences in distribution with convergences in probability.

Proof
This theorem follows from the fact that if Xn converges in distribution to X and Yn converges in probability to a constant c, then the joint vector (Xn, Yn) converges in distribution to (X, c) (see here).

Next we apply the continuous mapping theorem, recognizing the functions g(x,y) = x + y, g(x,y) = xy, and g(x,y) = x y−1 are continuous (for the last function to be continuous, y has to be invertible).

See also
 Convergence of random variables

References

Further reading
 
 
 

Asymptotic theory (statistics)
Probability theorems
Theorems in statistics