A mixed model, mixed-effects model or mixed error-component model is a statistical model containing both fixed effects and random effects.  These models are useful in a wide variety of disciplines in the physical, biological and social sciences.
They are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures analysis of variance.

This page will discuss mainly linear mixed-effects models (LMEM) rather than generalized linear mixed models or nonlinear mixed-effects models.

History and current status

Ronald Fisher introduced random effects models to study the correlations of trait values between relatives.  In the 1950s, Charles Roy Henderson
provided best linear unbiased estimates of fixed effects and best linear unbiased predictions of random effects.  Subsequently, mixed modeling has become a major area of statistical research, including work on computation of maximum likelihood estimates, non-linear mixed effects models, missing data in mixed effects models, and Bayesian estimation of mixed effects models. Mixed models are applied in many disciplines where multiple correlated measurements are made on each unit of interest.  They are prominently used in research involving human and animal subjects in fields ranging from genetics to marketing, and have also been used in baseball and industrial statistics.

Definition

In matrix notation a linear mixed model can be represented as

where

 is a known vector of observations, with mean ;
 is an unknown vector of fixed effects;
 is an unknown vector of random effects, with mean  and variance–covariance matrix ;
 is an unknown vector of random errors, with mean  and variance ;
 is the known design matrix for the fixed effects relating the observations  to , respectively
  is the known Design matrix for the random effects relating the observations  to , respectively.

Estimation

The joint density of  and  can be written as: .
Assuming normality, ,  and , and maximizing the joint density over  and , gives Henderson's "mixed model equations" (MME) for linear mixed models:

The solutions to the MME,  and  are best linear unbiased estimates and predictors for  and , respectively. This is a consequence of the Gauss–Markov theorem when the conditional variance of the outcome is not scalable to the identity matrix. When the conditional variance is known, then the inverse variance weighted least squares estimate is best linear unbiased estimates. However, the conditional variance is rarely, if ever, known. So it is desirable to jointly estimate the variance and weighted parameter estimates when solving MMEs.

One method used to fit such mixed models is that of the expectation–maximization algorithm (EM) where the variance components are treated as unobserved nuisance parameters in the joint likelihood. Currently, this is the method implemented in statistical software such as Python (statsmodels package) and SAS (proc mixed), and as initial step only in R's nlme package lme().  The solution to the mixed model equations is a maximum likelihood estimate when the distribution of the errors is normal.

There are several other methods to fit mixed models, including using an MEM initially, and then Newton-Raphson (used by R package nlme's lme()), penalized least squares to get a profiled log likelihood only depending on the (low-dimensional) variance-covariance parameters of , i.e., its cov matrix , and then modern direct optimization for that reduced objective function (used by R's lme4 package lmer() and the Julia package MixedModels.jl) and direct optimization of the likelihood (used by e.g. R's glmmTMB). Notably, while the canonical form proposed by Henderson is useful for theory, many popular software packages use a different formulation for numerical computation in order to take advantage of sparse matrix methods (e.g. lme4 and MixedModels.jl).

See also
 Nonlinear mixed-effects model
 Fixed effects model
 Generalized linear mixed model
 Linear regression
 Mixed-design analysis of variance
 Multilevel model
 Random effects model
 Repeated measures design
 Empirical Bayes method

References

Further reading
 

Regression models
Analysis of variance