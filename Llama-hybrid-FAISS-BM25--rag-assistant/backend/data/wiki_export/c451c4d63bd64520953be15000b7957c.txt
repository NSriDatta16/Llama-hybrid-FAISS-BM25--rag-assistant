In directional statistics, the von Mises–Fisher distribution (named after Richard von Mises and Ronald Fisher), is a probability distribution on the -sphere in . If 
the distribution reduces to the von Mises distribution on the circle.

Definition
The probability density function of the von Mises–Fisher distribution for the random p-dimensional unit vector  is given by:

where  and  
the normalization constant  is equal to

 

where  denotes the modified Bessel function of the first kind at order . If , the normalization constant reduces to
 

The parameters  and  are called the mean direction and concentration parameter, respectively. The greater the value of , the higher the concentration of the distribution around the mean direction . The distribution is unimodal for , and is uniform on the sphere for .

The von Mises–Fisher distribution for  is also called the Fisher distribution.
It was first used to model the interaction of electric dipoles in an electric field. Other applications are found in geology, bioinformatics, and text mining.

Note on the normalization constant
In the textbook, Directional Statistics  by Mardia and Jupp, the normalization constant given for the Von Mises Fisher probability density is apparently different from the one given here: . In that book, the normalization constant is specified as:
 
where  is the gamma function. This is resolved by noting that Mardia and Jupp give the density "with respect to the uniform distribution", while the density here is specified in the usual way, with respect to Lebesgue measure. The density (w.r.t. Lebesgue measure) of the uniform distribution is the reciprocal of the surface area of the (p-1)-sphere, so that the uniform density function is given by the constant:

It then follows that:
 
While the value for  was derived above via the surface area, the same result may be obtained by setting  in the above formula for . This can be done by noting that the series expansion for  divided by  has but one non-zero term at . (To evaluate that term, one needs to use the definition .)

Support 
The support of the Von Mises–Fisher distribution is the hypersphere, or more specifically, the -sphere, denoted as 

This is a -dimensional manifold embedded in -dimensional Euclidean space, .

Relation to normal distribution 

Starting from a normal distribution with isotropic covariance  and mean  of length , whose density function is: 

the Von Mises–Fisher distribution is obtained by conditioning on . By expanding

and using the fact that the first two right-hand-side terms are fixed, the Von Mises-Fisher density,  is recovered by recomputing the normalization constant by integrating  over the unit sphere. If , we get the uniform distribution, with density .

More succinctly, the restriction of any isotropic multivariate normal density to the unit hypersphere, gives a Von Mises-Fisher density, up to normalization.

This construction can be generalized by starting with a normal distribution with a general covariance matrix, in which case conditioning on  gives the Fisher-Bingham distribution.

Estimation of parameters

Mean direction
A series of N independent unit vectors  are drawn from a von Mises–Fisher distribution. 
The maximum likelihood estimates of the mean direction  is simply the normalized arithmetic mean, a sufficient statistic:

Concentration parameter
Use the modified Bessel function of the first kind to define

 

Then:

Thus  is the solution to

A simple approximation to  is (Sra, 2011)

A more accurate inversion can be obtained by iterating the Newton method a few times

Standard error
For N ≥ 25, the estimated spherical standard error of the sample mean direction can be computed as: 

where

It is then possible to approximate a  a spherical confidence interval (a confidence cone) about  with semi-vertical angle:
 where 
For example, for a 95% confidence cone,  and thus

Expected value
The expected value of the Von Mises–Fisher distribution is not on the unit hypersphere, but instead has a length of less than one. This length is given by  as defined above. For a Von Mises–Fisher distribution with mean direction  and concentration , the expected value is: 
. 
For , the expected value is at the origin. For finite , the length of the expected value, is strictly between zero and one and is a monotonic rising function of .

The empirical mean (arithmetic average) of a collection of points on the unit hypersphere behaves in a similar manner, being close to the origin for widely  spread data and close to the sphere for concentrated data. Indeed, for the Von Mises–Fisher distribution, the expected value of the maximum-likelihood estimate based on a collection of points is equal to the empirical mean of those points.

Entropy and KL divergence 
The expected value can be used to compute differential entropy and KL divergence.

The differential entropy of  is:
 
where the angle brackets denote expectation. Notice that the entropy is a function of  only.

The KL divergence between  and  is:

Transformation
Von Mises-Fisher (VMF) distributions are closed under orthogonal linear transforms. Let  be a -by- orthogonal matrix. Let  and apply the invertible linear transform: . The inverse transform is , because the inverse of an orthogonal matrix is its transpose: . The Jacobian of the transform is , for which the absolute value of its determinant is 1, also because of the orthogonality. Using these facts and the form of the VMF density, it follows that:  

One may verify that since  and  are unit vectors, then by the orthogonality, so are  and .

Pseudo-random number generation

General case 
An algorithm for drawing pseudo-random samples from the Von Mises Fisher (VMF) distribution was given by Ulrich and later corrected by Wood. An implementation in R is given by Hornik and Grün; and a fast Python implementation is described by Pinzón and Jung.

To simulate from a VMF distribution on the -dimensional unitsphere, , with mean direction , these algorithms use the following radial-tangential decomposition for a point  :

where  lives in the tangential -dimensional unit-subsphere that is centered at and perpendicular to ; while . To draw a sample  from a VMF with parameters 
 and ,  must be drawn from the uniform distribution on the tangential subsphere; and the radial component, , must be drawn independently from the distribution with density:

where . The normalization constant for this density may be verified by using:

as given in Appendix 1 (A.3) in Directional Statistics. Drawing the  samples from this density by using a rejection sampling algorithm is explained in the above references. To draw the uniform  samples perpendicular to , see the algorithm in, or otherwise a Householder transform can be used as explained in Algorithm 1 in.

3-D sphere 
To generate a Von Mises–Fisher distributed pseudo-random spherical 3-D unit vector  on the sphere for a given  and , define

where  is the polar angle,  the equatorial angle, and  the distance to the center of the sphere

for  the pseudo-random vector is then given by

where  is sampled from the continuous uniform distribution  with lower bound  and upper bound 

and

where  is sampled from the standard continuous uniform distribution 

here, should be set to  when  and  rotated to match any other desired .

Distribution of polar angle

For , the angle θ between  and  satisfies . It has the distribution
,
which can be easily evaluated as
.

For the general case, , the distribution for the cosine of this angle:

is given by , as explained above.

The uniform hypersphere distribution
When , the Von Mises–Fisher distribution,  on  simplifies to the uniform distribution on . The density is constant with value . Pseudo-random samples can be generated by generating samples in  from the standard multivariate normal distribution, followed by normalization to unit norm.

Component marginal of uniform distribution
For , let  be any component of . The marginal distribution for  has the density:

where  is the beta function. This distribution may be better understood by highlighting its relation to the beta distribution:

where the Legendre duplication formula is useful to understand the relationships between the normalization constants of the various densities above.

Note that the components of  are not independent, so that the uniform density is not the product of the marginal densities; and  cannot be assembled by independent sampling of the components.

Distribution of dot-products 
In machine learning, especially in image classification, to-be-classified inputs (e.g. images) are often compared using cosine similarity, which is the dot product between intermediate representations in the form of unitvectors (termed embeddings). The dimensionality is typically high, with  at least several hundreds. The deep neural networks that extract embeddings for classification should learn to spread the classes as far apart as possible and ideally this should give classes that are uniformly distributed on . For a better statistical understanding of across-class cosine similarity, the distribution of dot-products between unitvectors independently sampled from the uniform distribution may be helpful.

 
Let  be unitvectors in , independently sampled from the uniform distribution. Define:

where  is the dot-product and  are transformed versions of it. Then the distribution for  is the same as the marginal component distribution given above; the distribution for  is symmetric beta and the distribution for  is  symmetric logistic-beta:
 
The means and variances are:

and

where  is the first polygamma function. The variances decrease, the distributions of all three variables become more Gaussian, and the final approximation gets better as the dimensionality, , is increased.

Generalizations

Matrix Von Mises-Fisher

The matrix von Mises-Fisher distribution (also known as matrix Langevin distribution) has the density

supported on the Stiefel manifold of  orthonormal p-frames , where  is an arbitrary  real matrix.

Saw distributions 
Ulrich, in designing an algorithm for sampling from the VMF distribution, makes use of a family of distributions named after and explored by John G. Saw. A Saw distribution is a distribution on the -sphere, , with modal vector  and concentration , and of which the density function has the form:
 
where  is a non-negative, increasing function; and where  is the normalization constant. The above-mentioned radial-tangential decomposition generalizes to the Saw family and the radial compoment,  has the density:

where  is the beta function. Also notice that the left-hand factor of the radial density is the surface area of .

By setting , one recovers the VMF distribution.

See also
 Kent distribution, a related distribution on the two-dimensional unit sphere
 von Mises distribution, von Mises–Fisher distribution where p = 2, the one-dimensional unit circle
 Bivariate von Mises distribution
 Directional statistics

References

Further reading
 Dhillon, I., Sra, S. (2003) "Modeling Data using Directional Distributions". Tech. rep., University of Texas, Austin.
 Banerjee, A., Dhillon, I. S., Ghosh, J., & Sra, S. (2005). "Clustering on the unit hypersphere using von Mises-Fisher distributions". Journal of Machine Learning Research, 6(Sep), 1345-1382.
 

Directional statistics
Multivariate continuous distributions
Exponential family distributions
Continuous distributions