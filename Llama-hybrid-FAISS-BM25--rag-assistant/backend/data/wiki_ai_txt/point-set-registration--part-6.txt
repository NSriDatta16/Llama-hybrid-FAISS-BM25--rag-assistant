est if the initial pose of M {\displaystyle {\mathcal {M}}} is sufficiently close to S {\displaystyle {\mathcal {S}}} . In pseudocode, the basic algorithm is implemented as follows: algorithm ICP(M, S) θ := θ0 while not registered: X := ∅ for mi ∊ T(M, θ): ŝi := closest point in S to mi X := X + ⟨mi, ŝi⟩ θ := least_squares(X) return θ Here, the function least_squares performs least squares optimization to minimize the distance in each of the ⟨ m i , s ^ i ⟩ {\displaystyle \langle m_{i},{\hat {s}}_{i}\rangle } pairs, using the closed-form solutions by Horn and Arun. Because the cost function of registration depends on finding the closest point in S {\displaystyle {\mathcal {S}}} to every point in M {\displaystyle {\mathcal {M}}} , it can change as the algorithm is running. As such, it is difficult to prove that ICP will in fact converge exactly to the local optimum. In fact, empirically, ICP and EM-ICP do not converge to the local minimum of the cost function. Nonetheless, because ICP is intuitive to understand and straightforward to implement, it remains the most commonly used point set registration algorithm. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. For example, the expectation maximization algorithm is applied to the ICP algorithm to form the EM-ICP method, and the Levenberg-Marquardt algorithm is applied to the ICP algorithm to form the LM-ICP method. Robust point matching Robust point matching (RPM) was introduced by Gold et al. The method performs registration using deterministic annealing and soft assignment of correspondences between point sets. Whereas in ICP the correspondence generated by the nearest-neighbour heuristic is binary, RPM uses a soft correspondence where the correspondence between any two points can be anywhere from 0 to 1, although it ultimately converges to either 0 or 1. The correspondences found in RPM is always one-to-one, which is not always the case in ICP. Let m i {\displaystyle m_{i}} be the i {\displaystyle i} th point in M {\displaystyle {\mathcal {M}}} and s j {\displaystyle s_{j}} be the j {\displaystyle j} th point in S {\displaystyle {\mathcal {S}}} . The match matrix μ {\displaystyle \mathbf {\mu } } is defined as such: The problem is then defined as: Given two point sets M {\displaystyle {\mathcal {M}}} and S {\displaystyle {\mathcal {S}}} find the Affine transformation T {\displaystyle T} and the match matrix μ {\displaystyle \mathbf {\mu } } that best relates them. Knowing the optimal transformation makes it easy to determine the match matrix, and vice versa. However, the RPM algorithm determines both simultaneously. The transformation may be decomposed into a translation vector and a transformation matrix: T ( m ) = A m + t {\displaystyle T(m)=\mathbf {A} m+\mathbf {t} } The matrix A {\displaystyle \mathbf {A} } in 2D is composed of four separate parameters { a , θ , b , c } {\displaystyle \lbrace a,\theta ,b,c\rbrace } , which are scale, rotation, and the vertical and horizontal shear components respectively. The cost function is then: subject to ∀ j ∑ i = 1 M μ i j ≤ 1 {\textstyle \forall j~\sum _{i=1}^{M}\mu _{ij}\leq 1} , ∀ i ∑ j = 1 N μ i j ≤ 1 {\textstyle \forall i~\sum _{j=1}^{N}\mu _{ij}\leq 1} , ∀ i j μ i j ∈ { 0 , 1 } {\textstyle \forall ij~\mu _{ij}\in \lbrace 0,1\rbrace } . The α {\displaystyle \alpha } term biases the objective towards stronger correlation by decreasing the cost if the match matrix has more ones in it. The function g ( A ) {\displaystyle g(\mathbf {A} )} serves to regularize the Affine transformation by penalizing large values of the scale and shear components: g ( A ( a , θ , b , c ) ) = γ ( a 2 + b 2 + c 2 ) {\displaystyle g(\mathbf {A} (a,\theta ,b,c))=\gamma (a^{2}+b^{2}+c^{2})} for some regularization parameter γ {\displaystyle \gamma } . The RPM method optimizes the cost function using the Softassign algorithm. The 1D case will be derived he