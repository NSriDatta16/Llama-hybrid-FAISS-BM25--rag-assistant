robabilities: p t + 1 ( X i ) = 1 λ ∑ x ∈ S ( P ( t ) ) x i , ∀ i ∈ 1 , 2 , … , N . {\displaystyle p_{t+1}(X_{i})={\dfrac {1}{\lambda }}\sum _{x\in S(P(t))}x_{i},~\forall i\in 1,2,\dots ,N.} Every UMDA step can be described as follows D ( t + 1 ) = α UMDA ∘ S ∘ β λ ( D ( t ) ) . {\displaystyle D(t+1)=\alpha _{\text{UMDA}}\circ S\circ \beta _{\lambda }(D(t)).} Population-based incremental learning (PBIL) The PBIL, represents the population implicitly by its model, from which it samples new solutions and updates the model. At each generation, μ {\displaystyle \mu } individuals are sampled and λ ≤ μ {\displaystyle \lambda \leq \mu } are selected. Such individuals are then used to update the model as follows p t + 1 ( X i ) = ( 1 − γ ) p t ( X i ) + ( γ / λ ) ∑ x ∈ S ( P ( t ) ) x i , ∀ i ∈ 1 , 2 , … , N , {\displaystyle p_{t+1}(X_{i})=(1-\gamma )p_{t}(X_{i})+(\gamma /\lambda )\sum _{x\in S(P(t))}x_{i},~\forall i\in 1,2,\dots ,N,} where γ ∈ ( 0 , 1 ] {\displaystyle \gamma \in (0,1]} is a parameter defining the learning rate, a small value determines that the previous model p t ( X i ) {\displaystyle p_{t}(X_{i})} should be only slightly modified by the new solutions sampled. PBIL can be described as D ( t + 1 ) = α PIBIL ∘ S ∘ β μ ( D ( t ) ) {\displaystyle D(t+1)=\alpha _{\text{PIBIL}}\circ S\circ \beta _{\mu }(D(t))} Compact genetic algorithm (cGA) The CGA, also relies on the implicit populations defined by univariate distributions. At each generation t {\displaystyle t} , two individuals x , y {\displaystyle x,y} are sampled, P ( t ) = β 2 ( D ( t ) ) {\displaystyle P(t)=\beta _{2}(D(t))} . The population P ( t ) {\displaystyle P(t)} is then sorted in decreasing order of fitness, S Sort ( f ) ( P ( t ) ) {\displaystyle S_{{\text{Sort}}(f)}(P(t))} , with u {\displaystyle u} being the best and v {\displaystyle v} being the worst solution. The CGA estimates univariate probabilities as follows p t + 1 ( X i ) = p t ( X i ) + γ ( u i − v i ) , ∀ i ∈ 1 , 2 , … , N , {\displaystyle p_{t+1}(X_{i})=p_{t}(X_{i})+\gamma (u_{i}-v_{i}),\quad \forall i\in 1,2,\dots ,N,} where, γ ∈ ( 0 , 1 ] {\displaystyle \gamma \in (0,1]} is a constant defining the learning rate, usually set to γ = 1 / N {\displaystyle \gamma =1/N} . The CGA can be defined as D ( t + 1 ) = α CGA ∘ S Sort ( f ) ∘ β 2 ( D ( t ) ) {\displaystyle D(t+1)=\alpha _{\text{CGA}}\circ S_{{\text{Sort}}(f)}\circ \beta _{2}(D(t))} Bivariate factorizations Although univariate models can be computed efficiently, in many cases they are not representative enough to provide better performance than GAs. In order to overcome such a drawback, the use of bivariate factorizations was proposed in the EDA community, in which dependencies between pairs of variables could be modeled. A bivariate factorization can be defined as follows, where π i {\displaystyle \pi _{i}} contains a possible variable dependent to X i {\displaystyle X_{i}} , i.e. | π i | = 1 {\displaystyle |\pi _{i}|=1} . D Bivariate := p ( X 1 , … , X N ) = ∏ i = 1 N p ( X i | π i ) . {\displaystyle D_{\text{Bivariate}}:=p(X_{1},\dots ,X_{N})=\prod _{i=1}^{N}p(X_{i}|\pi _{i}).} Bivariate and multivariate distributions are usually represented as probabilistic graphical models (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed. Mutual information maximizing input clustering (MIMIC) The MIMIC factorizes the joint probability distribution in a chain-like model representing successive dependencies between variables. It finds a permutation of the decision variables, r : i ↦ j {\displaystyle r:i\mapsto j} , such that x r ( 1 ) x r ( 2 ) , … , x r ( N ) {\displaystyle x_{r(1)}x_{r(2)},\dots ,x_{r(N)}} minimizes the Kullback–Leibler divergence in relation to the true probability distribution, i.e. π r ( i + 1 ) = { X r ( i ) } {\displaystyle \pi _{r(i+1)}=\{X_{r(i)}\}} . MIMIC models a distribution p t + 1 (