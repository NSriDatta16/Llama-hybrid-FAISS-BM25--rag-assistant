the attributes of the related objects and the relationships that are independent of each other are included. Example Detailed example sequences can be found here. Dataset The Images considered for the Geman et al. work are that of ‘Urban street scenes’ dataset, which has scenes of streets from different cities across the world. This why the types of objects are constrained to people and vehicles for this experiment. Another dataset introduced by the Max Planck Institute for Informatics is known as DAQUAR dataset which has real world images of indoor scenes. But they propose a different version of the visual Turing test which takes on a holistic approach and expects the participating system to exhibit human like common sense. Conclusion This is a very recent work published on March 9, 2015, in the journal Proceedings of the National Academy of Sciences, by researchers from Brown University and Johns Hopkins University. It evaluates how the computer vision systems understand the Images as compared to humans. Currently the test is written and the interrogator is a machine because having an oral evaluation by a human interrogator gives the humans an undue advantage of being subjective, and also expects real time answers. The Visual Turing Test is expected to give a new direction to the computer vision research. Companies like Google and Facebook are investing millions of dollars into computer vision research, and are trying to build systems that closely resemble the human visual system. Recently Facebook announced its new platform M, which looks at an image and provides a description of it to help the visually impaired. Such systems might be able to perform well on the VTT. == References ==