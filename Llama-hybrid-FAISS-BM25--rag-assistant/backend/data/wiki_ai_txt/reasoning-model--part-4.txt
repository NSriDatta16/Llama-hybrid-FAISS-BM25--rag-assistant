automatically. This method generates multiple reasoning traces for each prompt, then filters out traces with incorrect final answers using a verifier. Reinforcement learning A pretrained language model can be further trained with RL. In the RL formalism, a generative language model is a policy π {\displaystyle \pi } . A task prompt is an environmental state x {\displaystyle x} , and the model's response is an action y {\displaystyle y} . The probability that the model responds x {\displaystyle x} with y {\displaystyle y} is π ( y | x ) {\displaystyle \pi (y|x)} . Training a reasoning language model with RL means constructing a reward model r ( x , y ) {\displaystyle r(x,y)} to guide the RL process. Intuitively, the reward says how good a response is for a prompt. For a reasoning task, the reward is high if the response solves the task and low if it does not. A response y {\displaystyle y} may be broken-down into multiple steps, written y 1 , y 2 , … , y n {\displaystyle y_{1},y_{2},\dots ,y_{n}} . Most recent systems use policy-gradient methods such as Proximal Policy Optimization (PPO) because PPO constrains each policy update with a clipped objective, which stabilises training for very large policies. Outcome reward model An outcome reward model, or outcome-supervised RM (ORM), gives the reward for a step r ( x , y 1 , … , y i ) {\displaystyle r(x,y_{1},\dots ,y_{i})} based on the final answer: r ( x , y 1 , … , y i ) = r ( x , y n ) {\displaystyle r(x,y_{1},\dots ,y_{i})=r(x,y_{n})} . Such models are often called "verifiers". For tasks with answers that are easy to verify, such as math word problems, the outcome reward can be binary: 1 if the final answer is correct, 0 otherwise. If automatic verification is hard, humans can label answers as correct or not, and those labels can be used to finetune a base model that predicts the human label. For tasks like creative writing, where quality is not simply true or false, one can train a reward model on human ranked preference data, as in reinforcement learning from human feedback. A base model can also be fine-tuned to predict, from a partial thinking trace x , y 1 , … , y m {\displaystyle x,y_{1},\dots ,y_{m}} , whether the final answer will be correct, and this prediction can serve as a binary reward. The ORM is usually trained with logistic regression, i.e. by minimizing cross-entropy loss. Given a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace, by taking the minimum, or by other ways of aggregating process rewards. DeepSeek used a simple ORM to train the R1 model. Process reward model A process reward model, or process-supervised RM (PRM), gives the reward for a step r ( x , y 1 , … , y i ) {\displaystyle r(x,y_{1},\dots ,y_{i})} based only on the steps so far: ( x , y 1 , … , y i ) {\displaystyle (x,y_{1},\dots ,y_{i})} . Given a partial thinking trace x , y 1 , … , y m {\displaystyle x,y_{1},\dots ,y_{m}} , a human can judge whether the steps so far are correct, without looking at the final answer. This yields a binary reward. Because human labels are costly, a base model can be fine-tuned to predict them. The PRM is usually trained with logistic regression on the human labels, i.e. by minimizing the cross-entropy loss between true and predicted labels. As an example, a 2023 OpenAI paper collected 800K process labels for 75K thinking traces. A labeler saw a trace and marked each step as "positive" if it moved toward a solution, "neutral" if it was not wrong but did not help, and "negative" if it was a mistake. After the first "negative" label, the labeler stopped on that trace and moved to another. The authors argued that labeling up to the first error was enough to train a capable PRM, even though labeling later steps could give richer signals. To avoid human labels, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method sa