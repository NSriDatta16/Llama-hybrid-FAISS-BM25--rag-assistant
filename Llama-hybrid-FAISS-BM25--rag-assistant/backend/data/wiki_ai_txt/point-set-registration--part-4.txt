use its runtime grows exponentially with respect to the outlier ratio. To fill the gap between the fast but inexact RANSAC scheme and the exact but exhaustive BnB optimization, recent researches have developed deterministic approximate methods to solve consensus maximization. Outlier removal Outlier removal methods seek to pre-process the set of highly corrupted correspondences before estimating the spatial transformation. The motivation of outlier removal is to significantly reduce the number of outlier correspondences, while maintaining inlier correspondences, so that optimization over the transformation becomes easier and more efficient (e.g., RANSAC works poorly when the outlier ratio is above 95 % {\displaystyle 95\%} but performs quite well when outlier ratio is below 50 % {\displaystyle 50\%} ). Parra et al. have proposed a method called Guaranteed Outlier Removal (GORE) that uses geometric constraints to prune outlier correspondences while guaranteeing to preserve inlier correspondences. GORE has been shown to be able to drastically reduce the outlier ratio, which can significantly boost the performance of consensus maximization using RANSAC or BnB. Yang and Carlone have proposed to build pairwise translation-and-rotation-invariant measurements (TRIMs) from the original set of measurements and embed TRIMs as the edges of a graph whose nodes are the 3D points. Since inliers are pairwise consistent in terms of the scale, they must form a clique within the graph. Therefore, using efficient algorithms for computing the maximum clique of a graph can find the inliers and effectively prune the outliers. The maximum clique based outlier removal method is also shown to be quite useful in real-world point set registration problems. Similar outlier removal ideas were also proposed by Parra et al.. M-estimation M-estimation replaces the least squares objective function in (cb.2) with a robust cost function that is less sensitive to outliers. Formally, M-estimation seeks to solve the following problem:where ρ ( ⋅ ) {\displaystyle \rho (\cdot )} represents the choice of the robust cost function. Note that choosing ρ ( x ) = x 2 {\displaystyle \rho (x)=x^{2}} recovers the least squares estimation in (cb.2). Popular robust cost functions include ℓ 1 {\displaystyle \ell _{1}} -norm loss, Huber loss, Geman-McClure loss and truncated least squares loss. M-estimation has been one of the most popular paradigms for robust estimation in robotics and computer vision. Because robust objective functions are typically non-convex (e.g., the truncated least squares loss v.s. the least squares loss), algorithms for solving the non-convex M-estimation are typically based on local optimization, where first an initial guess is provided, following by iterative refinements of the transformation to keep decreasing the objective function. Local optimization tends to work well when the initial guess is close to the global minimum, but it is also prone to get stuck in local minima if provided with poor initialization. Graduated non-convexity Graduated non-convexity (GNC) is a general-purpose framework for solving non-convex optimization problems without initialization. It has achieved success in early vision and machine learning applications. The key idea behind GNC is to solve the hard non-convex problem by starting from an easy convex problem. Specifically, for a given robust cost function ρ ( ⋅ ) {\displaystyle \rho (\cdot )} , one can construct a surrogate function ρ μ ( ⋅ ) {\displaystyle \rho _{\mu }(\cdot )} with a hyper-parameter μ {\displaystyle \mu } , tuning which can gradually increase the non-convexity of the surrogate function ρ μ ( ⋅ ) {\displaystyle \rho _{\mu }(\cdot )} until it converges to the target function ρ ( ⋅ ) {\displaystyle \rho (\cdot )} . Therefore, at each level of the hyper-parameter μ {\displaystyle \mu } , the following optimization is solved:Black and Rangarajan proved that the objective function of each optimization (cb.6)