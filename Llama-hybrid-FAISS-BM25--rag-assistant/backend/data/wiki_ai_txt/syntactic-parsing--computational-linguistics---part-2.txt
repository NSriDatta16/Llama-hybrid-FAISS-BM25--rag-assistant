also competitive with previous approaches to constituency parsing. Their work won the best paper award at ACL 2022. Transition-based Following the success of O ( n ) {\displaystyle O(n)} transition-based parsing for dependency grammars, work began on adapting the approach to constituency parsing. The first such work was by Kenji Sagae and Alon Lavie in 2005, which relied on a feature-based classifier to greedily make transition decisions. This was followed by the work of Yue Zhang and Stephen Clark in 2009, which added beam search to the decoder to make more globally-optimal parses. The first parser of this family to outperform a chart-based parser was the one by Muhua Zhu et al. in 2013, which took on the problem of length differences of different transition sequences due to unary constituency rules (a non-existent problem for dependency parsing) by adding a padding operation. Note that transition-based parsing can be purely greedy (i.e. picking the best option at each time-step of building the tree, leading to potentially non-optimal or ill-formed trees) or use beam search to increase performance while not sacrificing efficiency. Sequence-to-sequence A different approach to constituency parsing leveraging neural sequence models was developed by Oriol Vinyals et al. in 2015. In this approach, constituent parsing is modelled like machine translation: the task is sequence-to-sequence conversion from the sentence to a constituency parse, in the original paper using a deep LSTM with an attention mechanism. The gold training trees have to be linearised for this kind of model, but the conversion does not lose any information. This runs in O ( n ) {\displaystyle O(n)} with a beam search decoder of width 10 (but they found little benefit from greater beam size and even limiting it to greedy decoding performs well), and achieves competitive performance with traditional algorithms for context-free parsing like CKY. Dependency parsing Dependency parsing is parsing according to a dependency grammar formalism, such as Universal Dependencies (which is also a project that produces multilingual dependency treebanks). This means assigning a head (or multiple heads in some formalisms like Enhanced Dependencies, e.g. in the case of coordination) to every token and a corresponding dependency relation for each edge, eventually constructing a tree or graph over the whole sentence. There are broadly three modern paradigms for modelling dependency parsing: transition-based, grammar-based, and graph-based. Transition-based Many modern approaches to dependency tree parsing use transition-based parsing (the base form of this is sometimes called arc-standard) as formulated by Joakim Nivre in 2003, which extends on shift-reduce parsing by keeping a running stack of tokens, and deciding from three operations for the next token encountered: LeftArc (current token is a child of the top of the stack, is not added to stack) RightArc (current token is the parent of the top of the stack, replaces top) Shift (add current token to the stack) The algorithm can be formulated as comparing the top two tokens of the stack (after adding the next token to the stack) or the top token on the stack and the next token in the sentence. Training data for such an algorithm is created by using an oracle, which constructs a sequence of transitions from gold trees which are then fed to a classifier. The classifier learns which of the three operations is optimal given the current state of the stack, buffer, and current token. Modern methods use a neural classifier which is trained on word embeddings, beginning with work by Danqi Chen and Christopher Manning in 2014. In the past, feature-based classifiers were also common, with features chosen from part-of-speech tags, sentence position, morphological information, etc. This is an O ( n ) {\displaystyle O(n)} greedy algorithm, so it does not guarantee the best possible parse or even a necessarily valid parse, but it is efficient. It