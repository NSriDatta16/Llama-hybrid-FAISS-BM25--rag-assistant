 be expressed as: R cal Δ ( p ; a , c ) = | det ⁡ ( R F p E ) | = | a | 1 − n ∏ i = 1 n q i p i {\displaystyle R_{\text{cal}}^{\Delta }(\mathbf {p} ;a,\mathbf {c} )=\left|\operatorname {det} (\mathbf {RF_{p}E} )\right|=\left|a\right|^{1-n}\prod _{i=1}^{n}{\frac {q_{i}}{p_{i}}}} This result can also be obtained by factoring the density of the SGB distribution, which is obtained by sending Dirichlet variates through f cal {\displaystyle f_{\text{cal}}} . While calibration transforms are most often trained as discriminative models, the reinterpretation here as a probabilistic flow allows also the design of generative calibration models based on this transform. When used for calibration, the restriction a > 0 {\displaystyle a>0} can be imposed to prevent direction reversal in log-probability space. With the additional restriction c = 0 {\displaystyle \mathbf {c} ={\boldsymbol {0}}} , this transform (with discriminative training) is known in machine learning as temperature scaling. Generalized calibration transform The above calibration transform can be generalized to f gcal : Δ n − 1 → Δ n − 1 {\displaystyle f_{\text{gcal}}:\Delta ^{n-1}\to \Delta ^{n-1}} , with parameters c ∈ R n {\displaystyle \mathbf {c} \in \mathbb {R} ^{n}} and A {\displaystyle \mathbf {A} } n -by- n {\displaystyle n{\text{-by-}}n} invertible: q = f gcal ( p ; A , c ) = softmax ⁡ ( A log ⁡ p + c ) , subject to A 1 = λ 1 {\displaystyle \mathbf {q} =f_{\text{gcal}}(\mathbf {p} ;\mathbf {A} ,\mathbf {c} )=\operatorname {softmax} (\mathbf {A} \log \mathbf {p} +\mathbf {c} )\,,\;{\text{subject to}}\;\mathbf {A1} =\lambda \mathbf {1} } where the condition that A {\displaystyle \mathbf {A} } has 1 {\displaystyle \mathbf {1} } as an eigenvector ensures invertibility by sidestepping the information loss due to the invariance: softmax ⁡ ( x + α 1 ) = softmax ⁡ ( x ) {\displaystyle \operatorname {softmax} (\mathbf {x} +\alpha \mathbf {1} )=\operatorname {softmax} (\mathbf {x} )} . Note in particular that A = λ I n {\displaystyle \mathbf {A} =\lambda \mathbf {I} _{n}} is the only allowed diagonal parametrization, in which case we recover f cal ( p ; λ − 1 , c ) {\displaystyle f_{\text{cal}}(\mathbf {p} ;\lambda ^{-1},\mathbf {c} )} , while (for n > 2 {\displaystyle n>2} ) generalization is possible with non-diagonal matrices. The inverse is: p = f gcal − 1 ( q ; A , c ) = f gcal ( q ; A − 1 , − A − 1 c ) , where A 1 = λ 1 ⟹ A − 1 1 = λ − 1 1 {\displaystyle \mathbf {p} =f_{\text{gcal}}^{-1}(\mathbf {q} ;\mathbf {A} ,\mathbf {c} )=f_{\text{gcal}}(\mathbf {q} ;\mathbf {A} ^{-1},-\mathbf {A} ^{-1}\mathbf {c} )\,,\;{\text{where}}\;\mathbf {A1} =\lambda \mathbf {1} \Longrightarrow \mathbf {A} ^{-1}\mathbf {1} =\lambda ^{-1}\mathbf {1} } The differential volume ratio is: R gcal Δ ( p ; A , c ) = | det ⁡ ( A ) | | λ | ∏ i = 1 n q i p i {\displaystyle R_{\text{gcal}}^{\Delta }(\mathbf {p} ;\mathbf {A} ,\mathbf {c} )={\frac {\left|\operatorname {det} (\mathbf {A} )\right|}{|\lambda |}}\prod _{i=1}^{n}{\frac {q_{i}}{p_{i}}}} If f gcal {\displaystyle f_{\text{gcal}}} is to be used as a calibration transform, further constraint could be imposed, for example that A {\displaystyle \mathbf {A} } be positive definite, so that ( A x ) ′ x > 0 {\displaystyle (\mathbf {Ax} )'\mathbf {x} >0} , which avoids direction reversals. (This is one possible generalization of a > 0 {\displaystyle a>0} in the f cal {\displaystyle f_{\text{cal}}} parameter.) For n = 2 {\displaystyle n=2} , a > 0 {\displaystyle a>0} and A {\displaystyle \mathbf {A} } positive definite, then f cal {\displaystyle f_{\text{cal}}} and f gcal {\displaystyle f_{\text{gcal}}} are equivalent in the sense that in both cases, log ⁡ p 1 p 2 ↦ log ⁡ q 1 q 2 {\displaystyle \log {\frac {p_{1}}{p_{2}}}\mapsto \log {\frac {q_{1}}{q_{2}}}} is a straight line, the (positive) slope and offset of which are functions of the transform parameters. For n > 2 , {\displaystyle n>2,} f gcal {\displaystyle f_{\text{gcal}}} does generalize f cal {\d