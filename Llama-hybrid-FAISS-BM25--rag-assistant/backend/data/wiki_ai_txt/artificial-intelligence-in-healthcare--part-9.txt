mproving reliability, tighter human oversight, and domain-specific models. Peer-reviewed work cautioned that headline “benchmark gains” can overstate clinical value critiquing evaluation design and urging real-world assessments. While new studies showed LLMs still prioritize helpfulness over accuracy in medical reasoning, reinforcing the need for guardrails and clinician review. Alongside generalist systems, niche models and data platforms expanded rapidly: HistAI introduced foundation models for pathology and released a landmark open dataset of 112,000 whole-slide images across 46,000 cases to spur reproducible diagnostics, and announced a CELLDX Data Hub with over 100,000 diagnostic-quality WSIs and standardized metadata. Clinical and translational activity in pathology also accelerated, from Stanford Medicine’s Nuclei.io tool to speed case review and collaboration, to broader surveys arguing that digital pathology plus AI can improve diagnostic precision while noting adoption barriers. In patient-facing support, Healz.ai emphasized doctor-validated answers and added multilingual medical-document recognition, reflecting a trend toward combining LLMs with structured health records and rapid clinician review. Upstream data innovation also grew, with Louiza Labs Y Combinator building synthetic biomedical datasets to safely scale training and evaluation. Beyond triage and documentation, generative-model architectures akin to LLMs powered population-scale risk prediction e.g., Delphi-2M forecasting susceptibility to 1,000 diseases across international cohorts hinting at preventive-care applications pending validation and governance. Together, these advances portrayed 2025 as a year of cautious progress: better datasets and specialty models, promising accuracy on constrained tasks, and growing evidence that rigorous evaluation, transparency, and human-in-the-loop workflows remain essential for safe clinical use. Expanding care to developing nations Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public. Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before. With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not. Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient. The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries. In global health, the adoption of AI introduces significant equity challenges. One concern is the emergence of an "AI divide", where high-cost, advanced AI systems may remain concentrated in Global North institutions, reinforcing existing power imbalances. Additionally, a "transparency paradox" has been identified within the punitive accountability structures of global health: practitioners in resource-constrained settings who disclose using AI may risk having their work devalued, which creates incentives against transparency. As a counter-measure, a "framework for Artificial Intelligence as co-worker" has been proposed, which conceptualizes AI as a "partner" rather than a simple tool. This partnership model emphasizes integrating AI into peer learning networks to enhance collective problem-solving, support local leadership, and maintain human agency in decision-making. Regulation Challenges of