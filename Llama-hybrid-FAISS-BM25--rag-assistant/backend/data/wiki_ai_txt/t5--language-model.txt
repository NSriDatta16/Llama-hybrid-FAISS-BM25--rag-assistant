T5 (Text-to-Text Transfer Transformer) is a series of large language models developed by Google AI introduced in 2019. Like the original Transformer model, T5 models are encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text. T5 models are usually pretrained on a massive dataset of text and code, after which they can perform the text-based tasks that are similar to their pretrained tasks. They can also be finetuned to perform other tasks. T5 models have been employed in various applications, including chatbots, machine translation systems, text summarization tools, code generation, and robotics. Training The original T5 models are pre-trained on the Colossal Clean Crawled Corpus (C4), containing text and code scraped from the internet. This pre-training process enables the models to learn general language understanding and generation abilities. T5 models can then be fine-tuned on specific downstream tasks, adapting their knowledge to perform well in various applications. The T5 models were pretrained on many tasks, all in the format of <input text> -> <output text>. Some examples are: restoring corrupted text: Thank you <X> me to your party <Y> week. -> <X> for inviting <Y> last <Z>, where the <Z> means "end of output", and the <X> and <Y> denote blanks to be filled, called "sentinels" in the original report. translation: translate English to German: That is good. -> Das ist gut.. judging the grammatical acceptability of a sentence (CoLA sentence): The course is jumping well. -> not acceptable . Architecture The T5 series encompasses several models with varying sizes and capabilities, all encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text. These models are often distinguished by their parameter count, which indicates the complexity and potential capacity of the model. The original paper reported the following 5 models: *The encoder and the decoder have the same shape. So for example, the T5-small has 6 layers in the encoder and 6 layers in the decoder. In the above table, n layer {\displaystyle n_{\text{layer}}} : Number of layers in the encoder; also, number of layers in the decoder. They always have the same number of layers. n head {\displaystyle n_{\text{head}}} : Number of attention heads in each attention block. d model {\displaystyle d_{\text{model}}} : Dimension of the embedding vectors. d ff {\displaystyle d_{\text{ff}}} : Dimension of the feedforward network within each encoder and decoder layer. d kv {\displaystyle d_{\text{kv}}} : Dimension of the key and value vectors used in the self-attention mechanism. Note that unlike typical Transformers, the 3B and 11B models do not satisfy d model = d kv n head {\displaystyle d_{\text{model}}=d_{\text{kv}}n_{\text{head}}} . Compared to the original Transformer, it uses a few minor modifications: layer normalization with no additive bias; placing the layer normalization outside the residual path; relative positional embedding. For all experiments, they used a WordPiece tokenizer, with vocabulary size 32,000. The tokenizer is shared across both the input and output of each model. It was trained on a mixture of English, German, French, and Romanian data from the C4 dataset, at a ratio of 10:1:1:1. Variants Several subsequent models used the T5 architecture, with non-standardized naming conventions used to differentiate them. This section attempts to collect the main ones. An exhaustive list of the variants released by Google Brain is on the GitHub repo for T5X. Some models are trained from scratch while others are trained by starting with a previous trained model. By default, each model is trained from scratch, except otherwise noted. T5 small, base, large, 3B, 11B (2019): The original models. T5 1.1 small, base, large, XL, XXL: Improved versions of the original T5 series. These have roughly equal parameters. The activation function is GEGLU instead of 