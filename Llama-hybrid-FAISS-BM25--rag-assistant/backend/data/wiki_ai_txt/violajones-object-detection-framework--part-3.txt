hus, the object detection framework employs a variant of the learning algorithm AdaBoost to both select the best features and to train classifiers that use them. This algorithm constructs a "strong" classifier as a linear combination of weighted simple “weak” classifiers. h ( x ) = sgn ⁡ ( ∑ j = 1 M α j h j ( x ) ) {\displaystyle h(\mathbf {x} )=\operatorname {sgn} \left(\sum _{j=1}^{M}\alpha _{j}h_{j}(\mathbf {x} )\right)} Each weak classifier is a threshold function based on the feature f j {\displaystyle f_{j}} . h j ( x ) = { − s j if f j < θ j s j otherwise {\displaystyle h_{j}(\mathbf {x} )={\begin{cases}-s_{j}&{\text{if }}f_{j}<\theta _{j}\\s_{j}&{\text{otherwise}}\end{cases}}} The threshold value θ j {\displaystyle \theta _{j}} and the polarity s j ∈ ± 1 {\displaystyle s_{j}\in \pm 1} are determined in the training, as well as the coefficients α j {\displaystyle \alpha _{j}} . Here a simplified version of the learning algorithm is reported: Input: Set of N positive and negative training images with their labels ( x i , y i ) {\displaystyle {(\mathbf {x} ^{i},y^{i})}} . If image i is a face y i = 1 {\displaystyle y^{i}=1} , if not y i = − 1 {\displaystyle y^{i}=-1} . Initialization: assign a weight w 1 i = 1 N {\displaystyle w_{1}^{i}={\frac {1}{N}}} to each image i. For each feature f j {\displaystyle f_{j}} with j = 1 , . . . , M {\displaystyle j=1,...,M} Renormalize the weights such that they sum to one. Apply the feature to each image in the training set, then find the optimal threshold and polarity θ j , s j {\displaystyle \theta _{j},s_{j}} that minimizes the weighted classification error. That is θ j , s j = arg ⁡ min θ , s ∑ i = 1 N w j i ε j i {\displaystyle \theta _{j},s_{j}=\arg \min _{\theta ,s}\;\sum _{i=1}^{N}w_{j}^{i}\varepsilon _{j}^{i}} where ε j i = { 0 if y i = h j ( x i , θ j , s j ) 1 otherwise {\displaystyle \varepsilon _{j}^{i}={\begin{cases}0&{\text{if }}y^{i}=h_{j}(\mathbf {x} ^{i},\theta _{j},s_{j})\\1&{\text{otherwise}}\end{cases}}} Assign a weight α j {\displaystyle \alpha _{j}} to h j {\displaystyle h_{j}} that is inversely proportional to the error rate. In this way best classifiers are considered more. The weights for the next iteration, i.e. w j + 1 i {\displaystyle w_{j+1}^{i}} , are reduced for the images i that were correctly classified. Set the final classifier to h ( x ) = sgn ⁡ ( ∑ j = 1 M α j h j ( x ) ) {\displaystyle h(\mathbf {x} )=\operatorname {sgn} \left(\sum _{j=1}^{M}\alpha _{j}h_{j}(\mathbf {x} )\right)} Cascade architecture On average only 0.01% of all sub-windows are positive (faces) Equal computation time is spent on all sub-windows Must spend most time only on potentially positive sub-windows. A simple 2-feature classifier can achieve almost 100% detection rate with 50% FP rate. That classifier can act as a 1st layer of a series to filter out most negative windows 2nd layer with 10 features can tackle “harder” negative-windows which survived the 1st layer, and so on... A cascade of gradually more complex classifiers achieves even better detection rates. The evaluation of the strong classifiers generated by the learning process can be done quickly, but it isn't fast enough to run in real-time. For this reason, the strong classifiers are arranged in a cascade in order of complexity, where each successive classifier is trained only on those selected samples which pass through the preceding classifiers. If at any stage in the cascade a classifier rejects the sub-window under inspection, no further processing is performed and continue on searching the next sub-window. The cascade therefore has the form of a degenerate tree. In the case of faces, the first classifier in the cascade – called the attentional operator – uses only two features to achieve a false negative rate of approximately 0% and a false positive rate of 40%. The effect of this single classifier is to reduce by roughly half the number of times the entire cascade is evaluated. In cascading, each stage consists