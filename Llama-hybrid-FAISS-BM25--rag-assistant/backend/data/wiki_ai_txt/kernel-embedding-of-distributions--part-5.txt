el k ( x , x ′ ) = h ( x − x ′ ) {\displaystyle k(x,x')=h(x-x')} with x ∈ R b {\displaystyle x\in \mathbb {R} ^{b}} . Then Bochner's theorem guarantees the existence of a unique finite Borel measure μ {\displaystyle \mu } (called the spectral measure) on R b {\displaystyle \mathbb {R} ^{b}} such that h ( t ) = ∫ R b e − i ⟨ t , ω ⟩ d μ ( ω ) , ∀ t ∈ R b . {\displaystyle h(t)=\int _{\mathbb {R} ^{b}}e^{-i\langle t,\omega \rangle }d\mu (\omega ),\quad \forall t\in \mathbb {R} ^{b}.} For k {\displaystyle k} to be universal it suffices that the continuous part of μ {\displaystyle \mu } in its unique Lebesgue decomposition μ = μ c + μ s {\displaystyle \mu =\mu _{c}+\mu _{s}} is non-zero. Furthermore, if d μ c ( ω ) = s ( ω ) d ω , {\displaystyle d\mu _{c}(\omega )=s(\omega )d\omega ,} then s {\displaystyle s} is the spectral density of frequencies ω {\displaystyle \omega } in R b {\displaystyle \mathbb {R} ^{b}} and h {\displaystyle h} is the Fourier transform of s {\displaystyle s} . If the support of μ {\displaystyle \mu } is all of R b {\displaystyle \mathbb {R} ^{b}} , then k {\displaystyle k} is a characteristic kernel as well. If k {\displaystyle k} induces a strictly positive definite kernel matrix for any set of distinct points, then it is a universal kernel. For example, the widely used Gaussian RBF kernel k ( x , x ′ ) = exp ⁡ ( − 1 2 σ 2 ‖ x − x ′ ‖ 2 ) {\displaystyle k(x,x')=\exp \left(-{\frac {1}{2\sigma ^{2}}}\|x-x'\|^{2}\right)} on compact subsets of R b {\displaystyle \mathbb {R} ^{b}} is universal. Parameter selection for conditional distribution kernel embeddings The empirical kernel conditional distribution embedding operator C ^ Y | X {\displaystyle {\widehat {\mathcal {C}}}_{Y|X}} can alternatively be viewed as the solution of the following regularized least squares (function-valued) regression problem min C : H → H ∑ i = 1 n ‖ φ ( y i ) − C φ ( x i ) ‖ H 2 + λ ‖ C ‖ H S 2 {\displaystyle \min _{{\mathcal {C}}:{\mathcal {H}}\to {\mathcal {H}}}\sum _{i=1}^{n}\left\|\varphi (y_{i})-{\mathcal {C}}\varphi (x_{i})\right\|_{\mathcal {H}}^{2}+\lambda \|{\mathcal {C}}\|_{HS}^{2}} where ‖ ⋅ ‖ H S {\displaystyle \|\cdot \|_{HS}} is the Hilbert–Schmidt norm. One can thus select the regularization parameter λ {\displaystyle \lambda } by performing cross-validation based on the squared loss function of the regression problem. Rules of probability as operations in the RKHS This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al. The following notation is adopted: P ( X , Y ) = {\displaystyle P(X,Y)=} joint distribution over random variables X , Y {\displaystyle X,Y} P ( X ) = ∫ Ω P ( X , d y ) = {\displaystyle P(X)=\int _{\Omega }P(X,\mathrm {d} y)=} marginal distribution of X {\displaystyle X} ; P ( Y ) = {\displaystyle P(Y)=} marginal distribution of Y {\displaystyle Y} P ( Y ∣ X ) = P ( X , Y ) P ( X ) = {\displaystyle P(Y\mid X)={\frac {P(X,Y)}{P(X)}}=} conditional distribution of Y {\displaystyle Y} given X {\displaystyle X} with corresponding conditional embedding operator C Y ∣ X {\displaystyle {\mathcal {C}}_{Y\mid X}} π ( Y ) = {\displaystyle \pi (Y)=} prior distribution over Y {\displaystyle Y} Q {\displaystyle Q} is used to distinguish distributions which incorporate the prior from distributions P {\displaystyle P} which do not rely on the prior In practice, all embeddings are empirically estimated from data { ( x 1 , y 1 ) , … , ( x n , y n ) } {\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\}} and it assumed that a set of samples { y ~ 1 , … , y ~ n ~ } {\displaystyle \{{\widetilde {y}}_{1},\ldots ,{\widetilde {y}}_{\widetilde {n}}\}} may be used to estimate the kernel embedding of the prior distribution π ( Y ) {\displaystyle \pi (Y)} . Kernel sum rule In probability theory, the marginal distribution of X {\displaystyle X} can be computed by integrating out Y {\displayst