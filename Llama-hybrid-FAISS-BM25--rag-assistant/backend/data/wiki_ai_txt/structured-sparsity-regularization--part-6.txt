cluding the following: Data fusion: When each kernel corresponds to a different kind of modality/feature. Nonlinear variable selection: Consider kernels K g {\displaystyle K_{g}} depending only one dimension of the input. Generally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important. Additional uses and applications Structured sparsity regularization methods have been used in a number of settings where it is desired to impose an a priori input variable structure to the regularization process. Some such applications are: Compressive sensing in magnetic resonance imaging (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time Robust face recognition in the presence of misalignment, occlusion and illumination variation Uncovering socio-linguistic associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets See also Statistical learning theory Regularization Sparse approximation Proximal gradient methods Convex analysis Feature selection == References ==