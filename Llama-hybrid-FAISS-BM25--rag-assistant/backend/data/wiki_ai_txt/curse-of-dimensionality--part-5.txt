 the average. This phenomenon can have a considerable impact on various techniques for classification (including the k-NN classifier), semi-supervised learning, and clustering, and it also affects information retrieval. Anomaly detection In a 2012 survey, Zimek et al. identified the following problems when searching for anomalies in high-dimensional data: Concentration of scores and distances: derived values such as distances become numerically similar Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant Definition of reference sets: for local methods, reference sets are often nearest-neighbor based Incomparable scores for different dimensionalities: different subspaces produce incomparable scores Interpretability of scores: the scores often no longer convey a semantic meaning Exponential search space: the search space can no longer be systematically scanned Data snooping bias: given the large search space, for every desired significance a hypothesis can be found Hubness: certain objects occur more frequently in neighbor lists than others. Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions. Blessing of dimensionality Despite the expected "curse of dimensionality" difficulties, common-sense heuristics based on the most straightforward methods "can yield results which are almost surely optimal" for high-dimensional problems. The term "blessing of dimensionality" was introduced in the late 1990s. Donoho in his "Millennium manifesto" explained why he thinks the "blessing of dimensionality" will form a basis of future data mining. The effects of the blessing of dimensionality were discovered in many applications and found their foundation in the concentration of measure phenomena. One example of the blessing of dimensionality phenomenon is linear separability of a random point from a large finite random set with high probability even if this set is exponentially large: the number of elements in this random set can grow exponentially with dimension. Moreover, this linear functional can be selected in the form of the simplest linear Fisher discriminant. This separability theorem was proven for a wide class of probability distributions: general uniformly log-concave distributions, product distributions in a cube and many other families (reviewed recently in ). "The blessing of dimensionality and the curse of dimensionality are two sides of the same coin." For example, the typical property of essentially high-dimensional probability distributions in a high-dimensional space is: the squared distance of random points to a selected point is, with high probability, close to the average (or median) squared distance. This property significantly simplifies the expected geometry of data and indexing of high-dimensional data (blessing), but, at the same time, it makes the similarity search in high dimensions difficult and even useless (curse). Zimek et al. noted that while the typical formalizations of the curse of dimensionality affect i.i.d. data, having data that is separated in each attribute becomes easier even in high dimensions, and argued that the signal-to-noise ratio matters: data becomes easier with each attribute that adds signal, and harder with attributes that only add noise (irrelevant error) to the data. In particular for unsupervised data analysis this effect is known as swamping. See also == References ==