r all of the data. In addition to the standard hinge loss ( 1 − y f ( x ) ) + {\displaystyle (1-yf(x))_{+}} for labeled data, a loss function ( 1 − | f ( x ) | ) + {\displaystyle (1-|f(x)|)_{+}} is introduced over the unlabeled data by letting y = sign ⁡ f ( x ) {\displaystyle y=\operatorname {sign} {f(x)}} . TSVM then selects f ∗ ( x ) = h ∗ ( x ) + b {\displaystyle f^{*}(x)=h^{*}(x)+b} from a reproducing kernel Hilbert space H {\displaystyle {\mathcal {H}}} by minimizing the regularized empirical risk: f ∗ = argmin f ( ∑ i = 1 l ( 1 − y i f ( x i ) ) + + λ 1 ‖ h ‖ H 2 + λ 2 ∑ i = l + 1 l + u ( 1 − | f ( x i ) | ) + ) {\displaystyle f^{*}={\underset {f}{\operatorname {argmin} }}\left(\displaystyle \sum _{i=1}^{l}(1-y_{i}f(x_{i}))_{+}+\lambda _{1}\|h\|_{\mathcal {H}}^{2}+\lambda _{2}\sum _{i=l+1}^{l+u}(1-|f(x_{i})|)_{+}\right)} An exact solution is intractable due to the non-convex term ( 1 − | f ( x ) | ) + {\displaystyle (1-|f(x)|)_{+}} , so research focuses on useful approximations. Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case). Laplacian regularization Laplacian regularization has been historically approached through graph-Laplacian. Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its k {\displaystyle k} nearest neighbors or to examples within some distance ϵ {\displaystyle \epsilon } . The weight W i j {\displaystyle W_{ij}} of an edge between x i {\displaystyle x_{i}} and x j {\displaystyle x_{j}} is then set to e − ‖ x i − x j ‖ 2 / ϵ 2 {\displaystyle e^{-\|x_{i}-x_{j}\|^{2}/\epsilon ^{2}}} . Within the framework of manifold regularization, the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes argmin f ∈ H ( 1 l ∑ i = 1 l V ( f ( x i ) , y i ) + λ A ‖ f ‖ H 2 + λ I ∫ M ‖ ∇ M f ( x ) ‖ 2 d p ( x ) ) {\displaystyle {\underset {f\in {\mathcal {H}}}{\operatorname {argmin} }}\left({\frac {1}{l}}\displaystyle \sum _{i=1}^{l}V(f(x_{i}),y_{i})+\lambda _{A}\|f\|_{\mathcal {H}}^{2}+\lambda _{I}\int _{\mathcal {M}}\|\nabla _{\mathcal {M}}f(x)\|^{2}dp(x)\right)} where H {\displaystyle {\mathcal {H}}} is a reproducing kernel Hilbert space and M {\displaystyle {\mathcal {M}}} is the manifold on which the data lie. The regularization parameters λ A {\displaystyle \lambda _{A}} and λ I {\displaystyle \lambda _{I}} control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian L = D − W {\displaystyle L=D-W} where D i i = ∑ j = 1 l + u W i j {\displaystyle D_{ii}=\sum _{j=1}^{l+u}W_{ij}} and f {\displaystyle \mathbf {f} } is the vector [ f ( x 1 ) … f ( x l + u ) ] {\displaystyle [f(x_{1})\dots f(x_{l+u})]} , we have f T L f = ∑ i , j = 1 l + u W i j ( f i − f j ) 2 ≈ ∫ M ‖ ∇ M f ( x ) ‖ 2 d p ( x ) {\displaystyle \mathbf {f} ^{T}L\mathbf {f} =\displaystyle \sum _{i,j=1}^{l+u}W_{ij}(f_{i}-f_{j})^{2}\approx \int _{\mathcal {M}}\|\nabla _{\mathcal {M}}f(x)\|^{2}dp(x)} . The graph-based approach to Laplacian regularization is to put in relation with finite difference method. The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM. Heuristic approaches Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of un