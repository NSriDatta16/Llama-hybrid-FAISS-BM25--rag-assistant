ith Q {\displaystyle Q} and V {\displaystyle V} computed, the advantage function is calculated by subtracting the baseline estimate from the actual discounted return. If A > 0 {\displaystyle A>0} , the actual return of the action is better than the expected return from experience; if A < 0 {\displaystyle A<0} , the actual return is worse. Ratio function In PPO, the ratio function ( r t {\displaystyle r_{t}} ) calculates the probability of selecting action a {\displaystyle a} in state s {\displaystyle s} given the current policy network, divided by the previous probability under the old policy. In other words: If r t ( θ ) > 1 {\displaystyle r_{t}(\theta )>1} , where θ {\displaystyle \theta } are the policy network parameters, then selecting action a {\displaystyle a} in state s {\displaystyle s} is more likely based on the current policy than the previous policy. If 0 ≤ r t ( θ ) < 1 {\displaystyle 0\leq r_{t}(\theta )<1} , then selecting action a {\displaystyle a} in state s {\displaystyle s} is less likely based on the current policy than the old policy. Hence, this ratio function can easily estimate the divergence between old and current policies. PPO objective function The objective function of PPO takes the expectation operator (denoted as E {\displaystyle E} ) which means that this function will be computed over quantities of trajectories. The expectation operator takes the minimum of two terms: r t ( θ ) ⋅ A {\displaystyle r_{t}(\theta )\cdot A} : this is the product of the ratio function and the advantage function introduced in TRPO, also known as the normal policy gradient objective. clip ⁡ ( r t ( θ ) ) ⋅ A {\displaystyle \operatorname {clip} (r_{t}(\theta ))\cdot A} : the policy ratio is first clipped to the range [ 1 − ϵ , 1 + ϵ ] {\displaystyle [1-\epsilon ,1+\epsilon ]} ; generally, ϵ {\displaystyle \epsilon } is defined to be 0.2. Then, as before, it is multiplied by the advantage. The fundamental intuition behind PPO is the same as that of TRPO: conservatism. Clipping results in a conservative advantage estimate of the new policy. The reasoning is that if an agent makes significant changes due to high advantage estimates, its policy update will be large and unstable, and may diverge from the optimal policy with little possibility of recovery. There are two common applications of the clipping function: when an action under a new policy happens to be a good choice based on the advantage function, the clipping function limits how much credit can be given to a new policy for up-weighted good actions. On the other hand, when an action under the old policy is judged to be bad, the clipping function constrains how much the agent can accept the down-weighted bad actions of the new policy. Consequently, the clipping mechanism is designed to discourage the incentive of moving beyond the defined range by clipping both directions. The advantage of this method is that it can be optimized directly with gradient descent, as opposed to the strict KL divergence constraint of TRPO, making the implementation faster and more intuitive. After computing the clipped surrogate objective function, the agent has two probability ratios: one non-clipped and one clipped. Then, by taking the minimum of the two objectives, the final objective becomes a lower bound (a pessimistic bound) of what the agent knows is possible. In other words, the minimum method makes sure that the agent is doing the safest possible update. Advantages Simplicity PPO approximates what TRPO does, with considerably less computation. It uses first-order optimization (the clip function) to constrain the policy update, while TRPO uses KL divergence constraints (second-order optimization). Compared to TRPO, the PPO method is relatively easy to implement and requires less computational resource and time. Therefore, it is cheaper and more efficient to use PPO in large-scale problems. Stability While other RL algorithms require hyperparameter tuning, PPO comparatively does 