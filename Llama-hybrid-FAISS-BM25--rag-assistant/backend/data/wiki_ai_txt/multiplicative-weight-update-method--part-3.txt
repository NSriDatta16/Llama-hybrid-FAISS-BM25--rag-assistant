n(N)} where α β = ln ⁡ ( 1 β ) 1 − β {\displaystyle \alpha _{\beta }={\frac {\ln({\frac {1}{\beta }})}{1-\beta }}} and c β = 1 1 − β {\displaystyle c_{\beta }={\frac {1}{1-\beta }}} . Note that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction. In this randomized algorithm, α β → 1 {\displaystyle \alpha _{\beta }\rightarrow 1} if β → 1 {\displaystyle \beta \rightarrow 1} . Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define η = 1 / 2 {\displaystyle \eta =1/2} in weighted majority algorithm and allow 0 ≤ η ≤ 1 {\displaystyle 0\leq \eta \leq 1} in randomized weighted majority algorithm. Applications The multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event. Solving zero-sum games approximately (Oracle algorithm) Source: Suppose we were given the distribution P {\displaystyle P} on experts. Let A {\displaystyle A} = payoff matrix of a finite two-player zero-sum game, with n {\displaystyle n} rows. When the row player p r {\displaystyle p_{r}} uses plan i {\displaystyle i} and the column player p c {\displaystyle p_{c}} uses plan j {\displaystyle j} , the payoff of player p c {\displaystyle p_{c}} is A ( i , j ) {\displaystyle A\left(i,j\right)} ≔ A i j {\displaystyle A_{ij}} , assuming A ( i , j ) ∈ [ 0 , 1 ] {\displaystyle A\left(i,j\right)\in \left[0,1\right]} . If player p r {\displaystyle p_{r}} chooses action i {\displaystyle i} from a distribution P {\displaystyle P} over the rows, then the expected result for player p c {\displaystyle p_{c}} selecting action j {\displaystyle j} is A ( P , j ) = E i ∈ P [ A ( i , j ) ] {\displaystyle A\left(P,j\right)=E_{i\in P}\left[A\left(i,j\right)\right]} . To maximize A ( P , j ) {\displaystyle A\left(P,j\right)} , player p c {\displaystyle p_{c}} should choose plan j {\displaystyle j} . Similarly, the expected payoff for player p l {\displaystyle p_{l}} is A ( i , P ) = E j ∈ P [ A ( i , j ) ] {\displaystyle A\left(i,P\right)=E_{j\in P}\left[A\left(i,j\right)\right]} . Choosing plan i {\displaystyle i} would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain: min P max j A ( P , j ) = max Q min i A ( i , Q ) {\displaystyle \min _{P}\max _{j}A\left(P,j\right)=\max _{Q}\min _{i}A\left(i,Q\right)} where P and i changes over the distributions over rows, Q and j changes over the columns. Then, let λ ∗ {\displaystyle \lambda ^{*}} denote the common value of above quantities, also named as the "value of the game". Let δ > 0 {\displaystyle \delta >0} be an error parameter. To solve the zero-sum game bounded by additive error of δ {\displaystyle \delta } , λ ∗ − δ ≤ min i A ( i , q ) {\displaystyle \lambda ^{*}-\delta \leq \min _{i}A\left(i,q\right)} max j A ( p , j ) ≤ λ ∗ + δ {\displaystyle \max _{j}A\left(p,j\right)\leq \lambda ^{*}+\delta } So there is an algorithm solving zero-sum game up to an additive factor of δ using O(log2(n)/ δ 2 {\displaystyle \delta ^{2}} ) calls to ORACLE, with an additional processing time of O(n) per call Bailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it. Machine learning In machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also e