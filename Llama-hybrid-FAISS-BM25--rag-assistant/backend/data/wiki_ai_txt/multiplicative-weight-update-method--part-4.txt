mployed the Multiplicative Weight Update Method. Winnow algorithm Based on current knowledge in algorithms, the multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program. Given m {\displaystyle m} labeled examples ( a 1 , l 1 ) , … , ( a m , l m ) {\displaystyle \left(a_{1},l_{1}\right),{\text{…}},\left(a_{m},l_{m}\right)} where a j ∈ R n {\displaystyle a_{j}\in \mathbb {R} ^{n}} are feature vectors, and l j ∈ { − 1 , 1 } {\displaystyle l_{j}\in \left\{-1,1\right\}\quad } are their labels. The aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that l j a j x ≥ 0 {\displaystyle l_{j}a_{j}x\geq 0} for all j {\displaystyle j} . Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine a j {\displaystyle a_{j}} to be l j a j {\displaystyle l_{j}a_{j}} , the problem reduces to finding a solution to the following LP: ∀ j = 1 , 2 , … , m : a j x ≥ 0 {\displaystyle \forall j=1,2,{\text{…}},m:a_{j}x\geq 0} , 1 ∗ x = 1 {\displaystyle 1*x=1} , ∀ i : x i ≥ 0 {\displaystyle \forall i:x_{i}\geq 0} . This is general form of LP. Hedge algorithm Source: The hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different. It is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update. Analysis Assume the learning rate η > 0 {\displaystyle \eta >0} and for t ∈ [ T ] {\displaystyle t\in [T]} , p t {\displaystyle p^{t}} is picked by Hedge. Then for all experts i {\displaystyle i} , ∑ t ≤ T p t m t ≤ ∑ t ≤ T m i t + ln ⁡ ( N ) η + η T {\displaystyle \sum _{t\leq T}p^{t}m^{t}\leq \sum _{t\leq T}m_{i}^{t}+{\frac {\ln(N)}{\eta }}+\eta T} Initialization: Fix an η > 0 {\displaystyle \eta >0} . For each expert, associate the weight w i 1 {\displaystyle w_{i}^{1}} ≔1 For t=1,2,...,T: 1. Pick the distribution p i t = w i t Φ t {\displaystyle p_{i}^{t}={\frac {w_{i}^{t}}{\Phi t}}} where Φ t = ∑ i w i t {\displaystyle \Phi t=\sum _{i}w_{i}^{t}} . 2. Observe the cost of the decision m t {\displaystyle m^{t}} . 3. Set w i t + 1 = w i t exp ⁡ ( − η m i t {\displaystyle w_{i}^{t+1}=w_{i}^{t}\exp(-\eta m_{i}^{t}} ). AdaBoost algorithm This algorithm maintains a set of weights w t {\displaystyle w^{t}} over the training examples. On every iteration t {\displaystyle t} , a distribution p t {\displaystyle p^{t}} is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis h t {\displaystyle h_{t}} that (hopefully) has small error with respect to the distribution. Using the new hypothesis h t {\displaystyle h_{t}} , AdaBoost generates the next weight vector w t + 1 {\displaystyle w^{t+1}} . The process repeats. After T such iterations, the final hypothesis h f {\displaystyle h_{f}} is the output. The hypothesis h f {\displaystyle h_{f}} combines the outputs of the T weak hypotheses using a weighted majority vote. Input: Sequence of N {\displaystyle N} labeled examples ( x 1 {\displaystyle x_{1}} , y 1 {\displaystyle y_{1}} ),...,( x N {\displaystyle x_{N}} , y N {\displaystyle y_{N}} ) Distribution D {\displaystyle D} over the N {\displaystyle N} examples Weak learning algorithm "'WeakLearn"' Integer T {\displaystyle T} specifying number of iterations Initialize the weight vector: w i 1 = D ( i ) {\displaystyle w_{i}^{1}=D(i)} for i = 1 , 2 , . . . , N {\displaystyle i=1,2,...,N} . Do for t = 1 , 2 , . . . , T {\displayst