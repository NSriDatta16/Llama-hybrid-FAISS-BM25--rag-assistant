ed with a placeholder, creating a cloze-style question. The goal is to identify the masked entity from the article. SWAG (Situations With Adversarial Generations): 113K descriptions of activities or events, each with 4 candidate endings; the model must choose the most plausible ending. Adversarial against a few shallow language models (MLP, bag of words, one-layer CNN, etc). HellaSwag (Harder Endings, Longer contexts, and Low-shot Activities for SWAG): A harder version of SWAG. Contains 10K items. RACE (ReAding Comprehension Examinations): 100,000 reading comprehension problems in 28,000 passages, collected from the English exams for middle and high school Chinese students in the age range between 12 and 18. LAMBADA: 10,000 narrative passages from books, each with a missing last word that humans can guess if given the full passage but not from the last sentence alone. General language generation NaturalInstructions: 61 distinct tasks with human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Super-NaturalInstructions: 1,616 diverse NLP tasks and their expert-written instructions, and 5M task instances. IFEval (Instruction-Following Eval): 541 instructions to be followed, each containing at least one verifiable constraint, such as "mention the keyword of AI at least 3 times". LMArena (formerly Chatbot Arena): Human users vote between two outputs from two language models. An Elo rating for each language model is computed based on these human votes. MT-Bench (multi-turn benchmark): An automated version of Chatbot Arena where LLMs replace humans in generating votes. MultiChallenge: 273 instances. Each instance is a multi-turn (up to 10 turns) conversation history between two parties, ending with a final user turn containing a requirement/question. Designed to test for instruction-following, context allocation, and in-context reasoning at the same time. Scored by LLM as judge with instance-level rubrics. CharXiv: 9292 descriptive questions (examining basic chart elements) and 2323 reasoning questions (synthesizing information across complex visual elements) about 2323 charts from scientific papers. Open-book question-answering MCTest (Machine Comprehension Test): 500 fictional stories, each with 4 multiple-choice questions (with at least 2 requiring multi-sentence understanding), designed to be understandable by a 7-year-old. The vocabulary was limited to approximately 8,000 words probably known by a 7-year-old. The stories were written by workers on Amazon Mechanical Turk. SQuAD (Stanford Question Answering Dataset): 100,000+ questions posed by crowd workers on 500+ Wikipedia articles. The task is, given a passage from Wikipedia and a question, find a span of text in the text that answers the question. SQuAD 2.0: 50,000 unanswerable questions that look similar to SQuAD questions. Every such unanswerable question must be answered with an empty string. Written by crowd workers. ARC (AI2 Reasoning Challenge): Multiple choice questions, with a Challenge Set (2590 questions) and an Easy Set (5197 questions). Designed specifically to be adversarial against models that had saturated SNLI and SQuAD. CoQA (Conversational QA): 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. WebQuestions: 6,642 question-answer pairs designed to be answerable with knowledge present in the 2013 version of Freebase. Natural Questions: 323045 items. Each containing a question that had been searched on Google, a Wikipedia page relevant for answering the question, a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or "null" if no long/short answer is present. TriviaQA: 650K question-answer-evidence triples. Includes 95K question-answer pairs scraped from 14 trivia and quiz-league websites, and (on averag