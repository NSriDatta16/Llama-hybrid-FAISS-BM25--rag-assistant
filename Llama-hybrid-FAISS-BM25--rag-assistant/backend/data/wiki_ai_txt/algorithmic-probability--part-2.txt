osely related to the concept of Kolmogorov complexity. Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes's rule was invented by Solomonoff with Kolmogorov complexity as a side product. It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be. Solomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. When run for longer and longer periods of time, it will generate a sequence of approximations which converge to the universal probability distribution. Other methods of dealing with the issue include limiting the search space by including training sequences. Solomonoff proved this distribution to be machine-invariant within a constant factor (called the invariance theorem). Fundamental Theorems I. Kolmogorov's Invariance Theorem Kolmogorov's Invariance theorem clarifies that the Kolmogorov Complexity, or Minimal Description Length, of a dataset is invariant to the choice of Turing-Complete language used to simulate a Universal Turing Machine: ∀ x ∈ { 0 , 1 } ∗ , | K U ( x ) − K U ′ ( x ) | ≤ O ( 1 ) {\displaystyle \forall x\in \{0,1\}^{*},|K_{U}(x)-K_{U'}(x)|\leq {\mathcal {O}}(1)} where K U ( x ) = min p { | p | : U ( p ) = x } {\displaystyle K_{U}(x)=\min _{p}\{|p|:U(p)=x\}} . Interpretation The minimal description p {\displaystyle p} such that U ( p ) = x {\displaystyle U(p)=x} serves as a natural representation of the string x {\displaystyle x} relative to the Turing-Complete language U {\displaystyle U} . Moreover, as x {\displaystyle x} can't be compressed further p {\displaystyle p} is an incompressible and hence uncomputable string. This corresponds to a scientists' notion of randomness and clarifies the reason why Kolmogorov Complexity is not computable. It follows that any piece of data has a necessary and sufficient representation in terms of a random string. Proof The following is taken from From the theory of compilers, it is known that for any two Turing-Complete languages U 1 {\displaystyle U_{1}} and U 2 {\displaystyle U_{2}} , there exists a compiler Λ 1 {\displaystyle \Lambda _{1}} expressed in U 1 {\displaystyle U_{1}} that translates programs expressed in U 2 {\displaystyle U_{2}} into functionally-equivalent programs expressed in U 1 {\displaystyle U_{1}} . It follows that if we let p {\displaystyle p} be the shortest program that prints a given string x {\displaystyle x} then: K U 1 ( x ) ≤ | Λ 1 | + | p | ≤ K U 2 ( x ) + O ( 1 ) {\displaystyle K_{U_{1}}(x)\leq |\Lambda _{1}|+|p|\leq K_{U_{2}}(x)+{\mathcal {O}}(1)} where | Λ 1 | = O ( 1 ) {\displaystyle |\Lambda _{1}|={\mathcal {O}}(1)} , and by symmetry we obtain the opposite inequality. II. Levin's Universal Distribution Given that any uniquely-decodable code satisfies the Kraft-McMillan inequality, prefix-free Kolmogorov Complexity allows us to derive the Universal Distribution: P ( x ) = ∑ U ( p ) = x P ( U ( p ) = x ) = ∑ U ( p ) = x 2 − K U ( p ) ≤ 1 {\displaystyle P(x)=\sum _{U(p)=x}P(U(p)=x)=\sum _{U(p)=x}2^{-K_{U}(p)}\leq 1} where the fact that U {\displaystyle U} may simulate a prefix-free UTM implies that for two distinct descriptions p {\displaystyle p} and p ′ {\displaystyle p'} , p {\displaystyle p} isn't a substring of p ′ {\displaystyle p'} and p ′ {\displaystyle p'} isn't a substring of p {\displaystyle p} . Interpretation In a Computable Universe, given a phenomenon with encoding x ∈ { 0 , 1 } ∗ {\displaystyle x\in \{0,1\}^{*}} generated by a physical process the probability of 