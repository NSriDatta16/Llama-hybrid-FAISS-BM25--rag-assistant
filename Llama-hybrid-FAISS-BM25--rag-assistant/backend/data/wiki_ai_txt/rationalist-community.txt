The rationalist community is a 21st-century movement that formed around a group of internet blogs, primarily LessWrong and Astral Codex Ten (formerly known as Slate Star Codex). The movement initially gained prominence in the San Francisco Bay Area. Its members seek to use rationality to avoid cognitive biases. Common interests include probability, effective altruism, transhumanism, and mitigating existential risk from artificial general intelligence. The borders of the rationalist community are blurry and subject to debate among the community and adjacent groups. Members who diverge from typical rationalist beliefs often self-describe as "rationalist-adjacent", "post-rationalist" (also known as "ingroup" and "TPOT", an acronym for "this part of Twitter") or "EA-adjacent". Description Rationality Rationalists define rationality to include epistemic rationality — coming to true beliefs about the world, and instrumental rationality — acting in a way to achieve one's objectives. The rationalists are concerned with applying science and probability to various topics, with special attention to Bayesian inference. According to Ellen Huet, the rationalist community "aim[s] to keep their thinking unbiased, even when the conclusions are scary". The early rationalist blogs LessWrong and Slate Star Codex attracted a STEM-interested audience that cared about self-improvement, and was suspicious of the academic humanities and how emotions inhibit rational thinking. However, rationalists rejected the Spock-style archetype of emotionlessness, with Yudkowsky arguing that emotions can themselves be instrumentally rational responses to situations. The movement attracted the attention of the founder culture of Silicon Valley, leading to many shared cultural shibboleths and obsessions, especially optimism about the ability of intelligent capitalists and technocrats to create widespread prosperity. Writing for The New Atlantis, Tara Isabella Burton describes rationalist culture as having a "technocratic focus on ameliorating the human condition through hyper-utilitarian goals", with the "distinctly liberal optimism... that defines so much of Silicon Valley ideology — that intelligent people, using the right epistemic tools, can think better, and save the world by doing so". Burton writes that "Central to the rationalist worldview was the idea that nothing — not social niceties, not fear of political incorrectness, certainly not unwarranted emotion — could, or should, get between human beings and their ability to apprehend the world as it really is". AI safety One of the main interests of the rationalist community is combating existential risk posed by the emergence of an artificial superintelligence. Many members of the rationalist community believe that it is one of the only communities that has a chance at saving humanity from extinction. The stress associated with this consequential responsibility has been a contributing factor to mental health crises among several rationalists. Extreme values Bloomberg Businessweek journalist Ellen Huet adds that the rationalist movement "valorizes extremes: seeking rational truth above all else, donating the most money and doing the utmost good for the most important reason. This way of thinking can lend an attractive clarity, but it can also provide cover for destructive or despicable behavior". Writing in The New Yorker, Gideon Lewis-Kraus argues that rationalists "have given safe harbor to some genuinely egregious ideas," such as scientific racism and neoreactionary views, and that "the rationalists' general willingness to pursue orderly exchanges on objectionable topics, often with monstrous people, remains not only a point of pride but a constitutive part of the subculture's self-understanding." Though this attitude is based on "the view that vile ideas should be countenanced and refuted rather than left to accrue the status of forbidden knowledge", rationalists also hold the view that other ideas, referr