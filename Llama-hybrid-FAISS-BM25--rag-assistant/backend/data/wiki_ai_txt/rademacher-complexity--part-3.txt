L S ( f ) ) {\displaystyle \operatorname {Rep} _{P}({\mathcal {F}},S):=\sup _{f\in F}(L_{P}(f)-L_{S}(f))} Smaller representativeness is better, since it provides a way to avoid overfitting: it means that the true error of a classifier is not much higher than its estimated error, and so selecting a classifier that has low estimated error will ensure that the true error is also low. Note however that the concept of representativeness is relative and hence can not be compared across distinct samples. The expected representativeness of a sample can be bounded above by the Rademacher complexity of the function class: If F {\displaystyle {\mathcal {F}}} is a set of functions with range within [ 0 , 1 ] {\displaystyle [0,1]} , then Rad P , m ⁡ ( F ) − ln ⁡ 2 2 m ≤ E S ∼ P m [ Rep P ⁡ ( F , S ) ] ≤ 2 Rad P , m ⁡ ( F ) {\displaystyle \operatorname {Rad} _{P,m}({\mathcal {F}})-{\sqrt {\frac {\ln 2}{2m}}}\leq \mathbb {E} _{S\sim P^{m}}[\operatorname {Rep} _{P}({\mathcal {F}},S)]\leq 2\operatorname {Rad} _{P,m}({\mathcal {F}})} Furthermore, the representativeness is concentrated around its expectation: For any ϵ {\displaystyle \epsilon } , with probability ≥ 1 − 2 e − 2 ϵ 2 m {\displaystyle \geq 1-2e^{-2\epsilon ^{2}m}} , Rep P ⁡ ( F , S ) ∈ E S ∼ P m [ Rep P ⁡ ( F , S ) ] ± ϵ {\displaystyle \operatorname {Rep} _{P}({\mathcal {F}},S)\in \mathbb {E} _{S\sim P^{m}}[\operatorname {Rep} _{P}({\mathcal {F}},S)]\pm \epsilon } Bounding the generalization error The Rademacher complexity is a theoretical justification for empirical risk minimization. When the error function is binary (0-1 loss), for every δ > 0 {\displaystyle \delta >0} , sup f ∈ F ( L P ( f ) − L S ( f ) ) ≤ 2 Rad S ⁡ ( F ) + 4 2 ln ⁡ ( 4 / δ ) m {\displaystyle \sup _{f\in {\mathcal {F}}}(L_{P}(f)-L_{S}(f))\leq 2\operatorname {Rad} _{S}({\mathcal {F}})+4{\sqrt {2\ln(4/\delta ) \over m}}} with probability at least 1 − δ {\displaystyle 1-\delta } . There exists a constant c > 0 {\displaystyle c>0} , such that when the error function is squared ℓ ( y ^ , y ) := ( y ^ − y ) 2 {\displaystyle \ell ({\hat {y}},y):=({\hat {y}}-y)^{2}} , and the function class F {\displaystyle {\mathcal {F}}} consists of functions with range within [ − 1 , + 1 ] {\displaystyle [-1,+1]} , then for any δ > 0 {\displaystyle \delta >0} L P ( f ) − L S ( f ) ≤ c [ L S ( f ) + ( ln ⁡ m ) 4 Rad ¯ m ( F ) 2 + ln ⁡ ( 1 / δ ) m ] , ∀ f ∈ F {\displaystyle L_{P}(f)-L_{S}(f)\leq c\left[L_{S}(f)+(\ln m)^{4}{\overline {\operatorname {Rad} }}_{m}({\mathcal {F}})^{2}+{\frac {\ln(1/\delta )}{m}}\right],\quad \forall f\in {\mathcal {F}}} with probability at least 1 − δ {\displaystyle 1-\delta } . Oracle inequalities Let the Bayes risk L ∗ = inf f L P ( f ) {\displaystyle L^{*}=\inf _{f}L_{P}(f)} , where f {\displaystyle f} can be any measurable function. Let the function class F {\displaystyle {\mathcal {F}}} be split into "complexity classes" F r {\displaystyle {\mathcal {F}}_{r}} , where r ∈ R {\displaystyle r\in \mathbb {R} } are levels of complexity. Let p r {\displaystyle p_{r}} be real numbers. Let the complexity measure function p {\displaystyle p} be defined such that p ( f ) := min { p r : f ∈ F r } {\displaystyle p(f):=\min\{p_{r}:f\in {\mathcal {F}}_{r}\}} . For any dataset S {\displaystyle S} , let f ^ {\displaystyle {\hat {f}}} be a minimizer of L S ( f ) + p ( f ) {\displaystyle L_{S}(f)+p(f)} . If sup f ∈ F r | L P ( f ) − L S ( f ) | ≤ p r , ∀ r {\displaystyle \sup _{f\in {\mathcal {F}}_{r}}|L_{P}(f)-L_{S}(f)|\leq p_{r},\quad \forall r} then we have the oracle inequality L ( f ^ ) − L ∗ ≤ inf r ( inf f ∈ F r L ( f ) − L ∗ + 2 p r ) {\displaystyle L({\hat {f}})-L^{*}\leq \inf _{r}\left(\inf _{f\in {\mathcal {F}}_{r}}L(f)-L^{*}+2p_{r}\right)} Define f r ∗ ∈ arg ⁡ min f ∈ F r L ( f ) {\displaystyle f_{r}^{*}\in \arg \min _{f\in {\mathcal {F}}_{r}}L(f)} . If we further assume r ≤ s implies F r ⊆ F s and p r ≤ p s {\displaystyle r\leq s{\text{ implies }}{\mathcal {F}}_{r}\subseteq {\mathcal {F}}_{s}{\text{ and }}