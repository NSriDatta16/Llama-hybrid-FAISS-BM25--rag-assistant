 would also not agree as closely with the training data, but in this case the reason is inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage. Bias–variance decomposition of mean squared error Suppose that we have a training set consisting of a set of points x 1 , … , x n {\displaystyle x_{1},\dots ,x_{n}} and real-valued labels y i {\displaystyle y_{i}} associated with the points x i {\displaystyle x_{i}} . We assume that the data is generated by a function f ( x ) {\displaystyle f(x)} such as y = f ( x ) + ε {\displaystyle y=f(x)+\varepsilon } , where the noise, ε {\displaystyle \varepsilon } , has zero mean and variance σ 2 {\displaystyle \sigma ^{2}} . That is, y i = f ( x i ) + ε i {\displaystyle y_{i}=f(x_{i})+\varepsilon _{i}} , where ε i {\displaystyle \varepsilon _{i}} is a noise sample. We want to find a function f ^ ( x ; D ) {\displaystyle {\hat {f}}\!(x;D)} , that approximates the true function f ( x ) {\displaystyle f(x)} as well as possible, by means of some learning algorithm based on a training dataset (sample) D = { ( x 1 , y 1 ) … , ( x n , y n ) } {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}} . We make "as well as possible" precise by measuring the mean squared error between y {\displaystyle y} and f ^ ( x ; D ) {\displaystyle {\hat {f}}\!(x;D)} : we want ( y − f ^ ( x ; D ) ) 2 {\displaystyle (y-{\hat {f}}\!(x;D))^{2}} to be minimal, both for x 1 , … , x n {\displaystyle x_{1},\dots ,x_{n}} and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the y i {\displaystyle y_{i}} contain noise ε {\displaystyle \varepsilon } ; this means we must be prepared to accept an irreducible error in any function we come up with. Finding an f ^ {\displaystyle {\hat {f}}} that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function f ^ {\displaystyle {\hat {f}}} we select, we can decompose its expected error on an unseen sample x {\displaystyle x} (i.e. conditional to x) as follows: E D , ε [ ( y − f ^ ( x ; D ) ) 2 ] = ( Bias D ⁡ [ f ^ ( x ; D ) ] ) 2 + Var D ⁡ [ f ^ ( x ; D ) ] + σ 2 {\displaystyle \mathbb {E} _{D,\varepsilon }{\Big [}{\big (}y-{\hat {f}}\!(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}+\sigma ^{2}} where Bias D ⁡ [ f ^ ( x ; D ) ] ≜ E D [ f ^ ( x ; D ) − f ( x ) ] = E D [ f ^ ( x ; D ) ] − f ( x ) = E D [ f ^ ( x ; D ) ] − E y | x [ y ( x ) ] {\displaystyle {\begin{aligned}\operatorname {Bias} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}&\triangleq \mathbb {E} _{D}{\big [}{\hat {f}}\!(x;D)-f(x){\big ]}\\&=\mathbb {E} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}\,-\,f(x)\\&=\mathbb {E} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}\,-\,\mathbb {E} _{y|x}{\big [}y(x){\big ]}\end{aligned}}} and Var D ⁡ [ f ^ ( x ; D ) ] ≜ E D [ ( E D [ f ^ ( x ; D ) ] − f ^ ( x ; D ) ) 2 ] {\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}\!(x;D){\big ]}\triangleq \mathbb {E} _{D}\left[\left(\mathbb {E} _{D}[{\hat {f}}\!(x;D)]-{\hat {f}}\!(x;D)\right)^{2}\right]} and σ 2 = E y ⁡ [ ( y − f ( x ) ⏟ E y | x [ y ] ) 2 ] {\displaystyle \sigma ^{2}=\operatorname {E} _{y}{\Big [}{\big (}y-\underbrace {f(x)} _{E_{y|x}[y]}{\big )}^{2}{\Big ]}} The expectation ranges over different choices of the training set D = { ( x 1 , y 1 ) … , ( x n , y n ) } {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}} , all sampled from the same joint distribution P ( x , y ) {\