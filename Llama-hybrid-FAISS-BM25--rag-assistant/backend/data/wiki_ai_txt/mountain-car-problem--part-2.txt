 m o t o r = ( l e f t , n e u t r a l , r i g h t ) {\displaystyle motor=(left,neutral,right)} Reward For every time step: r e w a r d = − 1 {\displaystyle reward=-1} Update function For every time step: A c t i o n = [ − 1 , 0 , 1 ] {\displaystyle Action=[-1,0,1]} V e l o c i t y = V e l o c i t y + ( A c t i o n ) ∗ 0.001 + cos ⁡ ( 3 ∗ P o s i t i o n ) ∗ ( − 0.0025 ) {\displaystyle Velocity=Velocity+(Action)*0.001+\cos(3*Position)*(-0.0025)} P o s i t i o n = P o s i t i o n + V e l o c i t y {\displaystyle Position=Position+Velocity} Starting condition Optionally, many implementations include randomness in both parameters to show better generalized learning. P o s i t i o n = − 0.5 {\displaystyle Position=-0.5} V e l o c i t y = 0.0 {\displaystyle Velocity=0.0} Termination condition End the simulation when: P o s i t i o n ≥ 0.6 {\displaystyle Position\geq 0.6} Variations There are many versions of the mountain car which deviate in different ways from the standard model. Variables that vary include but are not limited to changing the constants (gravity and steepness) of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agent's ability to learn in a different manner. An example is changing the reward to be equal to the distance from the goal, or changing the reward to zero everywhere and one at the goal. Additionally, a 3D mountain car can be used, with a 4D continuous state space. References Implementations C++ Mountain Car Software. Richard s. Sutton. Java Mountain Car with support for RL Glue Python, with good discussion (blog post - down page) Further reading Sutton, Richard S. (1996). Mountain Car with Sparse Coarse Coding. Advances in Neural Information Processing Systems. MIT Press. pp. 1038–1044. CiteSeerx: 10.1.1.51.4764. Mountain Car with Replacing Eligibility Traces "More discussion on Continuous State Spaces". 2000. pp. 903–910. CiteSeerX 10.1.1.97.9314. Gaussian Processes with Mountain Car