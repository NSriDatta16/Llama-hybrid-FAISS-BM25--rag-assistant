 {x^{T}w}{(w^{T}Sw)^{1/2}}}{\bigg )}{\bigg ]}} , where ( w T S w ) 1 2 {\displaystyle (w^{T}Sw)^{\frac {1}{2}}} is the induced norm of S {\displaystyle S} , | | w | | s {\displaystyle ||w||_{s}} . Hence, it could be concluded that f B N ( w , γ ) = E x [ ϕ ( x T w ~ ) ] {\displaystyle f_{BN}(w,\gamma )=E_{x}[\phi (x^{T}{\tilde {w}})]} , where w ~ = γ w | | w | | s {\displaystyle {\tilde {w}}=\gamma {\frac {w}{||w||_{s}}}} , and γ {\displaystyle \gamma } and w {\displaystyle w} account for its length and direction separately. This property could then be used to prove the faster convergence of problems with batch normalization. Linear convergence Least-square problem With the reparametrization interpretation, it could then be proved that applying batch normalization to the ordinary least squares problem achieves a linear convergence rate in gradient descent, which is faster than the regular gradient descent with only sub-linear convergence. Denote the objective of minimizing an ordinary least squares problem as min w ~ ∈ R d f O L S ( w ~ ) = min w ~ ∈ R d ( E x , y [ ( y − x T w ~ ) 2 ] ) = min w ~ ∈ R d ( 2 u T w ~ + w ~ T S w ~ ) {\displaystyle \min _{{\tilde {w}}\in R^{d}}f_{OLS}({\tilde {w}})=\min _{{\tilde {w}}\in R^{d}}(E_{x,y}[(y-x^{T}{\tilde {w}})^{2}])=\min _{{\tilde {w}}\in R^{d}}(2u^{T}{\tilde {w}}+{\tilde {w}}^{T}S{\tilde {w}})} , where u = E [ − y x ] {\displaystyle u=E[-yx]} and S = E [ x x T ] {\displaystyle S=E[xx^{T}]} . Since w ~ = γ w | | w | | s {\displaystyle {\tilde {w}}=\gamma {\frac {w}{||w||_{s}}}} , the objective thus becomes min w ∈ R d ∖ { 0 } , γ ∈ R f O L S ( w , γ ) = min w ∈ R d ∖ { 0 } , γ ∈ R ( 2 γ u T w | | w | | S + γ 2 ) {\displaystyle \min _{w\in R^{d}\backslash \{0\},\gamma \in R}f_{OLS}(w,\gamma )=\min _{w\in R^{d}\backslash \{0\},\gamma \in R}{\bigg (}2\gamma {\frac {u^{T}w}{||w||_{S}+\gamma ^{2}}}{\bigg )}} , where 0 is excluded to avoid 0 in the denominator. Since the objective is convex with respect to γ {\displaystyle \gamma } , its optimal value could be calculated by setting the partial derivative of the objective against γ {\displaystyle \gamma } to 0. The objective could be further simplified to be min w ∈ R d ∖ { 0 } ρ ( w ) = min w ∈ R d ∖ { 0 } ( − w T u u T w w T S w ) {\displaystyle \min _{w\in R^{d}\backslash \{0\}}\rho (w)=\min _{w\in R^{d}\backslash \{0\}}{\bigg (}-{\frac {w^{T}uu^{T}w}{w^{T}Sw}}{\bigg )}} . Note that this objective is a form of the generalized Rayleigh quotient ρ ~ ( w ) = w T B w w T A w {\displaystyle {\tilde {\rho }}(w)={\frac {w^{T}Bw}{w^{T}Aw}}} , where B ∈ R d × d {\displaystyle B\in R^{d\times d}} is a symmetric matrix and A ∈ R d × d {\displaystyle A\in R^{d\times d}} is a symmetric positive definite matrix. It is proven that the gradient descent convergence rate of the generalized Rayleigh quotient is λ 1 − ρ ( w t + 1 ) ρ ( w t + 1 − λ 2 ) ≤ ( 1 − λ 1 − λ 2 λ 1 − λ m i n ) 2 t λ 1 − ρ ( w t ) ρ ( w t ) − λ 2 {\displaystyle {\frac {\lambda _{1}-\rho (w_{t+1})}{\rho (w_{t+1}-\lambda _{2})}}\leq {\bigg (}1-{\frac {\lambda _{1}-\lambda _{2}}{\lambda _{1}-\lambda _{min}}}{\bigg )}^{2t}{\frac {\lambda _{1}-\rho (w_{t})}{\rho (w_{t})-\lambda _{2}}}} , where λ 1 {\displaystyle \lambda _{1}} is the largest eigenvalue of B {\displaystyle B} , λ 2 {\displaystyle \lambda _{2}} is the second largest eigenvalue of B {\displaystyle B} , and λ m i n {\displaystyle \lambda _{min}} is the smallest eigenvalue of B {\displaystyle B} . In our case, B = u u T {\displaystyle B=uu^{T}} is a rank one matrix, and the convergence result can be simplified accordingly. Specifically, consider gradient descent steps of the form w t + 1 = w t − η t ▽ ρ ( w t ) {\displaystyle w_{t+1}=w_{t}-\eta _{t}\triangledown \rho (w_{t})} with step size η t = w t T S w t 2 L | ρ ( w t ) | {\displaystyle \eta _{t}={\frac {w_{t}^{T}Sw_{t}}{2L|\rho (w_{t})|}}} , and starting from ρ ( w 0 ) ≠ 0 {\displaystyle \rho (w_{0})\neq 0} , then ρ ( w t ) − ρ ( w ∗ ) ≤ ( 1 − μ L ) 2 t ( ρ ( w 0 ) − ρ ( 