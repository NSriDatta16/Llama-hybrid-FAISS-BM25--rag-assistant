o there can be high confidence in the result. However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result. Precision and recall Precision and recall can be interpreted as (estimated) conditional probabilities: Precision is given by P ( C = P | C ^ = P ) {\displaystyle P(C=P|{\hat {C}}=P)} while recall is given by P ( C ^ = P | C = P ) {\displaystyle P({\hat {C}}=P|C=P)} , where C ^ {\displaystyle {\hat {C}}} is the predicted class and C {\displaystyle C} is the actual class. Both quantities are therefore connected by Bayes' theorem. Relationships There are various relationships between these ratios. If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity: PPV = ( sensitivity ) ( prevalence ) ( sensitivity ) ( prevalence ) + ( 1 − specificity ) ( 1 − prevalence ) {\displaystyle {\text{PPV}}={\frac {({\text{sensitivity}})({\text{prevalence}})}{({\text{sensitivity}})({\text{prevalence}})+(1-{\text{specificity}})(1-{\text{prevalence}})}}} If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity: NPV = ( specificity ) ( 1 − prevalence ) ( specificity ) ( 1 − prevalence ) + ( 1 − sensitivity ) ( prevalence ) . {\displaystyle {\text{NPV}}={\frac {({\text{specificity}})(1-{\text{prevalence}})}{({\text{specificity}})(1-{\text{prevalence}})+(1-{\text{sensitivity}})({\text{prevalence}})}}.} Unitary metrics In addition to the paired metrics, there are also unitary metrics that give a single number to evaluate the test. Perhaps the simplest statistic is accuracy or fraction correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/total population = (TP + TN)/(TP + TN + FP + FN). As such, it compares estimates of pre- and post-test probability. In total ignorance, one can compare a rule to flipping a coin (p0=0.5). This measure is prevalence-dependent. If 90% of people with COVID symptoms don't have COVID, the prior probability P(-) is 0.9, and the simple rule "Classify all such patients as COVID-free." would be 90% accurate. Diagnosis should be better than that. One can construct a "One-proportion z-test" with p0 as max(priors) = max(P(-),P(+)) for a diagnostic method hoping to beat a simple rule using the most likely outcome. Here, the hypotheses are "Ho: p ≤ 0.9 vs. Ha: p > 0.9", rejecting Ho for large values of z. One diagnostic rule could be compared to another if the other's accuracy is known and substituted for p0 in calculating the z statistic. If not known and calculated from data, an accuracy comparison test could be made using "Two-proportion z-test, pooled for Ho: p1 = p2". Not used very much is the complementary statistic, the fraction incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the antidiagonal, divided by the total population. Cost-weighted fractions incorrect could compare expected costs of misclassification for different methods. The diagnostic odds ratio (DOR) can be a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of true rates or prediction values). This has a useful i