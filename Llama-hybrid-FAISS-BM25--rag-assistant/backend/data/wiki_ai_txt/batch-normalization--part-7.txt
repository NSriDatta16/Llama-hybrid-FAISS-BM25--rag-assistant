w ∗ ) ) {\displaystyle \rho (w_{t})-\rho (w^{*})\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2t}(\rho (w_{0})-\rho (w^{*}))} . Learning halfspace problem The problem of learning halfspaces refers to the training of the Perceptron, which is the simplest form of neural network. The optimization problem in this case is min w ~ ∈ R d f L H ( w ~ ) = E y , x [ ϕ ( z T w ~ ) ] {\displaystyle \min _{{\tilde {w}}\in R^{d}}f_{LH}({\tilde {w}})=E_{y,x}[\phi (z^{T}{\tilde {w}})]} , where z = − y x {\displaystyle z=-yx} and ϕ {\displaystyle \phi } is an arbitrary loss function. Suppose that ϕ {\displaystyle \phi } is infinitely differentiable and has a bounded derivative. Assume that the objective function f L H {\displaystyle f_{LH}} is ζ {\displaystyle \zeta } -smooth, and that a solution α ∗ = a r g m i n α | | ▽ f ( α w ) | | 2 {\displaystyle \alpha ^{*}=argmin_{\alpha }||\triangledown f(\alpha w)||^{2}} exists and is bounded such that − ∞ < α ∗ < ∞ {\displaystyle -\infty <\alpha ^{*}<\infty } . Also assume z {\displaystyle z} is a multivariate normal random variable. With the Gaussian assumption, it can be shown that all critical points lie on the same line, for any choice of loss function ϕ {\displaystyle \phi } . Specifically, the gradient of f L H {\displaystyle f_{LH}} could be represented as ▽ w ~ f L H ( w ~ ) = c 1 ( w ~ ) u + c 2 ( w ~ ) S w ~ {\displaystyle \triangledown _{\tilde {w}}f_{LH}({\tilde {w}})=c_{1}({\tilde {w}})u+c_{2}({\tilde {w}})S{\tilde {w}}} , where c 1 ( w ~ ) = E z [ ϕ ( 1 ) ( z T w ~ ) ] − E z [ ϕ ( 2 ) ( z T w ~ ) ] ( u T w ~ ) {\displaystyle c_{1}({\tilde {w}})=E_{z}[\phi ^{(1)}(z^{T}{\tilde {w}})]-E_{z}[\phi ^{(2)}(z^{T}{\tilde {w}})](u^{T}{\tilde {w}})} , c 2 ( w ~ ) = E z [ ϕ ( 2 ) ( z T w ~ ) ] {\displaystyle c_{2}({\tilde {w}})=E_{z}[\phi ^{(2)}(z^{T}{\tilde {w}})]} , and ϕ ( i ) {\displaystyle \phi ^{(i)}} is the i {\displaystyle i} -th derivative of ϕ {\displaystyle \phi } . By setting the gradient to 0, it thus follows that the bounded critical points w ~ ∗ {\displaystyle {\tilde {w}}_{*}} can be expressed as w ~ ∗ = g ∗ S − 1 u {\displaystyle {\tilde {w}}_{*}=g_{*}S^{-1}u} , where g ∗ {\displaystyle g_{*}} depends on w ~ ∗ {\displaystyle {\tilde {w}}_{*}} and ϕ {\displaystyle \phi } . Combining this global property with length-direction decoupling, it could thus be proved that this optimization problem converges linearly. First, a variation of gradient descent with batch normalization, Gradient Descent in Normalized Parameterization (GDNP), is designed for the objective function min w ∈ R d ∖ { 0 } , γ ∈ R f L H ( w , γ ) {\displaystyle \min _{w\in R^{d}\backslash \{0\},\gamma \in R}f_{LH}(w,\gamma )} , such that the direction and length of the weights are updated separately. Denote the stopping criterion of GDNP as h ( w t , γ t ) = E z [ ϕ ′ ( z T w ~ t ) ] ( u T w t ) − E z [ ϕ ″ ( z T w ~ t ) ] ( u T w t ) 2 {\displaystyle h(w_{t},\gamma _{t})=E_{z}[\phi '(z^{T}{\tilde {w}}_{t})](u^{T}w_{t})-E_{z}[\phi ''(z^{T}{\tilde {w}}_{t})](u^{T}w_{t})^{2}} . Let the step size be s t = s ( w t , γ t ) = − | | w t | | S 3 L g t h ( w t , γ t ) {\displaystyle s_{t}=s(w_{t},\gamma _{t})=-{\frac {||w_{t}||_{S}^{3}}{Lg_{t}h(w_{t},\gamma _{t})}}} . For each step, if h ( w t , γ t ) ≠ 0 {\displaystyle h(w_{t},\gamma _{t})\neq 0} , then update the direction as w t + 1 = w t − s t ▽ w f ( w t , γ t ) {\displaystyle w_{t+1}=w_{t}-s_{t}\triangledown _{w}f(w_{t},\gamma _{t})} . Then update the length according to γ t = Bisection ( T s , f , w t ) {\displaystyle \gamma _{t}={\text{Bisection}}(T_{s},f,w_{t})} , where Bisection() {\displaystyle {\text{Bisection()}}} is the classical bisection algorithm, and T s {\displaystyle T_{s}} is the total iterations ran in the bisection step. Denote the total number of iterations as T d {\displaystyle T_{d}} , then the final output of GDNP is w ~ T d = γ T d w T d | | w T d | | S {\displaystyle {\tilde {w}}_{T_{d}}=\gamma _{T_{d}}{\frac {w_{T_{d}}}{||w_{T_{d}}||_{S}}}} . The GD