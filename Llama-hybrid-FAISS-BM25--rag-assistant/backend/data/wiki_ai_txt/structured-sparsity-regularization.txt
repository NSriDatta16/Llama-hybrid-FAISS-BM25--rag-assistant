Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable Y {\displaystyle Y} (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space X {\displaystyle X} (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in X {\displaystyle X} . Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of X {\displaystyle X} may be higher than the number of observations n {\displaystyle n} ), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer. Definition and related concepts Sparsity regularization Consider the linear kernel regularized empirical risk minimization problem with a loss function V ( y i , f ( x ) ) {\displaystyle V(y_{i},f(x))} and the ℓ 0 {\displaystyle \ell _{0}} "norm" as the regularization penalty: min w ∈ R d 1 n ∑ i = 1 n V ( y i , ⟨ w , x i ⟩ ) + λ ‖ w ‖ 0 , {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}V(y_{i},\langle w,x_{i}\rangle )+\lambda \|w\|_{0},} where x , w ∈ R d {\displaystyle x,w\in \mathbb {R^{d}} } , and ‖ w ‖ 0 {\displaystyle \|w\|_{0}} denotes the ℓ 0 {\displaystyle \ell _{0}} "norm", defined as the number of nonzero entries of the vector w {\displaystyle w} . f ( x ) = ⟨ w , x i ⟩ {\displaystyle f(x)=\langle w,x_{i}\rangle } is said to be sparse if ‖ w ‖ 0 = s < d {\displaystyle \|w\|_{0}=s<d} . Which means that the output Y {\displaystyle Y} can be described by a small subset of input variables. More generally, assume a dictionary ϕ j : X → R {\displaystyle \phi _{j}:X\rightarrow \mathbb {R} } with j = 1 , . . . , p {\displaystyle j=1,...,p} is given, such that the target function f ( x ) {\displaystyle f(x)} of a learning problem can be written as: f ( x ) = ∑ j = 1 p ϕ j ( x ) w j {\displaystyle f(x)=\sum _{j=1}^{p}\phi _{j}(x)w_{j}} , ∀ x ∈ X {\displaystyle \forall x\in X} The ℓ 0 {\displaystyle \ell _{0}} norm ‖ f ‖ 0 = ‖ w ‖ 0 {\displaystyle \|f\|_{0}=\|w\|_{0}} as the number of non-zero components of w {\displaystyle w} is defined as ‖ w ‖ 0 = | { j | w j ≠ 0 , j ∈ { 1 , . . . , p } } | {\displaystyle \|w\|_{0}=|\{j|w_{j}\neq 0,j\in \{1,...,p\}\}|} , where | A | {\displaystyle |A|} is the cardinality of set A {\displaystyle A} . f {\displaystyle f} is said to be sparse if ‖ f ‖ 0 = ‖ w ‖ 0 = s < d {\displaystyle \|f\|_{0}=\|w\|_{0}=s<d} . However, while using the ℓ 0 {\displaystyle \ell _{0}} norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the ℓ 1 {\displaystyle \ell _{1}} norm; this has been shown to still favor sparser solutions and is additionally convex. Structured sparsity regularization Structured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization. Consider the above regularized empirical risk minimization problem with a general kernel and associated feature map ϕ j : X → R 