_{7},O_{8},O_{10}\},} and since there are six of these, the dependency of Q on P, γ P ( Q ) = 6 / 10. {\displaystyle \gamma _{P}(Q)=6/10.} This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired. We might then consider the dependency of the smaller attribute set Q = { P 4 } {\displaystyle Q=\{P_{4}\}} on the attribute set P = { P 2 , P 3 } . {\displaystyle P=\{P_{2},P_{3}\}.} The move from Q = { P 4 , P 5 } {\displaystyle Q=\{P_{4},P_{5}\}} to Q = { P 4 } {\displaystyle Q=\{P_{4}\}} induces a coarsening of the class structure [ x ] Q , {\displaystyle [x]_{Q},} as will be seen shortly. We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of [ x ] Q {\displaystyle [x]_{Q}} based on knowledge of [ x ] P . {\displaystyle [x]_{P}.} The equivalence classes of the new [ x ] Q {\displaystyle [x]_{Q}} and of [ x ] P {\displaystyle [x]_{P}} are shown below. Clearly, [ x ] Q {\displaystyle [x]_{Q}} has a coarser granularity than it did earlier. The objects that can now be definitively categorized according to the concept structure [ x ] Q {\displaystyle [x]_{Q}} based on [ x ] P {\displaystyle [x]_{P}} constitute the complete universe { O 1 , O 2 , … , O 10 } {\displaystyle \{O_{1},O_{2},\ldots ,O_{10}\}} , and thus the dependency of Q on P, γ P ( Q ) = 1. {\displaystyle \gamma _{P}(Q)=1.} That is, knowledge of membership according to category set [ x ] P {\displaystyle [x]_{P}} is adequate to determine category membership in [ x ] Q {\displaystyle [x]_{Q}} with complete certainty; In this case we might say that P → Q . {\displaystyle P\rightarrow Q.} Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency. However, we also note that the classes induced in [ x ] Q {\displaystyle [x]_{Q}} from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of [ x ] Q . {\displaystyle [x]_{Q}.} In general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence. Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and Lotfi Zadeh listed in the #References below. Component granulation Another perspective on concept granulation may be obtained from work on parametric models of categories. In mixture model learning, for example, a set of data is explained as a mixture of distinct Gaussian (or other) distributions. Thus, a large amount of data is "replaced" by a small number of distributions. The choice of the number of these distributions, and their size, can again be viewed as a problem of concept granulation. In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately coarsening the concept resolution. Finding the "right" concept resolution is a tricky problem for which many methods have been proposed (e.g., AIC, BIC, MDL, etc.), and these are frequently considered under the rubric of "model regularization". Different interpretations of granular computing Granular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving. In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation. By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities,