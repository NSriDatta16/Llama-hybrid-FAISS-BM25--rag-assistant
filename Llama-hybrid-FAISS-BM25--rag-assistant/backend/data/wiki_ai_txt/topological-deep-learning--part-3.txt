 where the task is to predict the class of each face or edge in a given mesh. Complex classification: Predict targets for an entire complex. For example, predict the class of each input mesh. Cell prediction: Predict properties of cell-cell interactions in a complex, and in some cases, predict whether a cell exists in the complex. An example is the prediction of linkages among entities in hyperedges of a hypergraph. In practice, to perform the aforementioned tasks, deep learning models designed for specific topological spaces must be constructed and implemented. These models, known as topological neural networks, are tailored to operate effectively within these spaces. Topological neural networks Central to TDL are topological neural networks (TNNs), specialized architectures designed to operate on data structured in topological domains. Unlike traditional neural networks tailored for grid-like structures, TNNs are adept at handling more intricate data representations, such as graphs, simplicial complexes, and cell complexes. By harnessing the inherent topology of the data, TNNs can capture both local and global relationships, enabling nuanced analysis and interpretation. Message passing topological neural networks In a general topological domain, higher-order message passing involves exchanging messages among entities and cells using a set of neighborhood functions. Definition: Higher-Order Message Passing on a General Topological Domain Let X {\displaystyle {\mathcal {X}}} be a topological domain. We define a set of neighborhood functions N = { N 1 , … , N n } {\displaystyle {\mathcal {N}}=\{{\mathcal {N}}_{1},\ldots ,{\mathcal {N}}_{n}\}} on X {\displaystyle {\mathcal {X}}} . Consider a cell x {\displaystyle x} and let y ∈ N k ( x ) {\displaystyle y\in {\mathcal {N}}_{k}(x)} for some N k ∈ N {\displaystyle {\mathcal {N}}_{k}\in {\mathcal {N}}} . A message m x , y {\displaystyle m_{x,y}} between cells x {\displaystyle x} and y {\displaystyle y} is a computation dependent on these two cells or the data supported on them. Denote N ( x ) {\displaystyle {\mathcal {N}}(x)} as the multi-set { { N 1 ( x ) , … , N n ( x ) } } {\displaystyle \{\!\!\{{\mathcal {N}}_{1}(x),\ldots ,{\mathcal {N}}_{n}(x)\}\!\!\}} , and let h x ( l ) {\displaystyle \mathbf {h} _{x}^{(l)}} represent some data supported on cell x {\displaystyle x} at layer l {\displaystyle l} . Higher-order message passing on X {\displaystyle {\mathcal {X}}} , induced by N {\displaystyle {\mathcal {N}}} , is defined by the following four update rules: m x , y = α N k ( h x ( l ) , h y ( l ) ) {\displaystyle m_{x,y}=\alpha _{{\mathcal {N}}_{k}}(\mathbf {h} _{x}^{(l)},\mathbf {h} _{y}^{(l)})} m x k = ⨁ y ∈ N k ( x ) m x , y {\displaystyle m_{x}^{k}=\bigoplus _{y\in {\mathcal {N}}_{k}(x)}m_{x,y}} , where ⨁ {\displaystyle \bigoplus } is the intra-neighborhood aggregation function. m x = ⨂ N k ∈ N m x k {\displaystyle m_{x}=\bigotimes _{{\mathcal {N}}_{k}\in {\mathcal {N}}}m_{x}^{k}} , where ⨂ {\displaystyle \bigotimes } is the inter-neighborhood aggregation function. h x ( l + 1 ) = β ( h x ( l ) , m x ) {\displaystyle \mathbf {h} _{x}^{(l+1)}=\beta (\mathbf {h} _{x}^{(l)},m_{x})} , where α N k , β {\displaystyle \alpha _{{\mathcal {N}}_{k}},\beta } are differentiable functions. Some remarks on Definition above are as follows. First, Equation 1 describes how messages are computed between cells x {\displaystyle x} and y {\displaystyle y} . The message m x , y {\displaystyle m_{x,y}} is influenced by both the data h x ( l ) {\displaystyle \mathbf {h} _{x}^{(l)}} and h y ( l ) {\displaystyle \mathbf {h} _{y}^{(l)}} associated with cells x {\displaystyle x} and y {\displaystyle y} , respectively. Additionally, it incorporates characteristics specific to the cells themselves, such as orientation in the case of cell complexes. This allows for a richer representation of spatial relationships compared to traditional graph-based message passing frameworks. Second, Equation 2 defines how