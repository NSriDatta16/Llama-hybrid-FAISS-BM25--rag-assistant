n 2024 the ban of using AI and deepfake for campaigning. They look to implement regulations that would apply as early as for the 2025 general elections. Spain In 2018, the Spanish Ministry of Science, Innovation and Universities approved an R&D Strategy on Artificial Intelligence. Switzerland Switzerland currently has no specific AI legislation, but on 12 February 2025, the Federal Council announced plans to ratify the Council of Europe's AI Convention and incorporate it into Swiss law. A draft bill and implementation plan are to be prepared by the end of 2026. The approach includes sector-specific regulation, limited cross-sector rules, such as data protection, and non-binding measures such as industry agreements. The goals are to support innovation, protect fundamental rights, and build public trust in AI. United Kingdom The UK supported the application and development of AI in business via the Digital Economy Strategy 2015–2018 introduced at the beginning of 2015 by Innovate UK as part of the UK Digital Strategy. In the public sector, the Department for Digital, Culture, Media and Sport advised on data ethics and the Alan Turing Institute provided guidance on responsible design and implementation of AI systems. In terms of cyber security, in 2020 the National Cyber Security Centre has issued guidance on 'Intelligent Security Tools'. The following year, the UK published its 10-year National AI Strategy, which describes actions to assess long-term AI risks, including AGI-related catastrophic risks. In March 2023, the UK released the white paper A pro-innovation approach to AI regulation. This white paper presents general AI principles, but leaves significant flexibility to existing regulators in how they adapt these principles to specific areas such as transport or financial markets. In November 2023, the UK hosted the first AI safety summit, with the prime minister Rishi Sunak aiming to position the UK as a leader in AI safety regulation. During the summit, the UK created an AI Safety Institute, as an evolution of the Frontier AI Taskforce led by Ian Hogarth. The institute was notably assigned the responsibility of advancing the safety evaluations of the world's most advanced AI models, also called frontier AI models. The UK government indicated its reluctance to legislate early, arguing that it may reduce the sector's growth and that laws might be rendered obsolete by further technological progress. United States Discussions on regulation of AI in the United States have included topics such as the timeliness of regulating AI, the nature of the federal regulatory framework to govern and promote AI, including what agency should lead, the regulatory and governing powers of that agency, and how to update regulations in the face of rapidly changing technology, as well as the roles of state governments and courts. Regulation of fully autonomous weapons Legal questions related to lethal autonomous weapons systems (LAWS), in particular compliance with the laws of armed conflict, have been under discussion at the United Nations since 2013, within the context of the Convention on Certain Conventional Weapons. Notably, informal meetings of experts took place in 2014, 2015 and 2016 and a Group of Governmental Experts (GGE) was appointed to further deliberate on the issue in 2016. A set of guiding principles on LAWS affirmed by the GGE on LAWS were adopted in 2018. In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue, and leading to proposals for global regulation. The possibility of a moratorium or preemptive ban of the development and use of LAWS has also been raised on several occasions by other national delegations to the Convention on Certain Conventional Weapons and is strongly advocated for by the Campaign to Stop Killer Robots – a coalition of non