 choice of parameters θ {\displaystyle \theta } (making updates coordinate-dependent), the natural policy gradient aims to provide a coordinate-free update, which is geometrically "natural". Motivation Standard policy gradient updates θ i + 1 = θ i + α ∇ θ J ( θ i ) {\displaystyle \theta _{i+1}=\theta _{i}+\alpha \nabla _{\theta }J(\theta _{i})} solve a constrained optimization problem: { max θ i + 1 J ( θ i ) + ( θ i + 1 − θ i ) T ∇ θ J ( θ i ) ‖ θ i + 1 − θ i ‖ ≤ α ⋅ ‖ ∇ θ J ( θ i ) ‖ {\displaystyle {\begin{cases}\max _{\theta _{i+1}}J(\theta _{i})+(\theta _{i+1}-\theta _{i})^{T}\nabla _{\theta }J(\theta _{i})\\\|\theta _{i+1}-\theta _{i}\|\leq \alpha \cdot \|\nabla _{\theta }J(\theta _{i})\|\end{cases}}} While the objective (linearized improvement) is geometrically meaningful, the Euclidean constraint ‖ θ i + 1 − θ i ‖ {\displaystyle \|\theta _{i+1}-\theta _{i}\|} introduces coordinate dependence. To address this, the natural policy gradient replaces the Euclidean constraint with a Kullback–Leibler divergence (KL) constraint: { max θ i + 1 J ( θ i ) + ( θ i + 1 − θ i ) T ∇ θ J ( θ i ) D ¯ K L ( π θ i + 1 ‖ π θ i ) ≤ ϵ {\displaystyle {\begin{cases}\max _{\theta _{i+1}}J(\theta _{i})+(\theta _{i+1}-\theta _{i})^{T}\nabla _{\theta }J(\theta _{i})\\{\bar {D}}_{KL}(\pi _{\theta _{i+1}}\|\pi _{\theta _{i}})\leq \epsilon \end{cases}}} where the KL divergence between two policies is averaged over the state distribution under policy π θ i {\displaystyle \pi _{\theta _{i}}} . That is, D ¯ K L ( π θ i + 1 ‖ π θ i ) := E s ∼ π θ i [ D K L ( π θ i + 1 ( ⋅ | s ) ‖ π θ i ( ⋅ | s ) ) ] {\displaystyle {\bar {D}}_{KL}(\pi _{\theta _{i+1}}\|\pi _{\theta _{i}}):=\mathbb {E} _{s\sim \pi _{\theta _{i}}}[D_{KL}(\pi _{\theta _{i+1}}(\cdot |s)\|\pi _{\theta _{i}}(\cdot |s))]} This ensures updates are invariant to invertible affine parameter transformations. Fisher information approximation For small ϵ {\displaystyle \epsilon } , the KL divergence is approximated by the Fisher information metric: D ¯ K L ( π θ i + 1 ‖ π θ i ) ≈ 1 2 ( θ i + 1 − θ i ) T F ( θ i ) ( θ i + 1 − θ i ) {\displaystyle {\bar {D}}_{KL}(\pi _{\theta _{i+1}}\|\pi _{\theta _{i}})\approx {\frac {1}{2}}(\theta _{i+1}-\theta _{i})^{T}F(\theta _{i})(\theta _{i+1}-\theta _{i})} where F ( θ ) {\displaystyle F(\theta )} is the Fisher information matrix of the policy, defined as: F ( θ ) = E s , a ∼ π θ [ ∇ θ ln ⁡ π θ ( a | s ) ( ∇ θ ln ⁡ π θ ( a | s ) ) T ] {\displaystyle F(\theta )=\mathbb {E} _{s,a\sim \pi _{\theta }}\left[\nabla _{\theta }\ln \pi _{\theta }(a|s)\left(\nabla _{\theta }\ln \pi _{\theta }(a|s)\right)^{T}\right]} This transforms the problem into a problem in quadratic programming, yielding the natural policy gradient update: θ i + 1 = θ i + α F ( θ i ) − 1 ∇ θ J ( θ i ) {\displaystyle \theta _{i+1}=\theta _{i}+\alpha F(\theta _{i})^{-1}\nabla _{\theta }J(\theta _{i})} The step size α {\displaystyle \alpha } is typically adjusted to maintain the KL constraint, with α ≈ 2 ϵ ( ∇ θ J ( θ i ) ) T F ( θ i ) − 1 ∇ θ J ( θ i ) {\textstyle \alpha \approx {\sqrt {\frac {2\epsilon }{(\nabla _{\theta }J(\theta _{i}))^{T}F(\theta _{i})^{-1}\nabla _{\theta }J(\theta _{i})}}}} . Inverting F ( θ ) {\displaystyle F(\theta )} is computationally intensive, especially for high-dimensional parameters (e.g., neural networks). Practical implementations often use approximations. Trust Region Policy Optimization (TRPO) Trust Region Policy Optimization (TRPO) is a policy gradient method that extends the natural policy gradient approach by enforcing a trust region constraint on policy updates. Developed by Schulman et al. in 2015, TRPO improves upon the natural policy gradient method. The natural gradient descent is theoretically optimal, if the objective is truly a quadratic function, but this is only an approximation. TRPO's line search and KL constraint attempts to restrict the solution to within a "trust region" in which this approximation does not break down. This makes TRPO more robust in p