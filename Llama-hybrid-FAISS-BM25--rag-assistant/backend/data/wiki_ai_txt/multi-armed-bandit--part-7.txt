laystyle (1-\gamma )} it prefers arms with higher weights (exploit), it chooses with probability γ {\displaystyle \gamma } to uniformly randomly explore. After receiving the rewards the weights are updated. The exponential growth significantly increases the weight of good arms. Regret analysis The (external) regret of the Exp3 algorithm is at most O ( K T l o g ( K ) ) {\displaystyle O({\sqrt {KTlog(K)}})} Follow the perturbed leader (FPL) algorithm Algorithm Parameters: Real η {\displaystyle \eta } Initialisation: ∀ i : R i ( 1 ) = 0 {\displaystyle \forall i:R_{i}(1)=0} For each t = 1,2,...,T 1. For each arm generate a random noise from an exponential distribution ∀ i : Z i ( t ) ∼ E x p ( η ) {\displaystyle \forall i:Z_{i}(t)\sim Exp(\eta )} 2. Pull arm I ( t ) {\displaystyle I(t)} : I ( t ) = a r g max i { R i ( t ) + Z i ( t ) } {\displaystyle I(t)=arg\max _{i}\{R_{i}(t)+Z_{i}(t)\}} Add noise to each arm and pull the one with the highest value 3. Update value: R I ( t ) ( t + 1 ) = R I ( t ) ( t ) + x I ( t ) ( t ) {\displaystyle R_{I(t)}(t+1)=R_{I(t)}(t)+x_{I(t)}(t)} The rest remains the same Explanation We follow the arm that we think has the best performance so far adding exponential noise to it to provide exploration. Exp3 vs FPL Infinite-armed bandit In the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable K {\displaystyle K} . In the infinite armed case, introduced by Agrawal (1995), the "arms" are a continuous variable in K {\displaystyle K} dimensions. Non-stationary bandit This framework refers to the multi-armed bandit problem in a non-stationary setting (i.e., in presence of concept drift). In the non-stationary setting, it is assumed that the expected reward for an arm k {\displaystyle k} can change at every time step t ∈ T {\displaystyle t\in {\mathcal {T}}} : μ t − 1 k ≠ μ t k {\displaystyle \mu _{t-1}^{k}\neq \mu _{t}^{k}} . Thus, μ t k {\displaystyle \mu _{t}^{k}} no longer represents the whole sequence of expected (stationary) rewards for arm k {\displaystyle k} . Instead, μ k {\displaystyle \mu ^{k}} denotes the sequence of expected rewards for arm k {\displaystyle k} , defined as μ k = { μ t k } t = 1 T {\displaystyle \mu ^{k}=\{\mu _{t}^{k}\}_{t=1}^{T}} . A dynamic oracle represents the optimal policy to be compared with other policies in the non-stationary setting. The dynamic oracle optimises the expected reward at each step t ∈ T {\displaystyle t\in {\mathcal {T}}} by always selecting the best arm, with expected reward of μ t ∗ {\displaystyle \mu _{t}^{*}} . Thus, the cumulative expected reward D ( T ) {\displaystyle {\mathcal {D}}(T)} for the dynamic oracle at final time step T {\displaystyle T} is defined as: D ( T ) = ∑ t = 1 T μ t ∗ . {\displaystyle {\mathcal {D}}(T)=\sum _{t=1}^{T}{\mu _{t}^{*}}.} Hence, the regret ρ π ( T ) {\displaystyle \rho ^{\pi }(T)} for policy π {\displaystyle \pi } is computed as the difference between D ( T ) {\displaystyle {\mathcal {D}}(T)} and the cumulative expected reward at step T {\displaystyle T} for policy π {\displaystyle \pi } : ρ π ( T ) = ∑ t = 1 T μ t ∗ − E π μ [ ∑ t = 1 T r t ] = D ( T ) − E π μ [ ∑ t = 1 T r t ] . {\displaystyle \rho ^{\pi }(T)=\sum _{t=1}^{T}{\mu _{t}^{*}}-\mathbb {E} _{\pi }^{\mu }\left[\sum _{t=1}^{T}{r_{t}}\right]={\mathcal {D}}(T)-\mathbb {E} _{\pi }^{\mu }\left[\sum _{t=1}^{T}{r_{t}}\right].} Garivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB and Sliding-Window UCB. A similar approach based on Thompson Sampling algorithm is the f-Discounted-Sliding-Window Thompson Sampling (f-dsw TS) proposed by Cavenaghi et al. The f-dsw TS algorithm exploits a discount factor on the reward history and an arm-related sliding window to contrast concept drift in non-station