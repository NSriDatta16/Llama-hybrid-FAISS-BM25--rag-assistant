hrough correlated variables in a non-obvious way. In complex human‐machine interactions, some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead. Physical Physical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots). Cobots are intended to operate in close proximity to humans, which makes impossible the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots. Automated guided vehicles are a type of cobot that as of 2019 are in common use, often as forklifts or pallet jacks in warehouses or factories. For cobots, sensor malfunctions or unexpected work environment conditions can lead to unpredictable robot behavior and thus to human–robot collisions. Self-driving cars are another example of AI-enabled robots. In addition, the ergonomics of control interfaces and human–machine interactions may give rise to hazards. Hazard controls AI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions, as well as information privacy measures. Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues. Proposed best practices for employer‐sponsored worker monitoring programs include using only validated sensor technologies; ensuring voluntary worker participation; ceasing data collection outside the workplace; disclosing all data uses; and ensuring secure data storage. For industrial cobots equipped with AI‐enabled sensors, the International Organization for Standardization (ISO) recommended: (a) safety‐related monitored stopping controls; (b) human hand guiding of the cobot; (c) speed and separation monitoring controls; and (d) power and force limitations. Networked AI-enabled cobots may share safety improvements with each other. Human oversight is another general hazard control for AI. Risk management Both applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management. As with all hazards, risk identification is most effective and least costly when done in the design phase. Workplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate and does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs. Proxies for skill content include educational requirements and classifications of routine versus non-routine, and cognitive versus physical jobs. However, these may still not be specific enough to distinguish specific occupations that have distinct impacts from AI. The United States Department of Labor's Occupational Information Network is an example of a database with a detailed taxonomy of skills. Additionally, data are often reported on a national level, while there is much geographical variation, especially between urban and rural areas. AI systems in the workplace raise ethical concerns related to privacy, fairness, human dignity, and transparency. According to the OECD, these risks must be addressed through robust governance frameworks and accountability mechanisms. Ethical deployment of AI requires clear policies on data usage, explainability of algorithms, and safeguards against discrimination and surveillance."Using Artificial Intelligence in the Workplace: What Are the Main Ethical Risks?". OECD. 2022. Retrieved 13 August 2025. Standards and regulation As of 2019, ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces. The standard is planned to include guidelines for both gathering da