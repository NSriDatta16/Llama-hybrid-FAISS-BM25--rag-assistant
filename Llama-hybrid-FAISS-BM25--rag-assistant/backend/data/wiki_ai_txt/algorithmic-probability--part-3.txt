that phenomenon is well-defined and equal to the sum over the probabilities of distinct and independent causes. The prefix-free criterion is precisely what guarantees causal independence. Proof This is an immediate consequence of the Kraft-McMillan inequality. Kraft's inequality states that given a sequence of strings { x i } i = 1 n {\displaystyle \{x_{i}\}_{i=1}^{n}} there exists a prefix code with codewords { σ i } i = 1 n {\displaystyle \{\sigma _{i}\}_{i=1}^{n}} where ∀ i , | σ i | = k i {\displaystyle \forall i,|\sigma _{i}|=k_{i}} if and only if: ∑ i = 1 n s − k i ≤ 1 {\displaystyle \sum _{i=1}^{n}s^{-k_{i}}\leq 1} where s {\displaystyle s} is the size of the alphabet S {\displaystyle S} . Without loss of generality, let's suppose we may order the k i {\displaystyle k_{i}} such that: k 1 ≤ k 2 ≤ . . . ≤ k n {\displaystyle k_{1}\leq k_{2}\leq ...\leq k_{n}} Now, there exists a prefix code if and only if at each step j {\displaystyle j} there is at least one codeword to choose that does not contain any of the previous j − 1 {\displaystyle j-1} codewords as a prefix. Due to the existence of a codeword at a previous step i < j , s k j − k i {\displaystyle i<j,s^{k_{j}-k_{i}}} codewords are forbidden as they contain σ i {\displaystyle \sigma _{i}} as a prefix. It follows that in general a prefix code exists if and only if: ∀ j ≥ 2 , s k j > ∑ i = 1 j − 1 s k j − k i {\displaystyle \forall j\geq 2,s^{k_{j}}>\sum _{i=1}^{j-1}s^{k_{j}-k_{i}}} Dividing both sides by s k j {\displaystyle s^{k_{j}}} , we find: ∑ i = 1 n s − k i ≤ 1 {\displaystyle \sum _{i=1}^{n}s^{-k_{i}}\leq 1} QED. History Solomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: "A Preliminary Report on a General Theory of Inductive Inference." He clarified these ideas more fully in 1964 with "A Formal Theory of Inductive Inference," Part I and Part II. Sequential Decisions Based on Algorithmic Probability Sequential Decisions Based on Algorithmic Probability is a theoretical framework proposed by Marcus Hutter to unify algorithmic probability with decision theory. The framework provides a foundation for creating universally intelligent agents capable of optimal performance in any computable environment. It builds on Solomonoff’s theory of induction and incorporates elements of reinforcement learning, optimization, and sequential decision-making. Background Inductive reasoning, the process of predicting future events based on past observations, is central to intelligent behavior. Hutter formalized this process using Occam’s razor and algorithmic probability. The framework is rooted in Kolmogorov complexity, which measures the simplicity of data by the length of its shortest descriptive program. This concept underpins the universal distribution MM, as introduced by Ray Solomonoff, which assigns higher probabilities to simpler hypotheses. Hutter extended the universal distribution to include actions, creating a framework capable of addressing problems such as prediction, optimization, and reinforcement learning in environments with unknown structures. The AIXI Model The AIXI model is the centerpiece of Hutter’s theory. It describes a universal artificial agent designed to maximize expected rewards in an unknown environment. AIXI operates under the assumption that the environment can be represented by a computable probability distribution. It uses past observations to infer the most likely environmental model, leveraging algorithmic probability. Mathematically, AIXI evaluates all possible future sequences of actions and observations. It computes their algorithmic probabilities and expected utilities, selecting the sequence of actions that maximizes cumulative rewards. This approach transforms sequential decision-making into an optimization problem. However, the general formulation of AIXI is incomputable, making it impractical for direct implementation. Optimality and Limitations AIXI 