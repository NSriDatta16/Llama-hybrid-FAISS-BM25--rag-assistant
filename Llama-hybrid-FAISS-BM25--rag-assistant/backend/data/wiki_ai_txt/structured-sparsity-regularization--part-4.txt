 system of hierarchies can be represented using directed acyclic graphs. Hierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents. Hierarchical models using Bayesian non-parametric methods have been used to learn topic models, which are statistical models for discovering the abstract "topics" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods. Hierarchical norms have been applied to bioinformatics, computer vision and topic models. Norms defined on grids If the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes. Such methods have applications in computer vision Algorithms for computation Best subset selection problem The problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as: min w ∈ R d 1 n ∑ i = 1 n V ( y i , w , x i ) + λ ‖ w ‖ 0 , {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}V(y_{i},w,x_{i})+\lambda \|w\|_{0},} Where ‖ w ‖ 0 {\displaystyle \|w\|_{0}} denotes the ℓ 0 {\displaystyle \ell _{0}} "norm", defined as the number of nonzero entries of the vector w {\displaystyle w} . Although this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables. Two main approaches for solving the optimization problem are: 1) greedy methods, such as step-wise regression in statistics, or matching pursuit in signal processing; and 2) convex relaxation formulation approaches and proximal gradient optimization methods. Convex relaxation A natural approximation for the best subset selection problem is the ℓ 1 {\displaystyle \ell _{1}} norm regularization: min w ∈ R d 1 n ∑ i = 1 n V ( y i , w , x i ) + λ ‖ w ‖ 1 {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}V(y_{i},w,x_{i})+\lambda \|w\|_{1}} Such a scheme is called basis pursuit or the Lasso, which substitutes the ℓ 0 {\displaystyle \ell _{0}} "norm" for the convex, non-differentiable ℓ 1 {\displaystyle \ell _{1}} norm. Proximal gradient methods Proximal gradient methods, also called forward-backward splitting, are optimization methods useful for minimizing functions with a convex and differentiable component, and a convex potentially non-differentiable component. As such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems of the following form: min w ∈ R d 1 n ∑ i = 1 n V ( y i , w , x i ) + R ( w ) {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}V(y_{i},w,x_{i})+R(w)} Where V ( y i , w , x i ) {\displaystyle V(y_{i},w,x_{i})} is a convex and differentiable loss function like the quadratic loss, and R ( w ) {\displaystyle R(w)} is a convex potentially non-differentiable regularizer such as the ℓ 1 {\displaystyle \ell _{1}} norm. Connections to Other Areas of Machine Learning Connection to Multiple Kernel Learning Structured Sparsity regularization can be applied in the context of multiple kernel learning. Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. In the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of whi