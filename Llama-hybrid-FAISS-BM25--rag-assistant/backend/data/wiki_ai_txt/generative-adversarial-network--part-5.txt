ic optimization to avoid mode collapse, as well as the Fréchet inception distance for evaluating GAN performances. Vanishing gradient Conversely, if the discriminator learns too fast compared to the generator, then the discriminator could almost perfectly distinguish μ G θ , μ ref {\displaystyle \mu _{G_{\theta }},\mu _{\text{ref}}} . In such case, the generator G θ {\displaystyle G_{\theta }} could be stuck with a very high loss no matter which direction it changes its θ {\displaystyle \theta } , meaning that the gradient ∇ θ L ( G θ , D ζ ) {\displaystyle \nabla _{\theta }L(G_{\theta },D_{\zeta })} would be close to zero. In such case, the generator cannot learn, a case of the vanishing gradient problem. Intuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try. One important method for solving this problem is the Wasserstein GAN. Evaluation GANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fréchet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS. Another evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS), which starts with a learned image featurizer f θ : Image → R n {\displaystyle f_{\theta }:{\text{Image}}\to \mathbb {R} ^{n}} , and finetunes it by supervised learning on a set of ( x , x ′ , p e r c e p t u a l d i f f e r e n c e ⁡ ( x , x ′ ) ) {\displaystyle (x,x',\operatorname {perceptual~difference} (x,x'))} , where x {\displaystyle x} is an image, x ′ {\displaystyle x'} is a perturbed version of it, and p e r c e p t u a l d i f f e r e n c e ⁡ ( x , x ′ ) {\displaystyle \operatorname {perceptual~difference} (x,x')} is how much they differ, as reported by human subjects. The model is finetuned so that it can approximate ‖ f θ ( x ) − f θ ( x ′ ) ‖ ≈ p e r c e p t u a l d i f f e r e n c e ⁡ ( x , x ′ ) {\displaystyle \|f_{\theta }(x)-f_{\theta }(x')\|\approx \operatorname {perceptual~difference} (x,x')} . This finetuned model is then used to define LPIPS ⁡ ( x , x ′ ) := ‖ f θ ( x ) − f θ ( x ′ ) ‖ {\displaystyle \operatorname {LPIPS} (x,x'):=\|f_{\theta }(x)-f_{\theta }(x')\|} . Other evaluation methods are reviewed in. Variants There is a veritable zoo of GAN variants. Some of the most prominent are as follows: Conditional GAN Conditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example, if we want to generate a cat face given a dog picture, we could use a conditional GAN. The generator in a GAN game generates μ G {\displaystyle \mu _{G}} , a probability distribution on the probability space Ω {\displaystyle \Omega } . This leads to the idea of a conditional GAN, where instead of generating one probability distribution on Ω {\displaystyle \Omega } , the generator generates a different probability distribution μ G ( c ) {\displaystyle \mu _{G}(c)} on Ω {\displaystyle \Omega } , for each given class label c {\displaystyle c} . For example, for generating images that look like ImageNet, the generator should be able to generate a picture of cat when given the class label "cat". In the original paper, the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator. Concretely, the conditional GAN game is just the GAN game with class labels provided: L ( μ G , D ) := E c ∼ μ C , x ∼ μ ref ( c ) ⁡ [ ln ⁡ D ( x , c ) ] + E c ∼ μ C , x ∼ μ G ( c ) ⁡ [ ln ⁡ ( 1 − D ( x , c ) ) ] {\displaystyle L(\m