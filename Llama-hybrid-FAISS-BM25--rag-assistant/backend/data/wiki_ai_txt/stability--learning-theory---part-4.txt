gorithm generalizes. Uniform Stability is a strong condition which is not met by all algorithms but is, surprisingly, met by the large and important class of Regularization algorithms. The generalization bound is given in the article. From Mukherjee et al. (06): For symmetric learning algorithms with bounded loss, if the algorithm has both Leave-one-out cross-validation (CVloo) Stability and Expected-leave-one-out error ( E l o o e r r {\displaystyle Eloo_{err}} ) Stability as defined above, then the algorithm generalizes. Neither condition alone is sufficient for generalization. However, both together ensure generalization (while the converse is not true). For ERM algorithms specifically (say for the square loss), Leave-one-out cross-validation (CVloo) Stability is both necessary and sufficient for consistency and generalization. This is an important result for the foundations of learning theory, because it shows that two previously unrelated properties of an algorithm, stability and consistency, are equivalent for ERM (and certain loss functions). The generalization bound is given in the article. Algorithms that are stable This is a list of algorithms that have been shown to be stable, and the article where the associated generalization bounds are provided. Linear regression k-NN classifier with a {0-1} loss function. Support Vector Machine (SVM) classification with a bounded kernel and where the regularizer is a norm in a Reproducing Kernel Hilbert Space. A large regularization constant C {\displaystyle C} leads to good stability. Soft margin SVM classification. Regularized Least Squares regression. The minimum relative entropy algorithm for classification. A version of bagging regularizers with the number k {\displaystyle k} of regressors increasing with n {\displaystyle n} . Multi-class SVM classification. All learning algorithms with Tikhonov regularization satisfies Uniform Stability criteria and are, thus, generalizable. References == Further reading ==