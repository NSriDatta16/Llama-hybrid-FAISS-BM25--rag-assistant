 ResNet model took 18 days to train on 592 V100 GPUs. The largest ViT model took 12 days on 256 V100 GPUs. All ViT models were trained on 224×224 image resolution. The ViT-L/14 was then boosted to 336×336 resolution by FixRes, resulting in a model. They found this was the best-performing model. In the OpenCLIP series, the ViT-L/14 model was trained on 384 A100 GPUs on the LAION-2B dataset, for 160 epochs for a total of 32B samples seen. Applications Cross-modal retrieval CLIP's cross-modal retrieval enables the alignment of visual and textual data in a shared latent space, allowing users to retrieve images based on text descriptions and vice versa, without the need for explicit image annotations. In text-to-image retrieval, users input descriptive text, and CLIP retrieves images with matching embeddings. In image-to-text retrieval, images are used to find related text content. CLIP’s ability to connect visual and textual data has found applications in multimedia search, content discovery, and recommendation systems. Image classification CLIP can perform zero-shot image classification tasks. This is achieved by prompting the text encoder with class names and selecting the class whose embedding is closest to the image embedding. For example, to classify an image, they compared the embedding of the image with the embedding of the text "A photo of a {class}.", and the {class} that results in the highest dot product is outputted. CLIP for multimodal learning CLIP has been used as a component in multimodal learning. For example, during the training of Google DeepMind's Flamingo (2022), the authors trained a CLIP pair, with BERT as the text encoder and NormalizerFree ResNet F6 as the image encoder. The image encoder of the CLIP pair was taken with parameters frozen and the text encoder was discarded. The frozen image encoder was then combined with a frozen Chinchilla language model, by finetuning with some further parameters that connect the two frozen models. Applications in other domains CLIP's image encoder is a pre-trained image featurizer. This can then be fed into other AI models. Models like Stable Diffusion use CLIP's text encoder to transform text prompts into embeddings for image generation. CLIP can also be used as a gradient signal for directly guiding diffusion ("CLIP guidance") or other generative art. Fine-tuned CLIP models can be used to rank images by aesthetic quality, which may be useful as a step in filtering a large dataset into a smaller one with higher quality. CLIP can be used to generate image captions by matching text inputs to image embeddings. Notes References External links OpenAI's CLIP webpage OpenCLIP: An open source implementation of CLIP Arora, Aman (2023-03-11). "The Annotated CLIP (Part-2)". amaarora.github.io. Retrieved 2024-09-11.