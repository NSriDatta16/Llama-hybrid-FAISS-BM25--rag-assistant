f {T} _{\mathbf {y} }'\mathbf {F_{x}T_{x}} )\right|} , the equality between R aff {\displaystyle R_{\text{aff}}} and R lin {\displaystyle R_{\text{lin}}} holds because not only: x ′ x = 1 ⟹ y = f aff ( x ; M , c ) = f lin ( x ; M + c x ′ ) {\displaystyle \mathbf {x} '\mathbf {x} =1\;\Longrightarrow \;\mathbf {y} =f_{\text{aff}}(\mathbf {x} ;\mathbf {M,c} )=f_{\text{lin}}(\mathbf {x} ;\mathbf {M+cx} ')} but also, by orthogonality of x {\displaystyle \mathbf {x} } to the local tangent space: x ′ T x = 0 ⟹ F x aff T x = F x lin T x {\displaystyle \mathbf {x} '\mathbf {T_{x}} ={\boldsymbol {0}}\;\Longrightarrow \;\mathbf {F} _{\mathbf {x} }^{\text{aff}}\mathbf {T_{x}} =\mathbf {F} _{\mathbf {x} }^{\text{lin}}\mathbf {T_{x}} } where F x lin = ‖ M x + c ‖ − 1 ( I n − y y ′ ) ( M + c x ′ ) {\displaystyle \mathbf {F} _{\mathbf {x} }^{\text{lin}}=\lVert \mathbf {Mx} +\mathbf {c} \rVert ^{-1}(\mathbf {I} _{n}-\mathbf {yy} ')(\mathbf {M+cx} ')} is the Jacobian of f lin {\displaystyle f_{\text{lin}}} differentiated w.r.t. its input, but not also w.r.t. to its parameter. Downsides Despite normalizing flows success in estimating high-dimensional densities, some downsides still exist in their designs. First of all, their latent space where input data is projected onto is not a lower-dimensional space and therefore, flow-based models do not allow for compression of data by default and require a lot of computation. However, it is still possible to perform image compression with them. Flow-based models are also notorious for failing in estimating the likelihood of out-of-distribution samples (i.e.: samples that were not drawn from the same distribution as the training set). Some hypotheses were formulated to explain this phenomenon, among which the typical set hypothesis, estimation issues when training models, or fundamental issues due to the entropy of the data distributions. One of the most interesting properties of normalizing flows is the invertibility of their learned bijective map. This property is given by constraints in the design of the models (cf.: RealNVP, Glow) which guarantee theoretical invertibility. The integrity of the inverse is important in order to ensure the applicability of the change-of-variable theorem, the computation of the Jacobian of the map as well as sampling with the model. However, in practice this invertibility is violated and the inverse map explodes because of numerical imprecision. Applications Flow-based generative models have been applied on a variety of modeling tasks, including: Audio generation Image generation Molecular graph generation Point-cloud modeling Video generation Lossy image compression Anomaly detection References External links Flow-based Deep Generative Models Normalizing flow models