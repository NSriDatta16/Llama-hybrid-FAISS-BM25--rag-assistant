isplaystyle r} . Even if, CrossE, does not rely on a neural network architecture, it is shown that this methodology can be encoded in such architecture. Roto-translational models This family of models, in addition or in substitution of a translation they employ a rotation-like transformation. TorusE: The regularization term of TransE makes the entity embedding to build a spheric space, and consequently loses the translation properties of the geometric space. To address this problem, TorusE leverages the use of a compact Lie group that in this specific case is n-dimensional torus space, and avoid the use of regularization. TorusE defines the distance functions to substitute the L1 and L2 norm of TransE. RotatE: RotatE is inspired by the Euler's identity and involves the use of Hadamard product to represent a relation r {\displaystyle r} as a rotation from the head h {\displaystyle h} to the tail t {\displaystyle t} in the complex space. For each element of the triple, the complex part of the embedding describes a counterclockwise rotation respect to an axis, that can be describe with the Euler's identity, whereas the modulus of the relation vector is 1. It is shown that the model is capable of embedding symmetric, asymmetric, inversion, and composition relations from the knowledge graph. Deep learning models This group of embedding models uses deep neural network to learn patterns from the knowledge graph that are the input data. These models have the generality to distinguish the type of entity and relation, temporal information, path information, underlay structured information, and resolve the limitations of distance-based and semantic-matching-based models in representing all the features of a knowledge graph. The use of deep learning for knowledge graph embedding has shown good predictive performance even if they are more expensive in the training phase, data-hungry, and often required a pre-trained embedding representation of knowledge graph coming from a different embedding model. Convolutional neural networks This family of models, instead of using fully connected layers, employs one or more convolutional layers that convolve the input data applying a low-dimensional filter capable of embedding complex structures with few parameters by learning nonlinear features. ConvE: ConvE is an embedding model that represents a good tradeoff expressiveness of deep learning models and computational expensiveness, in fact it is shown that it used 8x less parameters, when compared to DistMult. ConvE uses a one-dimensional d {\displaystyle d} -sized embedding to represent the entities and relations of a knowledge graph. To compute the score function of a triple, ConvE apply a simple procedure: first concatenes and merge the embeddings of the head of the triple and the relation in a single data [ h ; r ] {\displaystyle {\ce {[h;{\mathcal {r}}]}}} , then this matrix is used as input for the 2D convolutional layer. The result is then passed through a dense layer that apply a linear transformation parameterized by the matrix W {\displaystyle {\mathcal {W}}} and at the end, with the inner product is linked to the tail triple. ConvE is also particularly efficient in the evaluation procedure: using a 1-N scoring, the model matches, given a head and a relation, all the tails at the same time, saving a lot of evaluation time when compared to the 1-1 evaluation program of the other models. ConvR: ConvR is an adaptive convolutional network aimed to deeply represent all the possible interactions between the entities and the relations. For this task, ConvR, computes convolutional filter for each relation, and, when required, applies these filters to the entity of interest to extract convoluted features. The procedure to compute the score of triple is the same as ConvE. ConvKB: ConvKB, to compute score function of a given triple ( h , r , t ) {\displaystyle (h,r,t)} , it produces an input [ h ; r ; t ] {\displaystyle {\ce {[h;{\mathcal {r}};t]}}} of