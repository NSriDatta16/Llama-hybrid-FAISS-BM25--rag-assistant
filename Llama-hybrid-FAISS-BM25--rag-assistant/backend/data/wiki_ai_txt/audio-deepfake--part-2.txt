t-and-paste involves faking the requested sentence from a text-dependent system. Text-dependent speaker verification can be used to defend against replay-based attacks. A current technique that detects end-to-end replay attacks is the use of deep convolutional neural networks. Synthetic-based The category based on speech synthesis refers to the artificial production of human speech, using software or hardware system programs. Speech synthesis includes text-to-speech, which aims to transform the text into acceptable and natural speech in real-time, making the speech sound in line with the text input, using the rules of linguistic description of the text. A classical system of this type consists of three modules: a text analysis model, an acoustic model, and a vocoder. The generation usually has to follow two essential steps. It is necessary to collect clean and well-structured raw audio with the transcripted text of the original speech audio sentence. Second, the text-to-speech model must be trained using these data to build a synthetic audio generation model. Specifically, the transcribed text with the target speaker's voice is the input of the generation model. The text analysis module processes the input text and converts it into linguistic features. Then, the acoustic module extracts the parameters of the target speaker from the audio data based on the linguistic features generated by the text analysis module. Finally, the vocoder learns to create vocal waveforms based on the parameters of the acoustic features. The final audio file is generated, including the synthetic simulation audio in a waveform format, creating speech audio in the voice of many speakers, even those not in training. The first breakthrough in this regard was introduced by WaveNet, a neural network for generating raw audio waveforms capable of emulating the characteristics of many different speakers. This network has been overtaken over the years by other systems which synthesize highly realistic artificial voices within everyoneâ€™s reach. Text-to-speech is highly dependent on the quality of the voice corpus used to realize the system, and creating an entire voice corpus is expensive. Another disadvantage is that speech synthesis systems do not recognize periods or special characters. Also, ambiguity problems are persistent, as two words written in the same way can have different meanings. Imitation-based Audio deepfake based on imitation is a way of transforming an original speech from one speaker - the original - so that it sounds spoken like another speaker - the target one. An imitation-based algorithm takes a spoken signal as input and alters it by changing its style, intonation, or prosody, trying to mimic the target voice without changing the linguistic information. This technique is also known as voice conversion. This method is often confused with the previous synthetic-based method, as there is no clear separation between the two approaches regarding the generation process. Indeed, both methods modify acoustic-spectral and style characteristics of the speech audio signal, but the Imitation-based usually keeps the input and output text unaltered. This is obtained by changing how this sentence is spoken to match the target speaker's characteristics. Voices can be imitated in several ways, such as using humans with similar voices that can mimic the original speaker. In recent years, the most popular approach involves the use of particular neural networks called generative adversarial networks (GAN) due to their flexibility as well as high-quality results. Then, the original audio signal is transformed to say a speech in the target audio using an imitation generation method that generates a new speech, shown in the fake one. Detection methods The audio deepfake detection task determines whether the given speech audio is real or fake. Recently, this has become a hot topic in the forensic research community, trying to keep up with the rapid evolution 