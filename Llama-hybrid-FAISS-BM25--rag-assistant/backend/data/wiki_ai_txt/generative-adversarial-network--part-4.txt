e logistic function. In particular, if the prior probability for an image x {\displaystyle x} to come from the reference distribution is equal to 1 2 {\displaystyle {\frac {1}{2}}} , then D ( x ) {\displaystyle D(x)} is just the posterior probability that x {\displaystyle x} came from the reference distribution: D ( x ) = Pr ( x came from reference distribution ∣ x ) . {\displaystyle D(x)=\Pr(x{\text{ came from reference distribution}}\mid x).} Training and evaluating GAN Training Unstable convergence While the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets, the equilibrium is no longer guaranteed when they have a restricted strategy set. In practice, the generator has access only to measures of form μ Z ∘ G θ − 1 {\displaystyle \mu _{Z}\circ G_{\theta }^{-1}} , where G θ {\displaystyle G_{\theta }} is a function computed by a neural network with parameters θ {\displaystyle \theta } , and μ Z {\displaystyle \mu _{Z}} is an easily sampled distribution, such as the uniform or normal distribution. Similarly, the discriminator has access only to functions of form D ζ {\displaystyle D_{\zeta }} , a function computed by a neural network with parameters ζ {\displaystyle \zeta } . These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets. Further, even if an equilibrium still exists, it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN, and often the game "collapses" into one of several failure modes. To improve the convergence stability, some training strategies start with an easier task, such as generating low-resolution images or simple images (one object with uniform background), and gradually increase the difficulty of the task during training. This essentially translates to applying a curriculum learning scheme. Mode collapse GANs often suffer from mode collapse where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit might only generate pictures of digit 0. This was termed "the Helvetica scenario". One way this can happen is if the generator learns too fast compared to the discriminator. If the discriminator D {\displaystyle D} is held constant, then the optimal generator would only output elements of arg ⁡ max x D ( x ) {\displaystyle \arg \max _{x}D(x)} . So for example, if during GAN training for generating MNIST dataset, for a few epochs, the discriminator somehow prefers the digit 0 slightly more than other digits, the generator may seize the opportunity to generate only digit 0, then be unable to escape the local minimum after the discriminator improves. Some researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed, but it is still an open problem. Even the state-of-the-art architecture, BigGAN (2019), could not avoid mode collapse. The authors resorted to "allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results". Two time-scale update rule The two time-scale update rule (TTUR) is proposed to make GAN convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator, so that it does not "drive the discriminator steadily into new regions without capturing its gathered information". They proved that a general class of games that included the GAN game, when trained under TTUR, "converges under mild assumptions to a stationary local Nash equilibrium". They also proposed using the Adam stochast