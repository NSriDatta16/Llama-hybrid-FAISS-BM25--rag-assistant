ence, and Psychology & Cognitive Neuroscience. Sourced from 44 peer-reviewed publications that have released their code and data under permissive licenses. Each task is validated by domain experts. DSBench: 466 data analysis tasks and 74 data modeling tasks sourced from Kaggle and ModelOff competitions, spanning exploratory analysis, multi‑table joins, and predictive modeling with large CSVs and multimodal prompts. SpreadsheetBench: 912 real-world spreadsheet manipulation tasks scraped from public Excel help forums, spanning formula writing, data cleaning, filtering and layout edits in diverse formatting. Scored automatically on 2729 test cases at cell-, sheet- and overall levels. General GPQA (Google-Proof Q&A): 448 multiple-choice questions written by domain experts in biology, physics, and chemistry, designed to be PhD-level. OpenAI found that human experts achieve an average score of 69.7% on the Diamond subset. It is composed of 3 sets: "Extended" with 546 problems, containing all problems solicited from writers; "Main" with 448 problems, which were an expert-validated subset from "Extended"; "Diamond" with 198 problems, which were the hardest problems from "Main". In the dataset, there is also an anonymized list of 60 experts who validated the dataset, and their qualifications. The inter-expert agreement on the Extended set is only 74%. The construction of the dataset cost ~$120K. Each question cost an average of 2 expert-hours. Each expert was paid $100/hr. SuperGPQA: 26,529 multiple-choice questions collected by domain experts in 285 graduate-level disciplines. The questions were collected by individuals with or pursuing a PhD and then refined and inspected with the help of large language models. MathVista: 6,141 questions involving quantitative reasoning that requires reading a picture to solve. AGIEval: questions from 20 official, public, and high-standard admission and qualification exams, such as SAT, Gaokao, law school admission tests, math competitions, lawyer qualification tests, and national civil service exams. OlympicArena: 11,163 problems from 62 distinct Olympic competitions. OlympiadBench: 8,476 math and physics problems in English and Chinese, sourced from International Olympiads, Chinese Olympiads, and Gaokao. ARC-AGI (Abstraction and Reasoning Corpus for Artificial General Intelligence): Given three pairs of before-and-after diagrams of applying a rule, apply the same rule to the fourth before-diagram. It is similar to a Raven's Progressive Matrices test. LiveBench: A series of benchmarks released monthly, including high school math competition questions, competitive coding questions, logic puzzles, and other tasks. Humanity's Last Exam: 3,000 multimodal questions across over a hundred academic subjects, with a held-out private dataset left unreleased to prevent contamination. 10% of questions requires both image and text comprehension and the rest are fully text-based. 80% of questions are scored by exact string matching, and the rest are multiple-choice. SimpleBench: A multiple-choice text benchmark with over 200 questions covering spatio-temporal reasoning, social intelligence, and linguistic adversarial robustness (or trick questions). It is designed to test "everyday human reasoning". Others HealthBench: 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses evaluated using 48,562 conversation-specific rubrics and scored by a model-based grader (GPT‑4.1). GDPval: tasks from 44 occupations in the top 9 sectors of the US GDP. Occupations chosen to have over 60% of its component tasks classified as not involving physical work or manual labor. Tasks constructed from the representative work of industry professionals. Models are evaluated by win-rate against human reference solution. See also List of large language models List of datasets for machine-learning research References Sources Hodak, Miro; Ellison, David; Van Buren, Chris; Jiang, Xiaotong; Dh