es from generative AI pornography While both deepfake pornography and generative AI pornography utilize synthetic media, they differ in approach and ethical implications. Generative AI pornography is created entirely through algorithms, producing hyper-realistic content unlinked to real individuals. In contrast, deepfake pornography alters existing footage of real individuals, often without consent, by superimposing faces or modifying scenes. Hany Farid, a digital image analysis expert, has emphasized these distinctions. However, open source technology is evolving. The availability of models like Flux and SDXL, plus new techniques for fine-tuning, have accelerated the ability for home users to create generative AI deepfake pornography. One can create a digital "consistent character" avatar of a real person and then use that avatar to fit into pornographic images or replace the original person in a scene.[1] Most deepfake pornography is made using the faces of people who did not consent to their image being used in such a sexual way. In 2023, Sensity, an identity verification company, has found that "96% of deepfakes are sexually explicit and feature women who didn't consent to the creation of the content." Combatting deepfake pornography Technical approach Deepfake detection has become an increasingly important area of research in recent years as the spread of fake videos and images has become more prevalent. One promising approach to detecting deepfakes is through the use of Convolutional Neural Networks (CNNs), which have shown high accuracy in distinguishing between real and fake images. One CNN-based algorithm that has been developed specifically for deepfake detection is DeepRhythm, which has demonstrated an impressive accuracy score of 0.98 (i.e. successful at detecting deepfake images 98% of the time). This algorithm utilizes a pre-trained CNN to extract features from facial regions of interest and then applies a novel attention mechanism to identify discrepancies between the original and manipulated images. While the development of more sophisticated deepfake technology presents ongoing challenges to detection efforts, the high accuracy of algorithms like DeepRhythm offers a promising tool for identifying and mitigating the spread of harmful deepfakes. Aside from detection models, there are also video authenticating tools available to the public. In 2019, Deepware launched the first publicly available detection tool which allowed users to easily scan and detect deepfake videos. Similarly, in 2020 Microsoft released a free and user-friendly video authenticator. Users upload a suspected video or input a link, and receive a confidence score to assess the level of manipulation in a deepfake. Legal approach As of 2023, there is a lack of legislation that specifically addresses deepfake pornography. Instead, the harm caused by its creation and distribution is being addressed by the courts through existing criminal and civil laws. In the United States, the Take It Down Act was passed in 2025, which addressed non-consensual intimate images as well as deepfake pornography. In the United States, despite 38 states have laws regarding AI CSAM in 2025, several loopholes in enforcement remains, including the lack of definition of "student on student" AI generated deepfake child pornography. Victims of deepfake pornography often have claims for revenge porn, tort claims, and harassment. The legal consequences for revenge porn vary from state to state and country to country. For instance, in Canada, the penalty for publishing non-consensual intimate images is up to 5 years in prison, whereas in Malta it is a fine of up to â‚¬5,000. The "Deepfake Accountability Act" was introduced to the United States Congress in 2019 but died in 2020. It aimed to make the production and distribution of digitally altered visual media that was not disclosed to be such, a criminal offense. The title specifies that making any sexual, non-consensual altered 