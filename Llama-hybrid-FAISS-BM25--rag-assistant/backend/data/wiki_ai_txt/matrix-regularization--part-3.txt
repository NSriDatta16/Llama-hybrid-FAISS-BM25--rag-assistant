Y_{ij}.} Spectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression. In this setting, a reduced rank coefficient matrix can be found by keeping just the top n {\displaystyle n} singular values, but this can be extended to keep any reduced set of singular values and vectors. Structured sparsity Sparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the Lasso method). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise ℓ 0 {\displaystyle \ell ^{0}} -norm of the matrix, but the ℓ 0 {\displaystyle \ell ^{0}} -norm is not convex. In practice this can be implemented by convex relaxation to the ℓ 1 {\displaystyle \ell ^{1}} -norm. While entry-wise regularization with an ℓ 1 {\displaystyle \ell ^{1}} -norm will find solutions with a small number of nonzero elements, applying an ℓ 1 {\displaystyle \ell ^{1}} -norm to different groups of variables can enforce structure in the sparsity of solutions. The most straightforward example of structured sparsity uses the ℓ p , q {\displaystyle \ell _{p,q}} norm with p = 2 {\displaystyle p=2} and q = 1 {\displaystyle q=1} : ‖ W ‖ 2 , 1 = ∑ i ‖ w i ‖ 2 . {\displaystyle \left\|W\right\|_{2,1}=\sum _{i}\left\|w_{i}\right\|_{2}.} For example, the ℓ 2 , 1 {\displaystyle \ell _{2,1}} norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group. The grouping effect is achieved by taking the ℓ 2 {\displaystyle \ell ^{2}} -norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the ℓ 2 {\displaystyle \ell ^{2}} -norms of each column. More generally, the ℓ 2 , 1 {\displaystyle \ell _{2,1}} norm can be applied to arbitrary groups of variables: R ( W ) = λ ∑ g G ∑ j | G g | | w g j | 2 = λ ∑ g G ‖ w g ‖ g {\displaystyle R(W)=\lambda \sum _{g}^{G}{\sqrt {\sum _{j}^{|G_{g}|}\left|w_{g}^{j}\right|^{2}}}=\lambda \sum _{g}^{G}\left\|w_{g}\right\|_{g}} where the index g {\displaystyle g} is across groups of variables, and | G g | {\displaystyle |G_{g}|} indicates the cardinality of group g {\displaystyle g} . Algorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via matching pursuit: and proximal gradient methods. By writing the proximal gradient with respect to a given coefficient, w g i {\displaystyle w_{g}^{i}} , it can be seen that this norm enforces a group-wise soft threshold prox λ , R g ⁡ ( w g ) i = ( w g i − λ w g i ‖ w g ‖ g ) 1 ‖ w g ‖ g ≥ λ . {\displaystyle \operatorname {prox} _{\lambda ,R_{g}}\left(w_{g}\right)^{i}=\left(w_{g}^{i}-\lambda {\frac {w_{g}^{i}}{\left\|w_{g}\right\|_{g}}}\right)\mathbf {1} _{\|w_{g}\|_{g}\geq \lambda }.} where 1 ‖ w g ‖ g ≥ λ {\displaystyle \mathbf {1} _{\|w_{g}\|_{g}\geq \lambda }} is the indicator function for group norms ≥ λ {\displaystyle \geq \lambda } . Thus, using ℓ 2 , 1 {\displaystyle \ell _{2,1}} norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix Y {\displaystyle Y} ) will depend on the same sparse set of input variables. Multiple kernel selection The ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning. This can be useful when there are multiple types of input data (color and texture, for example) with diffe