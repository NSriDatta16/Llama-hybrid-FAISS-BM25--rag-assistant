reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a "skeptic penalty function," (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-Npar/2), where Npar is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references). Learning in NMF using dynamic logic algorithm The learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in MN items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic. An important aspect of dynamic logic is matching vagueness or fuzziness of similarity measures to the uncertainty of models. Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases. The maximization of similarity L is done as follows. First, the unknown parameters {Sm} are randomly initialized. Then the association variables f(m|n) are computed, f ( m | n ) = r ( m ) l ( X → ( n | m ) ) ∑ m ′ = 1 M r ( m ′ ) l ( X → ( n | m ′ ) ) {\displaystyle f(m|n)={\frac {r(m)l({\vec {X}}(n|m))}{\sum _{m'=1}^{M}{r(m')l({\vec {X}}(n|m'))}}}} (3). Equation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows: d S → m d t = ∑ n = 1 N f ( m | n ) ∂ ln ⁡ l ( n | m ) ∂ M → m ∂ M → m ∂ S → m {\displaystyle {\frac {d{\vec {S}}_{m}}{dt}}=\sum _{n=1}^{N}{f(m|n){\frac {\partial {\ln l(n|m)}}{\partial {{\vec {M}}_{m}}}}{\frac {\partial {{\vec {M}}_{m}}}{\partial {{\vec {S}}_{m}}}}}} (4). d f ( m | n ) d t = f ( m | n ) ∑ m ′ = 1 M [ δ m m ′ − f ( m ′ | n ) ] ∂ ln ⁡ l ( n | m ′ ) ∂ M → m ′ ∂ M → m ′ ∂ S → m ′ d S → m ′ d t {\displaystyle {\frac {df(m|n)}{dt}}=f(m|n)\sum _{m'=1}^{M}{[\delta _{mm'}-f(m'|n)]{\frac {\partial {\ln l(n|m')}}{\partial {{\vec {M}}_{m'}}}}}{\frac {\partial {{\vec {M}}_{m'}}}{\partial {{\vec {S}}_{m'}}}}{\frac {d{\vec {S}}_{m'}}{dt}}} (5) The following theorem has been proved (Perlovsky 2001): Theorem. Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{Sm}L. It follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {Sm} are asymptotically unbiased and efficient estimates of these parameters. The computational complexity of dynamic logic is linear in N. Practically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5). The proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning. Example of dynamic logic operations Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shap