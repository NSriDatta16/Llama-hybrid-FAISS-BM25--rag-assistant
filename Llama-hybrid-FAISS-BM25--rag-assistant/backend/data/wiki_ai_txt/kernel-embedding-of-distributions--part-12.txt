ay perform well even in the presence of conditional shifts other than location-scale changes. Domain generalization via invariant feature representation Given N sets of training examples sampled i.i.d. from distributions P ( 1 ) ( X , Y ) , P ( 2 ) ( X , Y ) , … , P ( N ) ( X , Y ) {\displaystyle P^{(1)}(X,Y),P^{(2)}(X,Y),\ldots ,P^{(N)}(X,Y)} , the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain P ∗ ( X , Y ) {\displaystyle P^{*}(X,Y)} where no data from the test domain is available at training time. If conditional distributions P ( Y ∣ X ) {\displaystyle P(Y\mid X)} are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals P ( X ) {\displaystyle P(X)} . Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains. DICA thus extracts invariants, features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis, transfer component analysis, and covariance operator inverse regression. Defining a probability distribution P {\displaystyle {\mathcal {P}}} on the RKHS H {\displaystyle {\mathcal {H}}} with P ( μ X ( i ) Y ( i ) ) = 1 N for i = 1 , … , N , {\displaystyle {\mathcal {P}}\left(\mu _{X^{(i)}Y^{(i)}}\right)={\frac {1}{N}}\qquad {\text{ for }}i=1,\dots ,N,} DICA measures dissimilarity between domains via distributional variance which is computed as V H ( P ) = 1 N tr ⁡ ( G ) − 1 N 2 ∑ i , j = 1 N G i j {\displaystyle V_{\mathcal {H}}({\mathcal {P}})={\frac {1}{N}}\operatorname {tr} (\mathbf {G} )-{\frac {1}{N^{2}}}\sum _{i,j=1}^{N}\mathbf {G} _{ij}} where G i j = ⟨ μ X ( i ) , μ X ( j ) ⟩ H {\displaystyle \mathbf {G} _{ij}=\left\langle \mu _{X^{(i)}},\mu _{X^{(j)}}\right\rangle _{\mathcal {H}}} so G {\displaystyle \mathbf {G} } is a N × N {\displaystyle N\times N} Gram matrix over the distributions from which the training data are sampled. Finding an orthogonal transform onto a low-dimensional subspace B (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that B aligns with the bases of a central subspace C for which Y {\displaystyle Y} becomes independent of X {\displaystyle X} given C T X {\displaystyle C^{T}X} across all domains. In the absence of target values Y {\displaystyle Y} , an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of X {\displaystyle X} (in the feature space) across all domains (rather than preserving a central subspace). Distribution regression In distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images. Given ( { X i , n } n = 1 N i , y i ) i = 1 ℓ {\displaystyle {\left(\{X_{i,n}\}_{n=1}^{N_{i}},y_{i}\right)}_{i=1}^{\ell }} training data, where the X i ^ := { X i , n } n = 1 N i {\displaystyle {\hat {X_{i}}}:=\{X_{i,n}\}_{n=1}^{N_{i}}} bag contains samples from a probability distribution X i 