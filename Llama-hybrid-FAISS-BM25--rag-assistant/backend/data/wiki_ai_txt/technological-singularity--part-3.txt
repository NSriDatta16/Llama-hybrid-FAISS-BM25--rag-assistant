dence of 50% that human-level AI would be developed by 2040–2050 was the outcome of four informal polls of AI researchers, conducted in 2012 and 2013 by Bostrom and Müller. In September 2025, a review of surveys of scientists and industry experts from the previous 15 years found that most agreed that artificial general intelligence (AGI), a level well below technological singularity, will occur by 2100. A more recent analysis by AIMultiple reported, "Current surveys of AI researchers are predicting AGI around 2040". Plausibility Prominent technologists and academics who dispute the plausibility of a technological singularity include Paul Allen, Jeff Hawkins, John Holland, Jaron Lanier, Steven Pinker, Theodore Modis, and Gordon Moore, whose law is often cited in support of the concept.Most proposed methods for creating superhuman or transhuman minds fall into two categories: intelligence amplification of human brains and artificial intelligence. The many speculated ways to augment human intelligence include bioengineering, genetic engineering, nootropic drugs, AI assistants, direct brain–computer interfaces, and mind uploading. Robin Hanson has expressed skepticism of human intelligence augmentation, writing that once the "low-hanging fruit" of easy methods for increasing human intelligence have been exhausted, further improvements will become increasingly difficult. The possibility of an intelligence explosion depends on three factors. The first accelerating factor is the new intelligence enhancements made possible by each previous improvement. But as the intelligences become more advanced, further advances will become more and more complicated, possibly outweighing the advantage of increased intelligence. Each improvement should generate at least one more improvement, on average, for movement toward singularity to continue. Finally, the laws of physics may eventually prevent further improvement. There are two logically independent, but mutually reinforcing, causes of intelligence improvements: increases in the speed of computation and improvements to the algorithms used. The former is predicted by Moore's Law and the forecasted improvements in hardware, and is comparatively similar to previous technological advances. "Most experts believe that Moore's law is coming to an end during this decade", the AIMultiple report reads, but "quantum computing can be used to efficiently train neural networks", potentially working around any end to Moore's Law. But Schulman and Sandberg argue that software will present more complex challenges than simply operating on hardware capable of running at human intelligence levels or beyond. A 2017 email survey of authors with publications at the 2015 NeurIPS and ICML machine learning conferences asked about the chance that "the intelligence explosion argument is broadly correct". Of the respondents, 12% said it was "quite likely", 17% said it was "likely", 21% said it was "about even", 24% said it was "unlikely", and 26% said it was "quite unlikely". Speed improvements Both for human and artificial intelligence, hardware improvements increase the rate of future hardware improvements. Some upper limit on speed may eventually be reached. Jeff Hawkins has said that a self-improving computer system will inevitably run into limits on computing power: "in the end there are limits to how big and fast computers can run. We would end up in the same place; we'd just get there a bit faster. There would be no singularity." It is difficult to directly compare silicon-based hardware with neurons. But Anthony Berglas notes that computer speech recognition is approaching human capabilities, and that this capability seems to require 0.01% of the volume of the brain. This analogy suggests that modern computer hardware is within a few orders of magnitude of being as powerful as the human brain, as well as taking up much less space. The costs of training systems with deep learning may be larger. Exponential growth Th