\\0&\dots &\sigma _{l}\\\end{bmatrix}}&\cdot &{\begin{bmatrix}{\begin{bmatrix}&&{\textbf {v}}_{1}&&\end{bmatrix}}\\\vdots \\{\begin{bmatrix}&&{\textbf {v}}_{l}&&\end{bmatrix}}\end{bmatrix}}\end{matrix}}} The values σ 1 , … , σ l {\displaystyle \sigma _{1},\dots ,\sigma _{l}} are called the singular values, and u 1 , … , u l {\displaystyle u_{1},\dots ,u_{l}} and v 1 , … , v l {\displaystyle v_{1},\dots ,v_{l}} the left and right singular vectors. Notice the only part of U {\displaystyle U} that contributes to t i {\displaystyle {\textbf {t}}_{i}} is the i 'th {\displaystyle i{\textrm {'th}}} row. Let this row vector be called t ^ i T {\displaystyle {\hat {\textrm {t}}}_{i}^{T}} . Likewise, the only part of V T {\displaystyle V^{T}} that contributes to d j {\displaystyle {\textbf {d}}_{j}} is the j 'th {\displaystyle j{\textrm {'th}}} column, d ^ j {\displaystyle {\hat {\textrm {d}}}_{j}} . These are not the eigenvectors, but depend on all the eigenvectors. It turns out that when you select the k {\displaystyle k} largest singular values, and their corresponding singular vectors from U {\displaystyle U} and V {\displaystyle V} , you get the rank k {\displaystyle k} approximation to X {\displaystyle X} with the smallest error (Frobenius norm). This approximation has a minimal error. But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector t ^ i T {\displaystyle {\hat {\textbf {t}}}_{i}^{T}} then has k {\displaystyle k} entries mapping it to a lower-dimensional space. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector d ^ j {\displaystyle {\hat {\textbf {d}}}_{j}} is an approximation in this lower-dimensional space. We write this approximation as X k = U k Σ k V k T {\displaystyle X_{k}=U_{k}\Sigma _{k}V_{k}^{T}} You can now do the following: See how related documents j {\displaystyle j} and q {\displaystyle q} are in the low-dimensional space by comparing the vectors Σ k ⋅ d ^ j {\displaystyle \Sigma _{k}\cdot {\hat {\textbf {d}}}_{j}} and Σ k ⋅ d ^ q {\displaystyle \Sigma _{k}\cdot {\hat {\textbf {d}}}_{q}} (typically by cosine similarity). Comparing terms i {\displaystyle i} and p {\displaystyle p} by comparing the vectors Σ k ⋅ t ^ i {\displaystyle \Sigma _{k}\cdot {\hat {\textbf {t}}}_{i}} and Σ k ⋅ t ^ p {\displaystyle \Sigma _{k}\cdot {\hat {\textbf {t}}}_{p}} . Note that t ^ {\displaystyle {\hat {\textbf {t}}}} is now a column vector. Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine. Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space. To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents: d ^ j = Σ k − 1 U k T d j {\displaystyle {\hat {\textbf {d}}}_{j}=\Sigma _{k}^{-1}U_{k}^{T}{\textbf {d}}_{j}} Note here that the inverse of the diagonal matrix Σ k {\displaystyle \Sigma _{k}} may be found by inverting each nonzero value within the matrix. This means that if you have a query vector q {\displaystyle q} , you must do the translation q ^ = Σ k − 1 U k T q {\displaystyle {\hat {\textbf {q}}}=\Sigma _{k}^{-1}U_{k}^{T}{\textbf {q}}} before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors: t i T = t ^ i T Σ k V k T {\displaystyle {\textbf {t}}_{i}^{T}={\hat {\textbf {t}}}_{i}^{T}\Sigma _{k}V_{k}^{T}} t ^ i T = t i T V k − T Σ k − 1 = t i T V k Σ k − 1 {\displaystyle {\hat {\textbf {t}}}_{i}^{T}={\textbf {t}}_{i}^{T}V_{k}^{-T}\Sigma _{k}^{-1}={\textbf {t}}_{i}^{T}V_{k}\Sigma _{k}^{-1}} t ^ i = Σ k − 1 V k T t i {\displaystyle {\hat {\textbf {t}}}_{i}=\Sigma _{k}^{-1}V_{k}^{T}{\textbf {t}}_{i}} Applications The 