create an embedded representation of the knowledge graph, which can be seen as a compression of the matrix product, but is more computationally efficient and scalable while keeping the capabilities to express asymmetric relation since the circular correlation is not commutative. HolE links holographic and complex embeddings since, if used together with Fourier, can be seen as a special case of ComplEx. TuckER: TuckER sees the knowledge graph as a tensor that could be decomposed using the Tucker decomposition in a collection of vectors—i.e., the embeddings of entities and relations—with a shared core. The weights of the core tensor are learned together with the embeddings and represent the level of interaction of the entries. Each entity and relation has its own embedding dimension, and the size of the core tensor is determined by the shape of the entities and relations that interact. The embedding of the subject and object of a fact are summed in the same way, making TuckER fully expressive, and other embedding models such as RESCAL, DistMult, ComplEx, and SimplE can be expressed as a special formulation of TuckER. MEI: MEI introduces the multi-partition embedding interaction technique with the block term tensor format, which is a generalization of CP decomposition and Tucker decomposition. It divides the embedding vector into multiple partitions and learns the local interaction patterns from data instead of using fixed special patterns as in ComplEx or SimplE models. This enables MEI to achieve optimal efficiency—expressiveness trade-off, not just being fully expressive. Previous models such as TuckER, RESCAL, DistMult, ComplEx, and SimplE are suboptimal restricted special cases of MEI. MEIM: MEIM goes beyond the block term tensor format to introduce the independent core tensor for ensemble boosting effects and the soft orthogonality for max-rank relational mapping, in addition to multi-partition embedding interaction. MEIM generalizes several previous models such as MEI and its subsumed models, RotaE, and QuatE. MEIM improves expressiveness while still being highly efficient in practice, helping it achieve good results using fairly small model sizes. Geometric models The geometric space defined by this family of models encodes the relation as a geometric transformation between the head and tail of a fact. For this reason, to compute the embedding of the tail, it is necessary to apply a transformation τ {\displaystyle \tau } to the head embedding, and a distance function δ {\displaystyle \delta } is used to measure the goodness of the embedding or to score the reliability of a fact. f r ( h , t ) = δ ( τ ( h , r ) , t ) {\displaystyle {\mathcal {f}}_{r}(h,t)=\delta (\tau (h,r),t)} Geometric models are similar to the tensor decomposition model, but the main difference between the two is that they have to preserve the applicability of the transformation τ {\displaystyle \tau } in the geometric space in which it is defined. Pure translational models This class of models is inspired by the idea of translation invariance introduced in word2vec. A pure translational model relies on the fact that the embedding vector of the entities are close to each other after applying a proper relational translation in the geometric space in which they are defined. In other words, given a fact, the embedding of the head plus the embedding of the relation should equal the embedding of the tail. The closeness of the entities embedding is given by some distance measure and quantifies the reliability of a fact. TransE: Uses a scoring function that forces the embeddings to satisfy a simple vector sum equation in each fact in which they appear: h + r = t {\displaystyle h+r=t} . The embedding will be exact if each entity and relation appears in only one fact, and so in practice is poor at representing one-to-many, many-to-one, and asymmetric relations. TransH: A modification of TransE for representing types of relations, by using a hyperplane as a geome