erate alternative versions of the time series, representing what might happen over non-specific time-periods in the future Simple or fully formed statistical models to describe the likely outcome of the time series in the immediate future, given knowledge of the most recent outcomes (forecasting). Forecasting on time series is usually done using automated statistical software packages and programming languages, such as Julia, Python, R, SAS, SPSS and many others. Forecasting on large scale data can be done with Apache Spark using the Spark-TS library, a third-party package. Classification Assigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language. Segmentation Splitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system. Clustering Time series data may be clustered, however special care has to be taken when considering subsequence clustering. Time series clustering may be split into whole time series clustering (multiple time series for which to find a cluster) subsequence time series clustering (single timeseries, split into chunks using sliding windows) time point clustering Subsequence time series clustering Subsequence time series clustering resulted in unstable (random) clusters induced by the feature extraction using chunking with sliding windows. It was found that the cluster centers (the average of the time series in a cluster - also a time series) follow an arbitrarily shifted sine pattern (regardless of the dataset, even on realizations of a random walk). This means that the found cluster centers are non-descriptive for the dataset because the cluster centers are always nonrepresentative sine waves. Models Classical models (AR, ARMA, ARIMA, and well-known variations) Models for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the autoregressive (AR) models, the integrated (I) models, and the moving-average (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving-average (ARMA) and autoregressive integrated moving-average (ARIMA) models. The autoregressive fractionally integrated moving-average (ARFIMA) model generalizes the former three. Another important generalization is the time-varying autoregressive (TVAR) model, in which the AR coefficients are allowed to change over time, enabling the model to capture evolving or non-stationary dynamics. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial "V" for "vector", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some "forcing" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final "X" for "exogenous". Time-varying autoregressive (TVAR) models Time-varying autoregressive (TVAR) models are especially useful for analyzing non-stationary time 