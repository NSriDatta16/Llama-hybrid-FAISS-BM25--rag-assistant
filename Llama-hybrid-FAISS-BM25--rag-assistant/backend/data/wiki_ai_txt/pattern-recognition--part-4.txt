he feature vector input is x {\displaystyle {\boldsymbol {x}}} , and the function f is typically parameterized by some parameters θ {\displaystyle {\boldsymbol {\theta }}} . In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability p ( x | l a b e l ) {\displaystyle p({{\boldsymbol {x}}|{\rm {label}}})} is instead estimated and combined with the prior probability p ( l a b e l | θ ) {\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})} using Bayes' rule, as follows: p ( l a b e l | x , θ ) = p ( x | l a b e l , θ ) p ( l a b e l | θ ) ∑ L ∈ all labels p ( x | L ) p ( L | θ ) . {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\sum _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})}}.} When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation: p ( l a b e l | x , θ ) = p ( x | l a b e l , θ ) p ( l a b e l | θ ) ∫ L ∈ all labels p ( x | L ) p ( L | θ ) d ⁡ L . {\displaystyle p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})={\frac {p({{\boldsymbol {x}}|{\rm {label,{\boldsymbol {\theta }}}}})p({\rm {label|{\boldsymbol {\theta }}}})}{\int _{L\in {\text{all labels}}}p({\boldsymbol {x}}|L)p(L|{\boldsymbol {\theta }})\operatorname {d} L}}.} The value of θ {\displaystyle {\boldsymbol {\theta }}} is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability p ( θ ) {\displaystyle p({\boldsymbol {\theta }})} on different values of θ {\displaystyle {\boldsymbol {\theta }}} . Mathematically: θ ∗ = arg ⁡ max θ p ( θ | D ) {\displaystyle {\boldsymbol {\theta }}^{*}=\arg \max _{\boldsymbol {\theta }}p({\boldsymbol {\theta }}|\mathbf {D} )} where θ ∗ {\displaystyle {\boldsymbol {\theta }}^{*}} is the value used for θ {\displaystyle {\boldsymbol {\theta }}} in the subsequent evaluation procedure, and p ( θ | D ) {\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )} , the posterior probability of θ {\displaystyle {\boldsymbol {\theta }}} , is given by p ( θ | D ) = [ ∏ i = 1 n p ( y i | x i , θ ) ] p ( θ ) . {\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )=\left[\prod _{i=1}^{n}p(y_{i}|{\boldsymbol {x}}_{i},{\boldsymbol {\theta }})\right]p({\boldsymbol {\theta }}).} In the Bayesian approach to this problem, instead of choosing a single parameter vector θ ∗ {\displaystyle {\boldsymbol {\theta }}^{*}} , the probability of a given label for a new instance x {\displaystyle {\boldsymbol {x}}} is computed by integrating over all possible values of θ {\displaystyle {\boldsymbol {\theta }}} , weighted according to the posterior probability: p ( l a b e l | x ) = ∫ p ( l a b e l | x , θ ) p ( θ | D ) d ⁡ θ . {\displaystyle p({\rm {label}}|{\boldsymbol {x}})=\int p({\rm {label}}|{\boldsymbol {x}},{\boldsymbol {\theta }})p({\boldsymbol {\theta }}|\mathbf {D} )\operatorname {d} {\boldsymbol {\theta }}.} Frequentist or Bayesian approach to pattern recognition The first pattern classifier – the linear discriminant presented by Fisher – was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix. Also the probability of each class p ( l a b e l | θ ) {\displ