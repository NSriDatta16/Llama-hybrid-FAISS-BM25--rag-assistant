 z ) = V ( f ( x ) , y ) {\displaystyle V(f,z)=V(f(x),y)} . The empirical error of f {\displaystyle f} is I S [ f ] = 1 n ∑ V ( f , z i ) {\displaystyle I_{S}[f]={\frac {1}{n}}\sum V(f,z_{i})} . The true error of f {\displaystyle f} is I [ f ] = E z V ( f , z ) {\displaystyle I[f]=\mathbb {E} _{z}V(f,z)} Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows: By removing the i-th element S | i = { z 1 , . . . , z i − 1 , z i + 1 , . . . , z m } {\displaystyle S^{|i}=\{z_{1},...,\ z_{i-1},\ z_{i+1},...,\ z_{m}\}} By replacing the i-th element S i = { z 1 , . . . , z i − 1 , z i ′ , z i + 1 , . . . , z m } {\displaystyle S^{i}=\{z_{1},...,\ z_{i-1},\ z_{i}',\ z_{i+1},...,\ z_{m}\}} Definitions of stability Hypothesis Stability An algorithm L {\displaystyle L} has hypothesis stability β with respect to the loss function V if the following holds: ∀ i ∈ { 1 , . . . , m } , E S , z [ | V ( f S , z ) − V ( f S | i , z ) | ] ≤ β . {\displaystyle \forall i\in \{1,...,m\},\mathbb {E} _{S,z}[|V(f_{S},z)-V(f_{S^{|i}},z)|]\leq \beta .} Point-wise Hypothesis Stability An algorithm L {\displaystyle L} has point-wise hypothesis stability β with respect to the loss function V if the following holds: ∀ i ∈ { 1 , . . . , m } , E S [ | V ( f S , z i ) − V ( f S | i , z i ) | ] ≤ β . {\displaystyle \forall i\in \ \{1,...,m\},\mathbb {E} _{S}[|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|]\leq \beta .} Error Stability An algorithm L {\displaystyle L} has error stability β with respect to the loss function V if the following holds: ∀ S ∈ Z m , ∀ i ∈ { 1 , . . . , m } , | E z [ V ( f S , z ) ] − E z [ V ( f S | i , z ) ] | ≤ β {\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},|\mathbb {E} _{z}[V(f_{S},z)]-\mathbb {E} _{z}[V(f_{S^{|i}},z)]|\leq \beta } Uniform Stability An algorithm L {\displaystyle L} has uniform stability β with respect to the loss function V if the following holds: ∀ S ∈ Z m , ∀ i ∈ { 1 , . . . , m } , sup z ∈ Z | V ( f S , z ) − V ( f S | i , z ) | ≤ β {\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},\sup _{z\in Z}|V(f_{S},z)-V(f_{S^{|i}},z)|\leq \beta } A probabilistic version of uniform stability β is: ∀ S ∈ Z m , ∀ i ∈ { 1 , . . . , m } , P S { sup z ∈ Z | V ( f S , z ) − V ( f S | i , z ) | ≤ β } ≥ 1 − δ {\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z)-V(f_{S^{|i}},z)|\leq \beta \}\geq 1-\delta } An algorithm is said to be stable, when the value of β {\displaystyle \beta } decreases as O ( 1 m ) {\displaystyle O({\frac {1}{m}})} . Leave-one-out cross-validation (CVloo) Stability An algorithm L {\displaystyle L} has CVloo stability β with respect to the loss function V if the following holds: ∀ i ∈ { 1 , . . . , m } , P S { | V ( f S , z i ) − V ( f S | i , z i ) | ≤ β C V } ≥ 1 − δ C V {\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|\leq \beta _{CV}\}\geq 1-\delta _{CV}} The definition of (CVloo) Stability is equivalent to Pointwise-hypothesis stability seen earlier. Expected-leave-one-out error ( E l o o e r r {\displaystyle Eloo_{err}} ) Stability An algorithm L {\displaystyle L} has E l o o e r r {\displaystyle Eloo_{err}} stability if for each n there exists a β E L m {\displaystyle \beta _{EL}^{m}} and a δ E L m {\displaystyle \delta _{EL}^{m}} such that: ∀ i ∈ { 1 , . . . , m } , P S { | I [ f S ] − 1 m ∑ i = 1 m V ( f S | i , z i ) | ≤ β E L m } ≥ 1 − δ E L m {\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{|I[f_{S}]-{\frac {1}{m}}\sum _{i=1}^{m}V(f_{S^{|i}},z_{i})|\leq \beta _{EL}^{m}\}\geq 1-\delta _{EL}^{m}} , with β E L m {\displaystyle \beta _{EL}^{m}} and δ E L m {\displaystyle \delta _{EL}^{m}} going to zero for m , → ∞ {\displaystyle m,\rightarrow \infty } Classic theorems From Bousquet and Elisseeff (02): For symmetric learning algorithms with bounded loss, if the algorithm has Uniform Stability with the probabilistic definition above, then the al