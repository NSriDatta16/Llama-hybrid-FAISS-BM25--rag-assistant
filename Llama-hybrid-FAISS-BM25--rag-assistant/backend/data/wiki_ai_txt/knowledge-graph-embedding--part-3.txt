 drug and a disease by using a biomedical knowledge graph built leveraging the availability of massive literature and biomedical databases. Knowledge graph embedding can also be used in the domain of social politics. Models Given a collection of triples (or facts) F = { ⟨ head , relation , tail ⟩ } {\displaystyle {\mathcal {F}}=\{\langle {\text{head}},{\text{relation}},{\text{tail}}\rangle \}} , the knowledge graph embedding model produces, for each entity and relation present in the knowledge graph a continuous vector representation. ( h , r , t ) {\displaystyle (h,r,t)} is the corresponding embedding of a triple with h , t ∈ I R d {\displaystyle h,t\in {\rm {I\!R}}^{d}} and r ∈ I R k {\displaystyle r\in {\rm {I\!R}}^{k}} , where d {\displaystyle d} is the embedding dimension for the entities, and k {\displaystyle k} for the relations. The score function of a given model is denoted by f r ( h , t ) {\displaystyle {\mathcal {f}}_{r}(h,t)} and measures the distance of the embedding of the head from the embedding of tail given the embedding of the relation. In other words, it quantifies the plausibility of the embedded representation of a given fact. Rossi et al. propose a taxonomy of the embedding models and identifies three main families of models: tensor decomposition models, geometric models, and deep learning models. Tensor decomposition model The tensor decomposition is a family of knowledge graph embedding models that use a multi-dimensional matrix to represent a knowledge graph, that is partially knowable due to gaps of the graph describing a particular domain thoroughly. In particular, these models use a third-order (3D) tensor, which is then factorized into low-dimensional vectors that are the embeddings. A third-order tensor is suitable for representing a knowledge graph because it records only the existence or absence of a relation between entities, and so is simple, and there is no need to know a priori the network structure, making this class of embedding models light, and easy to train even if they suffer from high-dimensionality and sparsity of data. Bilinear models This family of models uses a linear equation to embed the connection between the entities through a relation. In particular, the embedded representation of the relations is a bidimensional matrix. These models, during the embedding procedure, only use the single facts to compute the embedded representation and ignore the other associations to the same entity or relation. DistMult: Since the embedding matrix of the relation is a diagonal matrix, the scoring function can not distinguish asymmetric facts. ComplEx: As DistMult uses a diagonal matrix to represent the relations embedding but adds a representation in the complex vector space and the hermitian product, it can distinguish symmetric and asymmetric facts. This approach is scalable to a large knowledge graph in terms of time and space cost. ANALOGY: This model encodes in the embedding the analogical structure of the knowledge graph to simulate inductive reasoning. Using a differentiable objective function, ANALOGY has good theoretical generality and computational scalability. It is proven that the embedding produced by ANALOGY fully recovers the embedding of DistMult, ComplEx, and HolE. SimplE: This model is the improvement of canonical polyadic decomposition (CP), in which an embedding vector for the relation and two independent embedding vectors for each entity are learned, depending on whether it is a head or a tail in the knowledge graph fact. SimplE resolves the problem of independent learning of the two entity embeddings using an inverse relation and average the CP score of ( h , r , t ) {\displaystyle (h,r,t)} and ( t , r − 1 , h ) {\displaystyle (t,r^{-1},h)} . In this way, SimplE collects the relation between entities while they appear in the role of subject or object inside a fact, and it is able to embed asymmetric relations. Non-bilinear models HolE: HolE uses circular correlation to 