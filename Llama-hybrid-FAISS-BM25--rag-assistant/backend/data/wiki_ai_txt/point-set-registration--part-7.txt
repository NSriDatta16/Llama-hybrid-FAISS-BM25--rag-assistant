re. Given a set of variables { Q j } {\displaystyle \lbrace Q_{j}\rbrace } where Q j ∈ R 1 {\displaystyle Q_{j}\in \mathbb {R} ^{1}} . A variable μ j {\displaystyle \mu _{j}} is associated with each Q j {\displaystyle Q_{j}} such that ∑ j = 1 J μ j = 1 {\textstyle \sum _{j=1}^{J}\mu _{j}=1} . The goal is to find μ {\displaystyle \mathbf {\mu } } that maximizes ∑ j = 1 J μ j Q j {\textstyle \sum _{j=1}^{J}\mu _{j}Q_{j}} . This can be formulated as a continuous problem by introducing a control parameter β > 0 {\displaystyle \beta >0} . In the deterministic annealing method, the control parameter β {\displaystyle \beta } is slowly increased as the algorithm runs. Let μ {\displaystyle \mathbf {\mu } } be: this is known as the softmax function. As β {\displaystyle \beta } increases, it approaches a binary value as desired in Equation (rpm.1). The problem may now be generalized to the 2D case, where instead of maximizing ∑ j = 1 J μ j Q j {\textstyle \sum _{j=1}^{J}\mu _{j}Q_{j}} , the following is maximized: where Q i j = − ( ‖ s j − t − A m i ‖ 2 − α ) = − ∂ cost ∂ μ i j {\displaystyle Q_{ij}=-(\lVert s_{j}-\mathbf {t} -\mathbf {A} m_{i}\rVert ^{2}-\alpha )=-{\frac {\partial \operatorname {cost} }{\partial \mu _{ij}}}} This is straightforward, except that now the constraints on μ {\displaystyle \mu } are doubly stochastic matrix constraints: ∀ j ∑ i = 1 M μ i j = 1 {\textstyle \forall j~\sum _{i=1}^{M}\mu _{ij}=1} and ∀ i ∑ j = 1 N μ i j = 1 {\textstyle \forall i~\sum _{j=1}^{N}\mu _{ij}=1} . As such the denominator from Equation (rpm.3) cannot be expressed for the 2D case simply. To satisfy the constraints, it is possible to use a result due to Sinkhorn, which states that a doubly stochastic matrix is obtained from any square matrix with all positive entries by the iterative process of alternating row and column normalizations. Thus the algorithm is written as such: algorithm RPM2D ( M , S ) {\displaystyle ({\mathcal {M}},{\mathcal {S}})} t := 0 a, θ b, c := 0 β := β0 μ ^ i j := 1 + ϵ {\displaystyle {\hat {\mu }}_{ij}:=1+\epsilon } while β < βf: while μ has not converged: // update correspondence parameters by softassign Q i j := − ∂ cost ∂ μ i j {\displaystyle Q_{ij}:=-{\frac {\partial \operatorname {cost} }{\partial \mu _{ij}}}} μ i j 0 := exp ⁡ ( β Q i j ) {\displaystyle \mu _{ij}^{0}:=\exp(\beta Q_{ij})} // apply Sinkhorn's method while μ ^ {\displaystyle {\hat {\mu }}} has not converged: // update μ ^ {\displaystyle {\hat {\mu }}} by normalizing across all rows: μ ^ i j 1 := μ ^ i j 0 ∑ i = 1 M + 1 μ ^ i j 0 {\displaystyle {\hat {\mu }}_{ij}^{1}:={\frac {{\hat {\mu }}_{ij}^{0}}{\sum _{i=1}^{M+1}{\hat {\mu }}_{ij}^{0}}}} // update μ ^ {\displaystyle {\hat {\mu }}} by normalizing across all columns: μ ^ i j 0 := μ ^ i j 1 ∑ j = 1 N + 1 μ ^ i j 1 {\displaystyle {\hat {\mu }}_{ij}^{0}:={\frac {{\hat {\mu }}_{ij}^{1}}{\sum _{j=1}^{N+1}{\hat {\mu }}_{ij}^{1}}}} // update pose parameters by coordinate descent update θ using analytical solution update t using analytical solution update a, b, c using Newton's method β := β r β {\displaystyle \beta :=\beta _{r}\beta } γ := γ β r {\displaystyle \gamma :={\frac {\gamma }{\beta _{r}}}} return a, b, c, θ and t where the deterministic annealing control parameter β {\displaystyle \beta } is initially set to β 0 {\displaystyle \beta _{0}} and increases by factor β r {\displaystyle \beta _{r}} until it reaches the maximum value β f {\displaystyle \beta _{f}} . The summations in the normalization steps sum to M + 1 {\displaystyle M+1} and N + 1 {\displaystyle N+1} instead of just M {\displaystyle M} and N {\displaystyle N} because the constraints on μ {\displaystyle \mu } are inequalities. As such the M + 1 {\displaystyle M+1} th and N + 1 {\displaystyle N+1} th elements are slack variables. The algorithm can also be extended for point sets in 3D or higher dimensions. The constraints on the correspondence matrix μ {\displaystyle \mathbf {\mu } } are the same in the 3D case as in the 2D case. H