enging to the SOTA models of the time (2019) since the original had been saturated. Includes 8 additional tasks (e.g. logical reasoning, commonsense inference, coreference resolution). Big-Bench (Beyond the Imitation Game): A benchmark collection of 204 tasks. A particular subset of 23 tasks is called BBH (Big-Bench Hard). An adversarial variant of BBH is called BBEH (Big-Bench Extra Hard), made by replacing each of the 23 tasks from BBH with a similar but adversarial variant. MMLU (Measuring Massive Multitask Language Understanding): 16,000 multiple-choice questions spanning 57 academic subjects including mathematics, philosophy, law, and medicine. Upgraded to MMLU-Pro which increases the number of choices from 4 to 10, eliminated the trivial and noisy questions from MMLU, and added harder problems. MMMLU (Multilingual MMLU): The test set of MMLU, translated into 14 languages by professional human translators. CMMLU (Chinese MMLU): 1,528 multiple-choice questions across 67 subjects, 16 of which are "China-specific", like Classical Chinese. Some data collected from non-publicly available materials, mock exam questions, and questions from quiz shows to avoid contamination. More than 80% of the data was crawled from PDFs after OCR. Multimodal Some benchmarks specifically test for multimodal ability, usually between text, image, video, and audio. MMMU (Massive Multi-discipline Multimodal Understanding): A vision-language version of MMLU. 11550 questions collected from college exams, quizzes, and textbooks, covering 30 subjects. The questions require image-understanding to solve. Includes multiple-choice questions and open-ended QA (which are scored by regex extraction). Human expert baseline is 89%. VideoMMMU: Like MMMU, but with videos. Contains 300 college-level lecture videos in 30 subjects in 6 disciplines (Art, Business, Science, Medicine, Humanities, and Engineering), with 900 questions. MMMU-Pro: 1730 multiple-choice multimodal questions in the same format as MMMU, designed to be adversarial against text-only models. Some problems in MMMU turned out to be answerable without looking at the images, necessitating MMMU-Pro. Each question has 10 choices, and presented in both text-image format, and screenshot/photo format. Vibe-Eval: 269 visual understanding prompts, with standard responses written by experts. Of these, 100 were "hard" meaning they could not be solved by an LLM (Reka Core) at the time of publication. Automatic scoring by LLMs. MMT-Bench is designed to assess LVLMs performance on massive multimodal tasks that involve expert knowledge, visual recognition, localization, reasoning, and planning. The test bench includes 31,325 multi-choice questions from visual multimodal scenarios (like driving and navigation) that cover 32 core meta-tasks and 162 subtasks. Agency GAIA: 450 questions with unambiguous answers that require information that can be obtained by browsing the Internet, requiring different levels of tooling and autonomy to solve. Divided into 3 difficulty levels. WebArena: 241 mock-up websites based on real-world websites (Reddit, GitLab, Magento's admin portal, etc), and 812 tasks to be performed on the websites. The tasks include information-seeking, site navigation, and content and configuration operation. Mind2Web: 2,350 tasks collected from 137 websites, and crowdsourced action sequences. The task is to reproduce the action sequence. OSWorld: 369 multimodal computer-using tasks, involving multiple real web and desktop apps and OS file I/O. In both Windows and Ubuntu. Each task includes an initial state setup configuration, and is tested by an execution-based evaluation script. Windows Agent Arena: 154 multimodal tasks with the same format as OSWorld. Only in Windows. WebVoyager: 643 multimodal tasks based on 15 popular websites. Evaluation is by screenshotting the action sequence and asking a vision language model to judge. BFCL (Berkeley Function-Calling Leaderboard): The task is to write API calls a