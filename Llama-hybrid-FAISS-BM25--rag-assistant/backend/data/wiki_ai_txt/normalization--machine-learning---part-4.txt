idation set. Other works attempt to eliminate BatchNorm, such as the Normalizer-Free ResNet. Layer normalization Layer normalization (LayerNorm) is a popular alternative to BatchNorm. Unlike BatchNorm, which normalizes activations across the batch dimension for a given feature, LayerNorm normalizes across all the features within a single data sample. Compared to BatchNorm, LayerNorm's performance is not affected by batch size. It is a key component of transformer models. For a given data input and layer, LayerNorm computes the mean μ {\displaystyle \mu } and variance σ 2 {\displaystyle \sigma ^{2}} over all the neurons in the layer. Similar to BatchNorm, learnable parameters γ {\displaystyle \gamma } (scale) and β {\displaystyle \beta } (shift) are applied. It is defined by: x i ^ = x i − μ σ 2 + ϵ , y i = γ i x i ^ + β i {\displaystyle {\hat {x_{i}}}={\frac {x_{i}-\mu }{\sqrt {\sigma ^{2}+\epsilon }}},\quad y_{i}=\gamma _{i}{\hat {x_{i}}}+\beta _{i}} where: μ = 1 D ∑ i = 1 D x i , σ 2 = 1 D ∑ i = 1 D ( x i − μ ) 2 {\displaystyle \mu ={\frac {1}{D}}\sum _{i=1}^{D}x_{i},\quad \sigma ^{2}={\frac {1}{D}}\sum _{i=1}^{D}(x_{i}-\mu )^{2}} and the index i {\displaystyle i} ranges over the neurons in that layer. Examples For example, in CNN, a LayerNorm applies to all activations in a layer. In the previous notation, we have: μ ( l ) = 1 H W C ∑ h = 1 H ∑ w = 1 W ∑ c = 1 C x h , w , c ( l ) ( σ ( l ) ) 2 = 1 H W C ∑ h = 1 H ∑ w = 1 W ∑ c = 1 C ( x h , w , c ( l ) − μ ( l ) ) 2 x ^ h , w , c ( l ) = x ^ h , w , c ( l ) − μ ( l ) ( σ ( l ) ) 2 + ϵ y h , w , c ( l ) = γ ( l ) x ^ h , w , c ( l ) + β ( l ) {\displaystyle {\begin{aligned}\mu ^{(l)}&={\frac {1}{HWC}}\sum _{h=1}^{H}\sum _{w=1}^{W}\sum _{c=1}^{C}x_{h,w,c}^{(l)}\\(\sigma ^{(l)})^{2}&={\frac {1}{HWC}}\sum _{h=1}^{H}\sum _{w=1}^{W}\sum _{c=1}^{C}(x_{h,w,c}^{(l)}-\mu ^{(l)})^{2}\\{\hat {x}}_{h,w,c}^{(l)}&={\frac {{\hat {x}}_{h,w,c}^{(l)}-\mu ^{(l)}}{\sqrt {(\sigma ^{(l)})^{2}+\epsilon }}}\\y_{h,w,c}^{(l)}&=\gamma ^{(l)}{\hat {x}}_{h,w,c}^{(l)}+\beta ^{(l)}\end{aligned}}} Notice that the batch index b {\displaystyle b} is removed, while the channel index c {\displaystyle c} is added. In recurrent neural networks and transformers, LayerNorm is applied individually to each timestep. For example, if the hidden vector in an RNN at timestep t {\displaystyle t} is x ( t ) ∈ R D {\displaystyle x^{(t)}\in \mathbb {R} ^{D}} , where D {\displaystyle D} is the dimension of the hidden vector, then LayerNorm will be applied with: x i ^ ( t ) = x i ( t ) − μ ( t ) ( σ ( t ) ) 2 + ϵ , y i ( t ) = γ i x i ^ ( t ) + β i {\displaystyle {\hat {x_{i}}}^{(t)}={\frac {x_{i}^{(t)}-\mu ^{(t)}}{\sqrt {(\sigma ^{(t)})^{2}+\epsilon }}},\quad y_{i}^{(t)}=\gamma _{i}{\hat {x_{i}}}^{(t)}+\beta _{i}} where: μ ( t ) = 1 D ∑ i = 1 D x i ( t ) , ( σ ( t ) ) 2 = 1 D ∑ i = 1 D ( x i ( t ) − μ ( t ) ) 2 {\displaystyle \mu ^{(t)}={\frac {1}{D}}\sum _{i=1}^{D}x_{i}^{(t)},\quad (\sigma ^{(t)})^{2}={\frac {1}{D}}\sum _{i=1}^{D}(x_{i}^{(t)}-\mu ^{(t)})^{2}} Root mean square layer normalization Root mean square layer normalization (RMSNorm): x i ^ = x i 1 D ∑ i = 1 D x i 2 , y i = γ x i ^ + β {\displaystyle {\hat {x_{i}}}={\frac {x_{i}}{\sqrt {{\frac {1}{D}}\sum _{i=1}^{D}x_{i}^{2}}}},\quad y_{i}=\gamma {\hat {x_{i}}}+\beta } Essentially, it is LayerNorm where we enforce μ , ϵ = 0 {\displaystyle \mu ,\epsilon =0} . It is also called L2 normalization. It is a special case of Lp normalization, or power normalization: x i ^ = x i ( 1 D ∑ i = 1 D | x i | p ) 1 / p , y i = γ x i ^ + β {\displaystyle {\hat {x_{i}}}={\frac {x_{i}}{\left({\frac {1}{D}}\sum _{i=1}^{D}|x_{i}|^{p}\right)^{1/p}}},\quad y_{i}=\gamma {\hat {x_{i}}}+\beta } where p > 0 {\displaystyle p>0} is a constant. Adaptive Adaptive layer norm (adaLN) computes the γ , β {\displaystyle \gamma ,\beta } in a LayerNorm not from the layer activation itself, but from other data. It was first proposed for CNNs, and has been used effectively in diffusion transformers (DiTs)