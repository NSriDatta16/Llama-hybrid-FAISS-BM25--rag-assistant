_{i})} is equal to log ⁡ p i − 1 ( z i − 1 ) {\displaystyle \log p_{i-1}(z_{i-1})} subtracted by a non-recursive term, we can infer by induction that: log ⁡ p K ( z K ) = log ⁡ p 0 ( z 0 ) − ∑ i = 1 K log ⁡ | det d f i ( z i − 1 ) d z i − 1 | {\displaystyle \log p_{K}(z_{K})=\log p_{0}(z_{0})-\sum _{i=1}^{K}\log \left|\det {\frac {df_{i}(z_{i-1})}{dz_{i-1}}}\right|} Training method As is generally done when training a deep learning model, the goal with normalizing flows is to minimize the Kullback–Leibler divergence between the model's likelihood and the target distribution to be estimated. Denoting p θ {\displaystyle p_{\theta }} the model's likelihood and p ∗ {\displaystyle p^{*}} the target distribution to learn, the (forward) KL-divergence is: D KL [ p ∗ ( x ) ‖ p θ ( x ) ] = − E p ∗ ( x ) ⁡ [ log ⁡ p θ ( x ) ] + E p ∗ ( x ) ⁡ [ log ⁡ p ∗ ( x ) ] {\displaystyle D_{\text{KL}}[p^{*}(x)\|p_{\theta }(x)]=-\mathop {\mathbb {E} } _{p^{*}(x)}[\log p_{\theta }(x)]+\mathop {\mathbb {E} } _{p^{*}(x)}[\log p^{*}(x)]} The second term on the right-hand side of the equation corresponds to the entropy of the target distribution and is independent of the parameter θ {\displaystyle \theta } we want the model to learn, which only leaves the expectation of the negative log-likelihood to minimize under the target distribution. This intractable term can be approximated with a Monte-Carlo method by importance sampling. Indeed, if we have a dataset { x i } i = 1 N {\displaystyle \{x_{i}\}_{i=1}^{N}} of samples each independently drawn from the target distribution p ∗ ( x ) {\displaystyle p^{*}(x)} , then this term can be estimated as: − E ^ p ∗ ( x ) [ log ⁡ p θ ( x ) ] = − 1 N ∑ i = 0 N log ⁡ p θ ( x i ) {\displaystyle -{\hat {\mathop {\mathbb {E} } }}_{p^{*}(x)}[\log p_{\theta }(x)]=-{\frac {1}{N}}\sum _{i=0}^{N}\log p_{\theta }(x_{i})} Therefore, the learning objective a r g m i n θ D KL [ p ∗ ( x ) ‖ p θ ( x ) ] {\displaystyle {\underset {\theta }{\operatorname {arg\,min} }}\ D_{\text{KL}}[p^{*}(x)\|p_{\theta }(x)]} is replaced by a r g m a x θ ∑ i = 0 N log ⁡ p θ ( x i ) {\displaystyle {\underset {\theta }{\operatorname {arg\,max} }}\ \sum _{i=0}^{N}\log p_{\theta }(x_{i})} In other words, minimizing the Kullback–Leibler divergence between the model's likelihood and the target distribution is equivalent to maximizing the model likelihood under observed samples of the target distribution. A pseudocode for training normalizing flows is as follows: INPUT. dataset x 1 : n {\displaystyle x_{1:n}} , normalizing flow model f θ ( ⋅ ) , p 0 {\displaystyle f_{\theta }(\cdot ),p_{0}} . SOLVE. max θ ∑ j ln ⁡ p θ ( x j ) {\displaystyle \max _{\theta }\sum _{j}\ln p_{\theta }(x_{j})} by gradient descent RETURN. θ ^ {\displaystyle {\hat {\theta }}} Variants Planar Flow The earliest example. Fix some activation function h {\displaystyle h} , and let θ = ( u , w , b ) {\displaystyle \theta =(u,w,b)} with the appropriate dimensions, then x = f θ ( z ) = z + u h ( ⟨ w , z ⟩ + b ) {\displaystyle x=f_{\theta }(z)=z+uh(\langle w,z\rangle +b)} The inverse f θ − 1 {\displaystyle f_{\theta }^{-1}} has no closed-form solution in general. The Jacobian is | det ( I + h ′ ( ⟨ w , z ⟩ + b ) u w T ) | = | 1 + h ′ ( ⟨ w , z ⟩ + b ) ⟨ u , w ⟩ | {\displaystyle |\det(I+h'(\langle w,z\rangle +b)uw^{T})|=|1+h'(\langle w,z\rangle +b)\langle u,w\rangle |} . For it to be invertible everywhere, it must be nonzero everywhere. For example, h = tanh {\displaystyle h=\tanh } and ⟨ u , w ⟩ > − 1 {\displaystyle \langle u,w\rangle >-1} satisfies the requirement. Nonlinear Independent Components Estimation (NICE) Let x , z ∈ R 2 n {\displaystyle x,z\in \mathbb {R} ^{2n}} be even-dimensional, and split them in the middle. Then the normalizing flow functions are x = [ x 1 x 2 ] = f θ ( z ) = [ z 1 z 2 ] + [ 0 m θ ( z 1 ) ] {\displaystyle x={\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}}=f_{\theta }(z)={\begin{bmatrix}z_{1}\\z_{2}\end{bmatrix}}+{\begin{bmatrix}0\\m_{\theta }(z_{1})\end{bmatrix}}}