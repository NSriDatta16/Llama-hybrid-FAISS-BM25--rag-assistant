ment A {\displaystyle A} being true about possible worlds is then, P ( A ) = | { x : A ( x ) } | | x : ⊤ | {\displaystyle P(A)={\frac {|\{x:A(x)\}|}{|x:\top |}}} For a conditional probability. P ( B | A ) = | { x : A ( x ) ∧ B ( x ) } | | x : A ( x ) | {\displaystyle P(B|A)={\frac {|\{x:A(x)\land B(x)\}|}{|x:A(x)|}}} then P ( A ∧ B ) = | { x : A ( x ) ∧ B ( x ) } | | x : ⊤ | = | { x : A ( x ) ∧ B ( x ) } | | { x : A ( x ) } | | { x : A ( x ) } | | x : ⊤ | = P ( A ) P ( B | A ) {\displaystyle {\begin{aligned}P(A\land B)&={\frac {|\{x:A(x)\land B(x)\}|}{|x:\top |}}\\[8pt]&={\frac {|\{x:A(x)\land B(x)\}|}{|\{x:A(x)\}|}}{\frac {|\{x:A(x)\}|}{|x:\top |}}\\[8pt]&=P(A)P(B|A)\end{aligned}}} Using symmetry this equation may be written out as Bayes' law. P ( A ∧ B ) = P ( A ) P ( B | A ) = P ( B ) P ( A | B ) {\displaystyle P(A\land B)=P(A)P(B|A)=P(B)P(A|B)} This law describes the relationship between prior and posterior probabilities when new facts are learnt. Written as quantities of information Bayes' Theorem becomes, L ( A ∧ B ) = L ( A ) + L ( B | A ) = L ( B ) + L ( A | B ) {\displaystyle L(A\land B)=L(A)+L(B|A)=L(B)+L(A|B)} Two statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is, P ( B ) = P ( B | A ) {\displaystyle P(B)=P(B|A)} then Bayes' Theorem reduces to, P ( A ∧ B ) = P ( A ) P ( B ) {\displaystyle P(A\land B)=P(A)P(B)} The law of total of probability For a set of mutually exclusive possibilities A i {\displaystyle A_{i}} , the sum of the posterior probabilities must be 1. ∑ i P ( A i | B ) = 1 {\displaystyle \sum _{i}{P(A_{i}|B)}=1} Substituting using Bayes' theorem gives the law of total probability ∑ i P ( B | A i ) P ( A i ) = ∑ i P ( A i | B ) P ( B ) {\displaystyle \sum _{i}{P(B|A_{i})P(A_{i})}=\sum _{i}{P(A_{i}|B)P(B)}} P ( B ) = ∑ i P ( B | A i ) P ( A i ) {\displaystyle P(B)=\sum _{i}{P(B|A_{i})P(A_{i})}} This result is used to give the extended form of Bayes' theorem, P ( A i | B ) = P ( B | A i ) P ( A i ) ∑ j P ( B | A j ) P ( A j ) {\displaystyle P(A_{i}|B)={\frac {P(B|A_{i})P(A_{i})}{\sum _{j}{P(B|A_{j})P(A_{j})}}}} This is the usual form of Bayes' theorem used in practice, because it guarantees the sum of all the posterior probabilities for A i {\displaystyle A_{i}} is 1. Alternate possibilities For mutually exclusive possibilities, the probabilities add. P ( A ∨ B ) = P ( A ) + P ( B ) , if P ( A ∧ B ) = 0 {\displaystyle P(A\lor B)=P(A)+P(B),\qquad {\text{if }}P(A\land B)=0} Using A ∨ B = ( A ∧ ¬ ( A ∧ B ) ) ∨ ( B ∧ ¬ ( A ∧ B ) ) ∨ ( A ∧ B ) {\displaystyle A\lor B=(A\land \neg (A\land B))\lor (B\land \neg (A\land B))\lor (A\land B)} Then the alternatives A ∧ ¬ ( A ∧ B ) , B ∧ ¬ ( A ∧ B ) , A ∧ B {\displaystyle A\land \neg (A\land B),\quad B\land \neg (A\land B),\quad A\land B} are all mutually exclusive. Also, ( A ∧ ¬ ( A ∧ B ) ) ∨ ( A ∧ B ) = A {\displaystyle (A\land \neg (A\land B))\lor (A\land B)=A} P ( A ∧ ¬ ( A ∧ B ) ) + P ( A ∧ B ) = P ( A ) {\displaystyle P(A\land \neg (A\land B))+P(A\land B)=P(A)} P ( A ∧ ¬ ( A ∧ B ) ) = P ( A ) − P ( A ∧ B ) {\displaystyle P(A\land \neg (A\land B))=P(A)-P(A\land B)} so, putting it all together, P ( A ∨ B ) = P ( ( A ∧ ¬ ( A ∧ B ) ) ∨ ( B ∧ ¬ ( A ∧ B ) ) ∨ ( A ∧ B ) ) = P ( A ∧ ¬ ( A ∧ B ) + P ( B ∧ ¬ ( A ∧ B ) ) + P ( A ∧ B ) = P ( A ) − P ( A ∧ B ) + P ( B ) − P ( A ∧ B ) + P ( A ∧ B ) = P ( A ) + P ( B ) − P ( A ∧ B ) {\displaystyle {\begin{aligned}P(A\lor B)&=P((A\land \neg (A\land B))\lor (B\land \neg (A\land B))\lor (A\land B))\\&=P(A\land \neg (A\land B)+P(B\land \neg (A\land B))+P(A\land B)\\&=P(A)-P(A\land B)+P(B)-P(A\land B)+P(A\land B)\\&=P(A)+P(B)-P(A\land B)\end{aligned}}} Negation As, A ∨ ¬ A = ⊤ {\displaystyle A\lor \neg A=\top } then P ( A ) + P ( ¬ A ) = 1 {\displaystyle P(A)+P(\neg A)=1} Implication and condition probability Implication is related to conditional probability by the following equation, A → B ⟺ P ( B | A ) = 1 {\displaystyle A\to B\iff P(B|A)=1} Derivation