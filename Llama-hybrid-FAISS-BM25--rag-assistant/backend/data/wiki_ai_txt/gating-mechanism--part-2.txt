+ b ) ⊙ ( x V + c ) ReGLU ⁡ ( x , W , V , b , c ) = max ( 0 , x W + b ) ⊙ ( x V + c ) GEGLU ⁡ ( x , W , V , b , c ) = GELU ⁡ ( x W + b ) ⊙ ( x V + c ) SwiGLU ⁡ ( x , W , V , b , c , β ) = Swish β ⁡ ( x W + b ) ⊙ ( x V + c ) {\displaystyle {\begin{aligned}\operatorname {GLU} (x,W,V,b,c)&=\sigma (xW+b)\odot (xV+c)\\\operatorname {Bilinear} (x,W,V,b,c)&=(xW+b)\odot (xV+c)\\\operatorname {ReGLU} (x,W,V,b,c)&=\max(0,xW+b)\odot (xV+c)\\\operatorname {GEGLU} (x,W,V,b,c)&=\operatorname {GELU} (xW+b)\odot (xV+c)\\\operatorname {SwiGLU} (x,W,V,b,c,\beta )&=\operatorname {Swish} _{\beta }(xW+b)\odot (xV+c)\end{aligned}}} Other architectures Gating mechanism is used in highway networks, which were designed by unrolling an LSTM. Channel gating uses a gate to control the flow of information through different channels inside a convolutional neural network (CNN). See also Recurrent neural network Long short-term memory Gated recurrent unit Transformer Activation function References Further reading Zhang, Aston; Lipton, Zachary; Li, Mu; Smola, Alexander J. (2024). "10.1. Long Short-Term Memory (LSTM)". Dive into deep learning. Cambridge New York Port Melbourne New Delhi Singapore: Cambridge University Press. ISBN 978-1-009-38943-3.