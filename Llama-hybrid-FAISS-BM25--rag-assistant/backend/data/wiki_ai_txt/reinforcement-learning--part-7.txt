itions bug detection in software projects continuous learning combinations with logic-based frameworks (e.g., temporal-logic specifications, reward machines, and probabilistic argumentation). exploration in large Markov decision processes entity-based reinforcement learning human feedback interaction between implicit and explicit learning in skill acquisition intrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations large (or continuous) action spaces modular and hierarchical reinforcement learning multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding. occupant-centric control optimization of computing resources partial information (e.g., using predictive state representation) reward function based on maximising novel information sample-based planning (e.g., based on Monte Carlo tree search). securities trading transfer learning TD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error. value-function and policy search methods Comparison of key algorithms The following table lists the key algorithms for learning a policy depending on several criteria: The algorithm can be on-policy (it performs policy updates using trajectories sampled via the current policy) or off-policy. The action space may be discrete (e.g. the action space could be "going up", "going left", "going right", "going down", "stay") or continuous (e.g. moving the arm with a given angle). The state space may be discrete (e.g. the agent could be in a cell in a grid) or continuous (e.g. the agent could be located at a given position in the plane). Associative reinforcement learning Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment. Deep reinforcement learning This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning. Adversarial deep reinforcement learning Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies. Fuzzy reinforcement learning By introducing fuzzy inference in reinforcement learning, approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values). Inverse reinforcement learning In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal. One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject