to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure. The formulation of the union of groups approach is also referred to as latent group Lasso, and requires to modify the group ℓ 2 {\displaystyle \ell _{2}} norm considered above and introduce the following regularizer R ( w ) = i n f { ∑ g ‖ w g ‖ g : w = ∑ g = 1 G w ¯ g } {\displaystyle R(w)=inf\left\{\sum _{g}\|w_{g}\|_{g}:w=\sum _{g=1}^{G}{\bar {w}}_{g}\right\}} where w ∈ R d {\displaystyle w\in {\mathbb {R^{d}} }} , w g ∈ G g {\displaystyle w_{g}\in G_{g}} is the vector of coefficients of group g, and w ¯ g ∈ R d {\displaystyle {\bar {w}}_{g}\in {\mathbb {R^{d}} }} is a vector with coefficients w g j {\displaystyle w_{g}^{j}} for all variables j {\displaystyle j} in group g {\displaystyle g} , and 0 {\displaystyle 0} in all others, i.e., w ¯ g j = w g j {\displaystyle {\bar {w}}_{g}^{j}=w_{g}^{j}} if j {\displaystyle j} in group g {\displaystyle g} and w ¯ g j = 0 {\displaystyle {\bar {w}}_{g}^{j}=0} otherwise. This regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring w = ∑ g = 1 G w ¯ g {\displaystyle w=\sum _{g=1}^{G}{\bar {w}}_{g}} produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to. Issues with Group Lasso regularization and alternative approaches The objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group ℓ 1 {\displaystyle \ell _{1}} regularization term. An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions. An example of a way to fix this is to introduce the squared ℓ 2 {\displaystyle \ell _{2}} norm of the weight vector as an additional regularization term while keeping the ℓ 1 {\displaystyle \ell _{1}} regularization term from the group lasso approach. If the coefficient of the squared ℓ 2 {\displaystyle \ell _{2}} norm term is greater than 0 {\displaystyle 0} , then because the squared ℓ 2 {\displaystyle \ell _{2}} norm term is strongly convex, the resulting objective function will also be strongly convex. Provided that the ℓ 2 {\displaystyle \ell _{2}} coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group ℓ 2 {\displaystyle \ell _{2}} regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach. Thus this approach allows for simpler optimization while maintaining sparsity. Norms based on the structure over Input variables See: Submodular set function Besides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a directed acyclic graph over the variables while in the context of grid-based norms, the structure can be represented using a grid. Hierarchical Norms See: Unsupervised learning Unsupervised learning methods are often used to learn the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, "hierarchies" are assumed between the variables of the system; this