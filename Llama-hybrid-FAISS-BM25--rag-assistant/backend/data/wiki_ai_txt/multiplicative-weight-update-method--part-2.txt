 will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most log2(N) mistakes. Weighted majority algorithm Source: Unlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same "expert advice" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts. The very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm. Initialization: Fix an η ≤ 1 / 2 {\displaystyle \eta \leq 1/2} . For each expert, associate the weight w i 1 {\displaystyle {w_{i}}^{1}} ≔1. For t {\displaystyle t} = 1 {\displaystyle {\mathit {1}}} , 2 {\displaystyle {\mathit {2}}} ,..., T {\displaystyle T} 1. Make the prediction given by the weighted majority of the experts' predictions based on their weights w 1 t , . . . , w n t {\displaystyle \mathbb {w_{1}} ^{t},...,\mathbb {w_{n}} ^{t}} . That is, choose 0 or 1 depending on which prediction has a higher total weight of experts advising it (breaking ties arbitrarily). 2. For every expert i that predicted wrongly, decrease his weight for the next round by multiplying it by a factor of (1-η): w i t + 1 {\displaystyle w_{i}^{t+1}} = ( 1 − η ) w i t {\displaystyle (1-\eta )w_{i}^{t}} (update rule) If η = 0 {\displaystyle \eta =0} , the weight of the expert's advice will remain the same. When η {\displaystyle \eta } increases, the weight of the expert's advice will decrease. Note that some researchers fix η = 1 / 2 {\displaystyle \eta =1/2} in weighted majority algorithm. After T {\displaystyle T} steps, let m i T {\displaystyle m_{i}^{T}} be the number of mistakes of expert i and M T {\displaystyle M^{T}} be the number of mistakes our algorithm has made. Then we have the following bound for every i {\displaystyle i} : M T ≤ 2 ( 1 + η ) m i T + 2 ln ⁡ ( n ) η {\displaystyle M^{T}\leq 2(1+\eta )m_{i}^{T}+{\frac {2\ln(n)}{\eta }}} . In particular, this holds for i which is the best expert. Since the best expert will have the least m i T {\displaystyle m_{i}^{T}} , it will give the best bound on the number of mistakes made by the algorithm as a whole. Randomized weighted majority algorithm This algorithm can be understood as follows: Given the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction: predict f ( x ) = { 1 with probability q 1 W 0 otherwise {\displaystyle f(x)={\begin{cases}1&{\text{with probability}}{\frac {q_{1}}{W}}\\0&{\text{otherwise}}\end{cases}}} where W = ∑ i w i = q 0 + q 1 {\displaystyle W=\sum _{i}{w_{i}}=q_{0}+q_{1}} . The number of mistakes made by the randomized weighted majority algorithm is bounded as: E [ # mistakes of the learner ] ≤ α β ( # mistakes of the best expert ) + c β ln ⁡ ( N ) {\displaystyle E\left[\#{\text{mistakes of the learner}}\right]\leq \alpha _{\beta }\left(\#{\text{ mistakes of the best expert}}\right)+c_{\beta }\l