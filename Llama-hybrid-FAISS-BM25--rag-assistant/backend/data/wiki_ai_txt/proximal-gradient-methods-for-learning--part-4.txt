 ∈ R . {\displaystyle x_{i}\in \mathbb {R} ^{d}{\text{ and }}y_{i}\in \mathbb {R} .} For 0 < μ ≤ 1 {\displaystyle 0<\mu \leq 1} the penalty term λ ( ( 1 − μ ) ‖ w ‖ 1 + μ ‖ w ‖ 2 2 ) {\displaystyle \lambda \left((1-\mu )\|w\|_{1}+\mu \|w\|_{2}^{2}\right)} is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small μ > 0 {\displaystyle \mu >0} , the additional penalty term μ ‖ w ‖ 2 2 {\displaystyle \mu \|w\|_{2}^{2}} acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions. Exploiting group structure Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory. Certain problems in learning can often involve data which has additional structure that is known a priori. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods. Group lasso Group lasso is a generalization of the lasso method when features are grouped into disjoint blocks. Suppose the features are grouped into blocks { w 1 , … , w G } {\displaystyle \{w_{1},\ldots ,w_{G}\}} . Here we take as a regularization penalty R ( w ) = ∑ g = 1 G ‖ w g ‖ 2 , {\displaystyle R(w)=\sum _{g=1}^{G}\|w_{g}\|_{2},} which is the sum of the ℓ 2 {\displaystyle \ell _{2}} norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group w g {\displaystyle w_{g}} we have that proximity operator of λ γ ( ∑ g = 1 G ‖ w g ‖ 2 ) {\displaystyle \lambda \gamma \left(\sum _{g=1}^{G}\|w_{g}\|_{2}\right)} is given by S ~ λ γ ( w g ) = { w g − λ γ w g ‖ w g ‖ 2 , ‖ w g ‖ 2 > λ γ 0 , ‖ w g ‖ 2 ≤ λ γ {\displaystyle {\widetilde {S}}_{\lambda \gamma }(w_{g})={\begin{cases}w_{g}-\lambda \gamma {\frac {w_{g}}{\|w_{g}\|_{2}}},&\|w_{g}\|_{2}>\lambda \gamma \\0,&\|w_{g}\|_{2}\leq \lambda \gamma \end{cases}}} where w g {\displaystyle w_{g}} is the g {\displaystyle g} th group. In contrast to lasso, the derivation of the proximity operator for group lasso relies on the Moreau decomposition. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm. Other group structures In contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure. Such generalizations of group lasso have been considered in a variety of contexts. For overlapping groups one common approach is known as latent group lasso which introduces latent variables to account for overlap. Nested group structures are studied in hierarchical structure prediction and with directed acyclic graphs. See also Convex analysis Proximal gradient method Regularization Statistical learning theory == References ==