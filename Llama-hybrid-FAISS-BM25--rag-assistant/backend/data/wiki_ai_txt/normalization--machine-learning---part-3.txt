{\hat {x}}_{(b),h,w,c}^{(l)}&={\frac {x_{(b),h,w,c}^{(l)}-\mu _{c}^{(l)}}{\sqrt {(\sigma _{c}^{(l)})^{2}+\epsilon }}}\\y_{(b),h,w,c}^{(l)}&=\gamma _{c}{\hat {x}}_{(b),h,w,c}^{(l)}+\beta _{c}\end{aligned}}} Similar considerations apply for BatchNorm for n-dimensional convolutions. The following is a Python implementation of BatchNorm for 2D convolutions: For multilayered recurrent neural networks (RNN), BatchNorm is usually applied only for the input-to-hidden part, not the hidden-to-hidden part. Let the hidden state of the l {\displaystyle l} -th layer at time t {\displaystyle t} be h t ( l ) {\displaystyle h_{t}^{(l)}} . The standard RNN, without normalization, satisfies h t ( l ) = ϕ ( W ( l ) h t l − 1 + U ( l ) h t − 1 l + b ( l ) ) {\displaystyle h_{t}^{(l)}=\phi (W^{(l)}h_{t}^{l-1}+U^{(l)}h_{t-1}^{l}+b^{(l)})} where W ( l ) , U ( l ) , b ( l ) {\displaystyle W^{(l)},U^{(l)},b^{(l)}} are weights and biases, and ϕ {\displaystyle \phi } is the activation function. Applying BatchNorm, this becomes h t ( l ) = ϕ ( B N ( W ( l ) h t l − 1 ) + U ( l ) h t − 1 l ) {\displaystyle h_{t}^{(l)}=\phi (\mathrm {BN} (W^{(l)}h_{t}^{l-1})+U^{(l)}h_{t-1}^{l})} There are two possible ways to define what a "batch" is in BatchNorm for RNNs: frame-wise and sequence-wise. Concretely, consider applying an RNN to process a batch of sentences. Let h b , t ( l ) {\displaystyle h_{b,t}^{(l)}} be the hidden state of the l {\displaystyle l} -th layer for the t {\displaystyle t} -th token of the b {\displaystyle b} -th input sentence. Then frame-wise BatchNorm means normalizing over b {\displaystyle b} : μ t ( l ) = 1 B ∑ b = 1 B h i , t ( l ) ( σ t ( l ) ) 2 = 1 B ∑ b = 1 B ( h t ( l ) − μ t ( l ) ) 2 {\displaystyle {\begin{aligned}\mu _{t}^{(l)}&={\frac {1}{B}}\sum _{b=1}^{B}h_{i,t}^{(l)}\\(\sigma _{t}^{(l)})^{2}&={\frac {1}{B}}\sum _{b=1}^{B}(h_{t}^{(l)}-\mu _{t}^{(l)})^{2}\end{aligned}}} and sequence-wise means normalizing over ( b , t ) {\displaystyle (b,t)} : μ ( l ) = 1 B T ∑ b = 1 B ∑ t = 1 T h i , t ( l ) ( σ ( l ) ) 2 = 1 B T ∑ b = 1 B ∑ t = 1 T ( h t ( l ) − μ ( l ) ) 2 {\displaystyle {\begin{aligned}\mu ^{(l)}&={\frac {1}{BT}}\sum _{b=1}^{B}\sum _{t=1}^{T}h_{i,t}^{(l)}\\(\sigma ^{(l)})^{2}&={\frac {1}{BT}}\sum _{b=1}^{B}\sum _{t=1}^{T}(h_{t}^{(l)}-\mu ^{(l)})^{2}\end{aligned}}} Frame-wise BatchNorm is suited for causal tasks such as next-character prediction, where future frames are unavailable, forcing normalization per frame. Sequence-wise BatchNorm is suited for tasks such as speech recognition, where the entire sequences are available, but with variable lengths. In a batch, the smaller sequences are padded with zeroes to match the size of the longest sequence of the batch. In such setups, frame-wise is not recommended, because the number of unpadded frames decreases along the time axis, leading to increasingly poorer statistics estimates. It is also possible to apply BatchNorm to LSTMs. Improvements BatchNorm has been very popular and there were many attempted improvements. Some examples include: ghost batching: randomly partition a batch into sub-batches and perform BatchNorm separately on each; weight decay on γ {\displaystyle \gamma } and β {\displaystyle \beta } ; and combining BatchNorm with GroupNorm. A particular problem with BatchNorm is that during training, the mean and variance are calculated on the fly for each batch (usually as an exponential moving average), but during inference, the mean and variance were frozen from those calculated during training. This train-test disparity degrades performance. The disparity can be decreased by simulating the moving average during inference: μ = α E [ x ] + ( 1 − α ) μ x , train σ 2 = ( α E [ x ] 2 + ( 1 − α ) μ x 2 , train ) − μ 2 {\displaystyle {\begin{aligned}\mu &=\alpha E[x]+(1-\alpha )\mu _{x,{\text{ train}}}\\\sigma ^{2}&=(\alpha E[x]^{2}+(1-\alpha )\mu _{x^{2},{\text{ train}}})-\mu ^{2}\end{aligned}}} where α {\displaystyle \alpha } is a hyperparameter to be optimized on a val