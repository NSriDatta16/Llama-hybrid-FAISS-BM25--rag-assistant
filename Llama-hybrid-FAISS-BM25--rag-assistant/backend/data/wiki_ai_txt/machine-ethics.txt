Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence (AI), otherwise known as AI agents. Machine ethics differs from other ethical fields related to engineering and technology. It should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with technology's grander social effects. Definitions James H. Moor, one of the pioneering theoreticians in the field of computer ethics, defines four kinds of ethical robots. As an extensive researcher on the studies of philosophy of artificial intelligence, philosophy of mind, philosophy of science, and logic, Moor defines machines as ethical impact agents, implicit ethical agents, explicit ethical agents, or full ethical agents. A machine can be more than one type of agent. Ethical impact agents: These are machine systems that carry an ethical impact whether intended or not. At the same time, they have the potential to act unethically. Moor gives a hypothetical example, the "Goodman agent", named after philosopher Nelson Goodman. The Goodman agent compares dates but has the millennium bug. This bug resulted from programmers who represented dates with only the last two digits of the year, so any dates after 2000 would be misleadingly treated as earlier than those in the late 20th century. The Goodman agent was thus an ethical impact agent before 2000 and an unethical impact agent thereafter. Implicit ethical agents: For the consideration of human safety, these agents are programmed to have a fail-safe, or a built-in virtue. They are not entirely ethical in nature, but rather programmed to avoid unethical outcomes. Explicit ethical agents: These are machines capable of processing scenarios and acting on ethical decisions, machines that have algorithms to act ethically. Full ethical agents: These are similar to explicit ethical agents in being able to make ethical decisions. But they also have human metaphysical features (i.e., have free will, consciousness, and intentionality). (See artificial systems and moral responsibility.) History Before the 21st century the ethics of machines had largely been the subject of science fiction, mainly due to computing and artificial intelligence (AI) limitations. Although the definition of "machine ethics" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI magazine article "A Question of Responsibility":One thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. Thus, as computers and robots become more and more intelligent, it becomes imperative that we think carefully and explicitly about what those built-in values are. Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimov's three laws of robotics. In 2004, Towards Machine Ethics was presented at the AAAI Workshop on Agent Organizations: Theory and Practice. Theoretical foundations for machine ethics were laid out. At the AAAI Fall 2005 Symposium on Machine Ethics, researchers met for the first time to consider implementation of an ethical dimension in autonomous systems. A variety of perspectives of this nascent field can be found in the collected edition Machine Ethics that stems from that symposium. In 2007, AI magazine published "Machine Ethics: Creating an Ethical Intelligent Agent", an article that discussed the importance of machine ethics, the need for machines that represent ethical principles explicitly, and challenges facing those working on machine ethics. It also demonstrated that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of ethical ju