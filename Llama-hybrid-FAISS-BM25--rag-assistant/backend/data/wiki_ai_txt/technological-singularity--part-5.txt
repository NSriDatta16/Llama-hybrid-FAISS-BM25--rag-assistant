humans to create the improved hardware, or to program factories appropriately. An AI rewriting its own source code could do so while contained in an AI box. Second, as with Vernor Vinge's conception of the singularity, it is much harder to predict the outcome. While speed increases seem to be only a quantitative difference from human intelligence, actual algorithm improvements would be qualitatively different. Substantial dangers are associated with an intelligence explosion singularity originating from a recursively self-improving set of algorithms. First, the goal structure of the AI might self-modify, potentially causing the AI to optimise for something other than what was originally intended. Second, AIs could compete for the resources humankind uses to survive. While not actively malicious, AIs would promote the goals of their programming, not necessarily broader human goals, and thus might crowd out humans. Carl Shulman and Anders Sandberg suggest that algorithm improvements may be the limiting factor for a singularity; while hardware efficiency tends to improve at a steady pace, software innovations are more unpredictable and may be bottlenecked by serial, cumulative research. They suggest that in the case of a software-limited singularity, intelligence explosion would actually become more likely than with a hardware-limited singularity, because in the software-limited case, once human-level AI is developed, it could run serially on very fast hardware, and the abundance of cheap hardware would make AI research less constrained. An abundance of accumulated hardware that can be unleashed once the software figures out how to use it has been called "computing overhang". Criticism Linguist and cognitive scientist Steven Pinker wrote in 2008: "There is not the slightest reason to believe in a coming singularity. The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible. Look at domed cities, jet-pack commuting, underwater cities, mile-high buildings, and nuclear-powered automobiles—all staples of futuristic fantasies when I was a child that have never arrived. Sheer processing power is not a pixie dust that magically solves all your problems." Jaron Lanier denies that the singularity is inevitable: "I do not think the technology is creating itself. It's not an autonomous process [...] The reason to believe in human agency over technological determinism is that you can then have an economy where people earn their own way and invent their own lives. If you structure a society on not emphasizing individual human agency, it's the same thing operationally as denying people clout, dignity, and self-determination ... to embrace [the idea of the Singularity] would be a celebration of bad data and bad politics." Philosopher and cognitive scientist Daniel Dennett said in 2017: "The whole singularity stuff, that's preposterous. It distracts us from much more pressing problems [...] AI tools that we become hyper-dependent on—that is going to happen. And one of the dangers is that we will give them more authority than they warrant." Some critics suggest religious motivations for believing in the singularity, especially Kurzweil's version. The buildup to the singularity is compared to Christian end-times scenarios. Beam called it "a Buck Rogers vision of the hypothetical Christian Rapture". John Gray has said, "the Singularity echoes apocalyptic myths in which history is about to be interrupted by a world-transforming event". In The New York Times, David Streitfeld questioned whether "it might manifest first and foremost—thanks, in part, to the bottom-line obsession of today’s Silicon Valley—as a tool to slash corporate America’s head count." Astrophysicist and scientific philosopher Adam Becker criticizes Kurzweil's concept of human mind uploads to computers on the grounds that they are too fundamentally different and incompatible. Skepticism of exponential growth Theodore Modis holds 