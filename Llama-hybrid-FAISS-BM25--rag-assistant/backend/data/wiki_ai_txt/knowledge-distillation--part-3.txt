 , which is the derivative of 1 2 ( z i − z ^ i ) 2 {\displaystyle {\frac {1}{2}}\left(z_{i}-{\hat {z}}_{i}\right)^{2}} , i.e. the loss is equivalent to matching the logits of the two models, as done in model compression. "Optimal Brain Damage" algorithm The Optimal Brain Damage (OBD) algorithm is as follows: Do until a desired level of sparsity or performance is reached: Train the network (by methods such as backpropagation) until a reasonable solution is obtained Compute the saliencies for each parameter Delete some lowest-saliency parameters Deleting a parameter means fixing the parameter to zero. The "saliency" of a parameter θ {\displaystyle \theta } is defined as 1 2 ( ∂ θ 2 L ) θ 2 {\displaystyle {\frac {1}{2}}(\partial _{\theta }^{2}L)\theta ^{2}} , where L {\displaystyle L} is the loss function. The second-derivative ∂ θ 2 L {\displaystyle \partial _{\theta }^{2}L} can be computed by second-order backpropagation. The idea for optimal brain damage is to approximate the loss function in a neighborhood of optimal parameter θ ∗ {\displaystyle \theta ^{*}} by Taylor expansion: L ( θ ) ≈ L ( θ ∗ ) + 1 2 ∑ i ( ∂ θ i 2 L ( θ ∗ ) ) ( θ i − θ i ∗ ) 2 {\displaystyle L(\theta )\approx L(\theta ^{*})+{\frac {1}{2}}\sum _{i}(\partial _{\theta _{i}}^{2}L(\theta ^{*}))(\theta _{i}-\theta _{i}^{*})^{2}} where ∇ L ( θ ∗ ) ≈ 0 {\displaystyle \nabla L(\theta ^{*})\approx 0} , since θ ∗ {\displaystyle \theta ^{*}} is optimal, and the cross-derivatives ∂ θ i ∂ θ j L {\displaystyle \partial _{\theta _{i}}\partial _{\theta _{j}}L} are neglected to save compute. Thus, the saliency of a parameter approximates the increase in loss if that parameter is deleted. History A related methodology was model compression or pruning, where a trained network is reduced in size. This was first done in 1965 by Alexey Ivakhnenko and Valentin Lapa in USSR (1965). Their deep networks were trained layer by layer through regression analysis. Superfluous hidden units were pruned using a separate validation set. Other neural network compression methods include Biased Weight Decay and Optimal Brain Damage. An early example of neural network distillation was published by Jürgen Schmidhuber in 1991, in the field of recurrent neural networks (RNNs). The problem was sequence prediction for long sequences, i.e., deep learning. Their approach was to use two RNNs. One of them (the automatizer) predicted the sequence, and another (the chunker) predicted the errors of the automatizer. Simultaneously, the automatizer predicted the internal states of the chunker. After the automatizer manages to predict the chunker's internal states well, it would start fixing the errors, and soon the chunker is obsoleted, leaving just one RNN in the end. The idea of using the output of one neural network to train another neural network was also studied as the teacher-student network configuration. In 1992, several papers studied the statistical mechanics of teacher-student configurations with committee machines or parity machines. Compressing the knowledge of multiple models into a single neural network was called model compression in 2006: compression was achieved by training a smaller model on large amounts of pseudo-data labelled by a higher-performing ensemble, optimizing to match the logit of the compressed model to the logit of the ensemble. The knowledge distillation preprint of Geoffrey Hinton et al. (2015) formulated the concept and showed some results achieved in the task of image classification. Knowledge distillation is also related to the concept of behavioral cloning discussed by Faraz Torabi et. al. References External links Distilling the knowledge in a neural network – Google AI