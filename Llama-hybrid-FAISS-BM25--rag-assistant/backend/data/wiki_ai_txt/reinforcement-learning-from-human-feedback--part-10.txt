, π ref ) = E x , y ∼ D [ a x , y v ( r θ ( x , y ) − E y ′ ∼ Q [ r θ ( x , y ′ ) ] ⏟ reference point ) ] + C D {\displaystyle f(\pi _{\theta },\pi _{\text{ref}})=\mathbb {E} _{x,y\sim D}[a_{x,y}v{\Bigl (}r_{\theta }(x,y)\;-\;\underbrace {E_{y'\sim Q}[\,r_{\theta }(x,y')\,]} _{\text{reference point}}{\Bigr )}]+C_{D}} where D {\displaystyle D} is the preference data, C D {\displaystyle C_{D}} is some constant relevant to the dataset, and Q {\displaystyle Q} is some distribution representing the baseline or “reference”. Each training example is attached a label a x , y ∈ { + 1 , − 1 } {\displaystyle a_{x,y}\in \{+1,-1\}} that tells us if the example is desirable (we want to push up its reward) and -1 if it’s undesirable (in order to push down its reward). Unlike previous definitions of the reward, KTO defines r θ ( x , y ) {\displaystyle r_{\theta }(x,y)} as the “implied reward” taken by the log-likelihood ratio between the policy model and the reference model log ⁡ ( π θ ( y | x ) π ref ( y | x ) ) {\displaystyle \log \left({\frac {\pi _{\theta }(y|x)}{\pi _{\text{ref}}(y|x)}}\right)} . Here, the value function v {\displaystyle v} is a non-linear (typically concave) function that mimics human loss aversion and risk aversion. As opposed to previous preference optimization algorithms, the motivation of KTO lies in maximizing the utility of model outputs from a human perspective rather than maximizing the likelihood of a “better” label (chosen vs. rejected responses). Hence, it constructs a more relaxed generalization to preference distributions by requiring only a binary feedback signal a x , y {\displaystyle a_{x,y}} instead of explicit preference pairs. For each example ( x , y ) {\displaystyle (x,y)} in the dataset D {\displaystyle D} , KTO explicitly optimizes the HALO objective as: π θ ∗ = arg ⁡ max π θ E ( x , y ) ∼ D [ γ y − v ( x , y ) ] {\displaystyle \pi _{\theta }^{*}\;=\;\arg \max _{\pi _{\theta }}\;\;\mathbb {E} _{(x,y)\,\sim \,D}{\Bigl [}\gamma _{y}\;-\;v(x,y){\Bigr ]}} , where γ y {\displaystyle \gamma _{y}} is a class-specific constant (e.g., γ y = λ D or λ U {\displaystyle \gamma _{y}=\lambda _{D}{\text{ or }}\lambda _{U}} ) controlling how strongly the model should push up good outputs vs. push down bad ones. The value function v ( x , y ) {\displaystyle v(x,y)} is defined piecewise depending on whether y {\displaystyle y} is desirable ( λ D {\displaystyle \lambda _{D}} ) or undesirable ( λ U {\displaystyle \lambda _{U}} ): v ( x , y ) = { λ D σ ( β ( r θ ( x , y ) − z 0 ) ) , if y ∼ y d e s i r a b l e ∣ x , λ U σ ( β ( z 0 − r θ ( x , y ) ) ) , if y ∼ y u n d e s i r a b l e ∣ x {\displaystyle v(x,y)\;=\;{\begin{cases}\lambda _{D}\,\sigma \!{\bigl (}\,\beta \,{\bigl (}r_{\theta }(x,y)\;-\;z_{0}{\bigr )}{\bigr )},&\quad {\text{if }}y\sim y_{\mathrm {desirable} \mid x},\\[6pt]\lambda _{U}\,\sigma \!{\bigl (}\,\beta \,{\bigl (}z_{0}\;-\;r_{\theta }(x,y){\bigr )}{\bigr )},&\quad {\text{if }}y\sim y_{\mathrm {undesirable} \mid x}\end{cases}}} and z 0 = K L ( π θ ( y ′ ∣ x ) ‖ π r e f ( y ′ ∣ x ) ) {\textstyle z_{0}=\mathrm {KL} \!{\Bigl (}\,\pi _{\theta }(y'\mid x)\;{\big \Vert }\;\pi _{\mathrm {ref} }(y'\mid x){\Bigr )}} is a baseline given by the Kullback–Leibler divergence. Here, β {\displaystyle \beta } controls how “risk-averse” the value function is (larger β {\displaystyle \beta } = faster saturation in the logistic function σ {\displaystyle \sigma } ). Intuitively, desirable outputs push the model to increase r θ {\displaystyle r_{\theta }} so that r θ − z 0 {\displaystyle r_{\theta }-z_{0}} becomes more positive. Undesirable ones push it in the opposite direction, so the reward is less than the reference. Since many real-world feedback pipelines yield "like/dislike" data more easily than pairwise comparisons, KTO is designed to be data-cheap and to reflect "loss aversion" more directly by using a straightforward notion of "good vs. bad" at the example level. See also Human-in-the-loop Reward-based selectio