The Mark I Perceptron was a pioneering supervised image classification learning system developed by Frank Rosenblatt in 1958. It was the first implementation of an Artificial Intelligence (AI) machine. It differs from the Perceptron which is a software architecture proposed in 1943 by Warren McCulloch and Walter Pitts, which was also employed in Mark I, and enhancements of which have continued to be an integral part of cutting edge AI technologies like the Transformer. Architecture The Mark I Perceptron was organized into three layers: A set of sensory units which receive optical input A set of association units, each of which fire based on input from multiple sensory units A set of response units, which fire based on input from multiple association units The connection between sensory units and association units were random. The working of association units was very similar to the response units. Different versions of the Mark I used different numbers of units in each of the layers. Capabilities In his 1957 proposal for funding for development of the "Cornell Photoperceptron", Rosenblatt claimed:"Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic."With the first version of the Mark I Perceptron as early as 1958, Rosenblatt demonstrated a simple binary classification experiment, namely distinguishing between sheets of paper marked on the right versus those marked on the left side. One of the later experiments distinguished a square from a circle printed on paper. The shapes were perfect and their sizes fixed; the only variation was in their position and orientation. The Mark I Perceptron achieved 99.8% accuracy on a test dataset with 500 neurons in a single layer. The size of the training dataset was 10,000 example images. It took 3 seconds for the training pipeline to go through a single image. Higher accuracy was observed with thick outline figures compared to solid figures, likely because outline figures reduced overfitting. Another experiment distinguished between a square and a diamond for which 100% accuracy was achieved with only 60 training images, with a Perceptron having 1,000 neurons in a single layer. The time taken to process each training input for this larger perceptron was 15 seconds. The only variation was in position of the image, since rotation would have been ambiguous. In that same experiment, it could distinguish between the letters X and E with 100% accuracy when trained with only 20 images (10 images of each letter). Variations in the images included both position and rotation by up to 30 degrees. When variation in rotation was increased to any angle (both in training and test datasets), the accuracy reduced to 90% with 60 training images (30 images of each letter). For distinguishing between the letters E and F, a more challenging problem due to their similarity, the same 1,000 neuron perceptron achieved an accuracy of more than 80% with 60 training images. Variation was only in the position of the image, with no rotation. See also History of artificial intelligence History of artificial neural networks == References ==