icy to kill tens of thousands of people in their homes. Limits of human review The Guardian cited Moyes as saying a commander who's handed a computer-generated list of targets may not know how the list was generated or be able to question the targeting recommendations, and is in danger of losing the ability to meaningfully consider the risk of civilian harm. In an opinion piece in Le Monde, reporter Élise Vincent wrote that automated weapons are divided into fully automated systems, which aren't really on the market, and lethal autonomous weapons, which in principle allow human control, and that this division allows Israel to claim the Gospel falls on the side of the more appropriate use of force. She cited Laure de Roucy-Rochegonde, a researcher at Institut français des relations internationales, as saying the war could obsolete these blurred categories and invigorate a stricter regulatory definition, significant human control, which human rights activists including Article 36 have been trying to advocate. She quoted de Roucy-Rochegonde as saying it's not known what kind of algorithm the Israeli army uses, or how the data has been aggregated, which wouldn't be a problem if they didn't lead to a life-or-death decision. Diligence Dr. Marta Bo, researcher at the Stockholm International Peace Research Institute, noted that the humans in human-in-the-loop risk "automation bias": overreliance on systems, giving those systems too much influence over decisions that need to be made by humans. Suchman observed that the huge volume of targets is likely putting pressure on the human reviewers, saying that "in the face of this kind of acceleration, those reviews become more and more constrained in terms of what kind of judgment people can actually exercise." Tal Mimran, lecturer at Hebrew University in Jerusalem who's previously worked with the government on targeting, added that pressure will make analysts more likely to accept the AI's targeting recommendations, whether they are correct, and they may be tempted to make life easier for themselves by going along with the machine's recommendations, which could create a "whole new level of problems" if the machine is systematically misidentifying targets. Accountability Khlaaf noted the difficulty of pursuing accountability when AIs are involved. Humans retain the culpability, but who's responsible if the targeting system fails, and it's impossible to trace the failure to any one mistake by one person? The NPR article went on: "Is it the analyst who accepted the AI recommendation? The programmers who made the system? The intelligence officers who gathered the training data?" Reactions United Nations Secretary-General, Antonio Guterres, said he was "deeply troubled" by reports that Israel used artificial intelligence in its military campaign in Gaza, saying the practice puts civilians at risk and blurs accountability. Speaking about the Lavender system, Marc Owen Jones, a professor at Hamad Bin Khalifa University stated, "Let's be clear: This is an AI-assisted genocide, and going forward, there needs to be a call for a moratorium on the use of AI in the war". Ben Saul, a United Nations special rapporteur, stated that if reports about Israel's use of AI were true, then "many Israeli strikes in Gaza would constitute the war crimes of launching disproportionate attacks". Ramesh Srinivasan, a professor at UCLA, stated, "Corporate America Big Tech is actually aligned with many of the Israeli military's actions. The fact that AI systems are being used indicates there's a lack of regard by the Israeli state. Everybody knows these AI systems will make mistakes." Microsoft was criticized by activists and its own employees for providing Microsoft Azure computing services to Unit 8200 and other Israeli government organizations. Microsoft opened an inquiry into IDF use of its services after The Guardian reported the use of phone call data collected from mass surveillance in Gaza and the West Bank to ide