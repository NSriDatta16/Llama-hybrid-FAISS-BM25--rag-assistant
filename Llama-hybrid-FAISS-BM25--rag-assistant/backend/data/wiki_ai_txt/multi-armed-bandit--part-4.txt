rd under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett, Ortner Filippi, Cappé, and Garivier, and Honda and Takemura. For Bernoulli multi-armed bandits, Pilarski et al. studied computation methods of deriving fully optimal solutions (not just asymptotically) using dynamic programming in the paper "Optimal Policy for Bernoulli Bandits: Computation and Algorithm Gauge." Via indexing schemes, lookup tables, and other techniques, this work provided practically applicable optimal solutions for Bernoulli bandits provided that time horizons and numbers of arms did not become excessively large. Pilarski et al. later extended this work in "Delayed Reward Bernoulli Bandits: Optimal Policy and Predictive Meta-Algorithm PARDI" to create a method of determining the optimal policy for Bernoulli bandits when rewards may not be immediately revealed following a decision and may be delayed. This method relies upon calculating expected values of reward outcomes which have not yet been revealed and updating posterior probabilities when rewards are revealed. When optimal solutions to multi-arm bandit tasks are used to derive the value of animals' choices, the activity of neurons in the amygdala and ventral striatum encodes the values derived from these policies, and can be used to decode when the animals make exploratory versus exploitative choices. Moreover, optimal policies better predict animals' choice behavior than alternative strategies (described below). This suggests that the optimal solutions to multi-arm bandit problems are biologically plausible, despite being computationally demanding. Approximate solutions Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below. Semi-uniform strategies Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a greedy behavior where the best lever (based on previous observations) is always pulled except when a (uniformly) random action is taken. Epsilon-greedy strategy: The best lever is selected for a proportion 1 − ϵ {\displaystyle 1-\epsilon } of the trials, and a lever is selected at random (with uniform probability) for a proportion ϵ {\displaystyle \epsilon } . A typical parameter value might be ϵ = 0.1 {\displaystyle \epsilon =0.1} , but this can vary widely depending on circumstances and predilections. Epsilon-first strategy: A pure exploration phase is followed by a pure exploitation phase. For N {\displaystyle N} trials in total, the exploration phase occupies ϵ N {\displaystyle \epsilon N} trials and the exploitation phase ( 1 − ϵ ) N {\displaystyle (1-\epsilon )N} trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected. Epsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of ϵ {\displaystyle \epsilon } decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish. Adaptive epsilon-greedy strategy based on value differences (VDBE): Similar to the epsilon-decreasing strategy, except that epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010). High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation); low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a softmax-weighted action selection in case o