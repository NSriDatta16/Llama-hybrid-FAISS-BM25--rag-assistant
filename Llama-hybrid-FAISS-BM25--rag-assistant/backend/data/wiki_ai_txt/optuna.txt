Optuna is an open-source Python library for automatic hyperparameter tuning of machine learning models. It was first introduced in 2018 by Preferred Networks, a Japanese startup that works on practical applications of deep learning in various fields. The beta version of Optuna was released at the end of the year, with the subsequent first major stable release announced in January 2020. Hyperparameter optimization Hyperparameter optimization involves finding the optimal value of non-trainable parameters, defined by the user. Examples of hyperparameters are learning rate, number of layers or neurons, regularization strength and tree depth. However, they strongly depend on the specific algorithm (e.g., classification, regression, clustering, etc.). Hyperparameter optimization can be especially relevant when dealing with large-scale problems or limited resources, as it improves model accuracy, reduces overfitting, and decreases training time. However, when the hyperparameter space increases, it may become computationally expensive. Hence, there are methods (e.g., grid search, random search, or bayesian optimization) that considerably simplify this process. Features Optuna is designed to optimize the model hyperparameters, by searching large spaces and discarding combinations that show no significant improvements in the model. Moreover, it can parallelize the hyperparameter search over multiple threads or processes. Optuna works with a high degree of modularity, allowing the definition of different configurations for searching hyperparameters. It is possible to choose which hyperparameters to optimize and how to optimize them. Additionally, it permits parameter customization at runtime, meaning that the values to explore for the hyperparameters (i.e., the search space) can be defined while writing the code, rather than being defined in advance. Sampling Optuna exploits well-established algorithms to perform hyperparameter optimization, progressively reducing the search space, in light of objective values. Examples are gaussian-process-based algorithms (i.e., a gaussian process to model the objective function), tree-structured parzen estimator (TPE) (i.e., a model-based optimization method that estimates the objective function and selects the best hyperparameters), and random search (i.e., a basic optimization approach used for benchmarking). Early stopping Optuna includes a pruning feature to stop trials early, when the results show no significant improvement in the model. This allows for the prevention of unnecessary computation, and it is used for models with long training times, in order to save time and computational resources. Specifically, Optuna exploits techniques such as median and threshold pruning. Scalability Optuna is designed to scale with distributed computing resources, supporting parallel execution. This feature allows users to run optimization trials across multiple processors or machines. Integration with third-part libraries Optuna integrates with various ML libraries and frameworks: Catboost Dask Fast.ai Keras LightGBM MLflow PyTorch PyTorch Ignite PyTorch Lightning TensorBoard TensorFlow tf.keras Weights & Biases XGBoost Moreover, Optuna offers a real-time dashboard that allows to monitor, through graphs and tables, the optimization history and the hyperparameter importance. Applications Optuna was designed to be framework agnostic, so that it can be used with any machine-learning (ML) or deep-learning (DL) framework. Machine learning Optuna can be used to optimize hyperparamenters of ML models. Examples are: Random forest: number of trees, maximum depth, and minimum samples per leaf. Gradient boosting machines (GBM): learning rate, number of estimators, and maximum depth. Support vector machines (SVM): regularization parameter (C), kernel type (e.g., linear, radial basis function), and gamma (gamma). K-nearest neighbors (KNN): number of neighbors (k), distance metrics (e.g., Euclidean or Manhattan), and weigh