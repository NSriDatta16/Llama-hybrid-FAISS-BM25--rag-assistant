Artificial intelligence and moral enhancement involves the application of artificial intelligence to the enhancement of moral reasoning and the acceleration of moral progress. Artificial moral reasoning With respect to moral reasoning, some consider humans to be suboptimal information processors, moral judges, and moral agents. Due to stress or time constraints, people often fail to consider all the relevant factors and information necessary to make well-reasoned moral judgments, people lack consistency, and they are prone to biases. Ideal observer theory The classical ideal observer theory is a metaethical theory about the meaning of moral statements. It holds that a moral statement is any statement to which an "ideal observer" would react or respond in a certain way. An ideal observer is defined as being: (1) omniscient with respect to non-ethical facts, (2) omnipercipient, (3) disinterested, (4) dispassionate, (5) consistent, and (6) normal in all other respects. Adam Smith and David Hume espoused versions of the ideal observer theory and Roderick Firth provided a more sophisticated and modern version. An analogous idea in law is the reasonable person criterion. Artificial moral advisors can be compared and contrasted with ideal observers. Ideal observers have to be omniscient and omnipercipient about non-ethical facts, while artificial moral advisors would just need to know those morally relevant facts which pertain to a decision. Exhaustive versus auxiliary enhancement Exhaustive enhancement involves scenarios where human moral decision-making is supplanted, left entirely to machines. Some proponents consider machines as being morally superior to humans and that just doing as the machines say would constitute moral improvement. Opponents of exhaustive enhancement list five main concerns: (1) the existence of pluralism may complicate finding consensuses on which to build, configure, train, or inform systems, (2) even if such consensuses could be achieved, people might still fail to construct good systems due to human or nonhuman limitations, (3) resultant systems might not be able to make autonomous moral decisions, (4) moral progress might be hindered, (5) it would mean the death of morality. Dependence on artificial intelligence systems to perform moral reasoning would not only neglect the cultivation of moral excellence but actively undermine it, exposing people to risks of disengagement, of atrophy of human faculties, and of moral manipulation at the hands of the systems or their creators. Auxiliary enhancement addresses these concerns and involves scenarios where machines augment or supplement human decision-making. Artificial intelligence assistants would be tools to help people to clarify and keep track of their moral commitments and contexts while providing accompanying explanations, arguments, and justifications for conclusions. The ultimate decision-making, however, would rest with the human users. Some proponents of auxiliary enhancement also support educational technologies with respect to morality, technologies which teach moral reasoning, e.g., assistants which utilize the Socratic method. It may be the case that a “right” or “best” answer to a moral question is a “best” dialogue which provides value for users. See also AI alignment Artificial intelligence Automated decision-making Decision support system Intelligent tutoring system Legal informatics Machine ethics Moral reasoning Multi-agent systems Project Debater Superintelligence == References ==