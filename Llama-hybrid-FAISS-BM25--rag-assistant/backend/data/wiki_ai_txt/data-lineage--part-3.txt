ta is unstructured data. The fundamental challenge of unstructured data sources is that they are difficult for non-technical business users and data analysts alike to unbox, understand and prepare for analytic use. Beyond issues of structure, the sheer volume of this type of data contributes to such difficulty. Because of this, current data mining techniques often leave out valuable information and make analyzing unstructured data laborious and expensive. In today's competitive business environment, companies have to find and analyze the relevant data they need quickly. The challenge is going through the volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and parallel processing to crunch large volumes of data quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even with this level of sophisticated hardware and software, a few of the image processing tasks in large scale take a few days to few weeks. Debugging of the data processing is extremely hard due to long run times. A third approach of advanced data discovery solutions combines self-service data prep with visual data discovery, enabling analysts to simultaneously prepare and visualize data side-by-side in an interactive analysis environment offered by newer companies, such as Trifacta, Alteryx and others. Another method to track data lineage is spreadsheet programs such as Excel that offer users cell-level lineage, or the ability to see which cells are dependent on another. However, the structure of the transformation is lost. Similarly, ETL or mapping software provide transform-level lineage, yet this view typically doesn't display data and is too coarse-grained to distinguish between transforms that are logically independent (e.g. transforms that operate on distinct columns) or dependent. Big Data platforms have a very complicated structure, where data is distributed across a vast range. Typically, the jobs are mapped into several machines and results are later combined by the reduce operations. Debugging a Big Data pipeline becomes very challenging due to the very nature of the system. It will not be an easy task for the data scientist to figure out which machine's data has outliers and unknown features causing a particular algorithm to give unexpected results. Proposed solution Data provenance or data lineage can be used to make the debugging of Big Data pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail. Data provenance In information systems, data provenance is information about the entities, activities, and agents involved in producing a piece of data; it records how data was derived and can be used to assess quality, reliability, and trustworthiness. Classical database research distinguishes why, where, and how provenance and shows how these forms support tasks such as query debugging, view maintenance, confidence estimation, and annotation propagation. In scientific workflows, provenance documents the derivation history from original sources through workflow steps, supporting reproducibility and reuse of results. In industry usage, data lineage is closely related: lineage typically denotes the end-to-end flow of datasets and transformations across systems (from sources through processing to outputs), while provenance emphasises derivations and attribution of specific data items; the two are complementary. Open, implementation-oriented specifications such as OpenLineage model lineage in terms of jobs, runs, and datasets to enable automated capture from modern data pipelines. Uses. Provenance/lineage information underpins impact analysis and debugging of da