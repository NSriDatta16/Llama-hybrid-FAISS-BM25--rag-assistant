3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes. The essence of an image is to project a 3D scene onto a 2D plane, during which process, the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views, which convey that the corresponding sets of points must contain some structure, and that this structure is related to the poses and the calibration of the camera. In recent decades, there has been a significant demand for 3D content in application to computer graphics, virtual reality and communication, which also demanded a change in the required tools and devices in creating 3D. Most existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs), resulting in a high cost. This gap stimulates the use of digital imaging facilities (like cameras). An early method was proposed by Tomasi and Kanade, in which they used an affine factorization approach to extract 3D from image sequences. However, the assumption of orthographic projection is a significant limitation of this system. Processing The task of converting multiple 2D images into a 3D model consists of a series of processing steps: Camera calibration consists of intrinsic and extrinsic parameters, without which, at some level, no arrangement of algorithms will work. The dotted line between Calibration and Depth determination shows that the camera calibration is usually required for determining depth. Depth determination serves as the most challenging part in the whole process, as it calculates the 3D component missing from any given image – depth. The key issue, in this process, is the correspondence problem, which is finding the matches between two images so the position of the matched elements can then be triangulated in 3D space. Once you have the multiple depth maps, you have to combine them to create a final mesh by calculating depth and projecting out of the camera – registration. Camera calibration will be used to identify where the many meshes created by depth maps can be combined to develop a larger one, providing more than one view for observation. By the Material Application stage, you already have a complete 3D mesh, which may be the final goal, but usually, you will want to apply the color from the original photographs to the mesh. This can range from projecting the images onto the mesh randomly, through approaches of combining the textures for super resolution and finally to segmenting the mesh by material, such as specular and diffuse properties. Mathematical description of reconstruction Given a group of 3D points viewed by N cameras with matrices { P i } i = 1 … N {\displaystyle \{P^{i}\}_{i=1\ldots N}} , define m j i ≃ P i w j {\displaystyle m_{j}^{i}\simeq P^{i}w_{j}} to be the homogeneous coordinates of the projection of the j t h {\displaystyle j^{th}} point onto the i t h {\displaystyle i^{th}} camera. The reconstruction problem can be changed to: given the group of pixel coordinates { m j i } {\displaystyle \{m_{j}^{i}\}} , find the corresponding set of camera matrices { P i } {\displaystyle \{P^{i}\}} and the scene structure { w j } {\displaystyle \{w_{j}\}} such that m j i ≃ P i w j {\displaystyle m_{j}^{i}\simeq P^{i}w_{j}} (1) Generally, without further restrictions, we will obtain a projective reconstruction. If { P i } {\displaystyle \{P^{i}\}} and { w j } {\displaystyle \{w_{j}\}} satisfy (1), { P i T } {\displaystyle \{P^{i}T\}} and { T − 1 w j } {\displaystyle \{T^{-1}w_{j}\}} will satisfy (1) with any 4