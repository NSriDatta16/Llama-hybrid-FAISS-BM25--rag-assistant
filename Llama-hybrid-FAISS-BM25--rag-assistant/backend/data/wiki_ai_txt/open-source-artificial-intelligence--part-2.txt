 influenced the development of open-source AI, as more developers began to see the potential benefits of open collaboration in software creation, including AI models and algorithms. In the 1990s, open-source software began to gain more traction, the rise of machine learning and statistical methods also led to the development of more practical AI tools. In 1993, the CMU Artificial Intelligence Repository was initiated, with a variety of openly shared software. 2000s: Emergence of open-source AI In the early 2000s open-source AI began to take off, with the release of more user-friendly foundational libraries and frameworks that were available for anyone to use and contribute to. OpenCV was released in 2000 with a variety of traditional AI algorithms like decision trees, k-Nearest Neighbors (kNN), Naive Bayes and Support Vector Machines (SVM). In 2007, Scikit-learn was released. It became one of the most widely used libraries for general-purpose machine learning due to its ease of use and robust functionality, providing implementations of common algorithms like regression, classification, and clustering. Theano was also released in the same year. Rise of open-source AI frameworks (2010s) Open-source deep learning framework as Torch was released in 2002 and made open-source with Torch7 in 2011, and was later augmented by PyTorch, and TensorFlow. These frameworks allowed researchers and developers to build and train neural networks for tasks like image recognition, natural language processing (NLP), and autonomous driving. AlexNet was released in 2012, and Word2vec for natural language processing by Google in 2013. In 2014, GloVe, a competitor to Word2vec, was released source code under an Apache 2.0 license, documented the datasets they trained on, and released the model weights under a Public Domain Dedication and License. Open-source generative AI (2020sâ€“Present) With the announcement of GPT-2, OpenAI originally planned to keep the source code of their models private citing concerns about malicious applications. After OpenAI faced public backlash, however, it released the source code for GPT-2 to GitHub three months after its release. OpenAI has not publicly released the source code or pretrained weights for the GPT-3 or GPT-4 models, though their functionalities can be integrated by developers through the OpenAI API. The rise of large language models (LLMs) and generative AI, such as OpenAI's GPT-3 (2020), further propelled the demand for open-source AI frameworks. These models have been used in a variety of applications, including chatbots, content creation, and code generation, demonstrating the broad capabilities of AI systems. At the time of GPT-3's release GPT-2 was still the most powerful open source language model in the world, spurring EleutherAI to train and release GPT-Neo and GPT-J in 2021. In February 2022 EleutherAI released GPT-NeoX-20B, taking back the title of most powerful open source language model in the world from Meta whose FairSeq Dense 13B model had surpassed GPT-J at the end of 2021. 2022 also saw the rise of larger and more powerful models under various non-open source licenses including Meta's OPT and Galactica, the BigScience Research Workshop's BLOOM, and Tsinghua University's GLM. During early negotiations in 2021 and 2022 around AI legislation in Europe, proposals were made to avoid over-regulating open-source AI. Noting that some organizations were mis-applying the "open-source" label to their work, in 2022, the Open Source Initiative, which originally came up with the widely accepted standard for open-source software in 1998, started working with experts on a definition of "open-source" that would fit the needs of AI software and models. The most controversial aspect relates to data access, since some models are trained on sensitive data which can't be released. In 2024, they finalized the Open Source AI Definition 1.0 (OSAID 1.0), with endorsements from over 20 organizations. It requires full re