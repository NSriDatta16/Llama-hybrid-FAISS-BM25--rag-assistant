 names were viewed as more "pleasant" than African-Americans names, and that the words "woman" and "girl" were more likely to be associated with the arts instead of science and math, "which were most likely connected to males." Generative AI models typically work based on user-entered word-based prompts, especially in the case of diffusion models, and this word-related bias may lead to biased results. Along with this, generative AI can perpetuate harmful stereotypes regarding women. For example, Lensa, an AI app that trended on TikTok in 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women. Melissa Heikkil√§, a senior reporter at MIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner. Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery. In 2024, Google's chatbot Gemini's AI image generator was criticized for racial bias, with claims that Gemini deliberately underrepresented white people in its results. Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as "happy white people" and "ideal nuclear family". Google later apologized for "missing the mark" and took Gemini's image generator offline for updates. This prompted discussions about the ethical implications of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts. In addition to the well-documented representational issues such as racial and gender bias, some scholars have also pointed out deeper conceptual assumptions that shape how we perceive AI-generated art. For instance, framing AI strictly as a passive tool overlooks how cultural and technological factors influence its outputs. Others suggest viewing AI as part of a collaborative creative process, where both human and machine contribute to the artistic result. Copyright Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of using gathered data to produce new artwork. In 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program. A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define "author" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the "work for hire" doctrine, under which ownership of a copyright is transferred to the "employer." In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship. In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation. In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that "there are no safeguards for artists to be able to identify works in databases that are being used and opt out." Some have c