in deployment and scope. The Leeds Abdominal Pain System went operational in 1971 for the University of Leeds hospital. It was reported to have produced a correct diagnosis in 91.8% of cases, compared to the clinicians' success rate of 79.6%. Despite the wide range of efforts by institutions to produce and use these systems, widespread adoption and acceptance have still not yet been achieved for most offerings. One large roadblock to acceptance has historically been workflow integration. A tendency to focus only on the functional decision-making core of the CDSS existed, causing a deficiency in planning how the clinician will use the product in situ. CDSSs were stand-alone applications, requiring the clinician to cease working on their current system, switch to the CDSS, input the necessary data (even if it had already been inputted into another system), and examine the results produced. The additional steps break the flow from the clinician's perspective and cost precious time. Technical challenges and barriers to implementation Clinical decision support systems face steep technical challenges in several areas. Biological systems are profoundly complicated, and a clinical decision may utilise an enormous range of potentially relevant data. For example, an electronic evidence-based medicine system may potentially consider a patient's symptoms, medical history, family history and genetics, as well as historical and geographical trends of disease occurrence, and published clinical data on therapeutic effectiveness when recommending a patient's course of treatment. Clinically, a large deterrent to CDSS acceptance is workflow integration. While it has been shown that clinicians require explanations of Machine Learning-Based CDSS, in order to able to understand and trust their suggestions, there is an overall distinct lack of application of explainable Artificial Intelligence in the context of CDSS, thus adding another barrier to the adoption of these systems. Another source of contention with many medical support systems is that they produce a massive number of alerts. When systems produce a high volume of warnings (especially those that do not require escalation), besides the annoyance, clinicians may pay less attention to warnings, causing potentially critical alerts to be missed. This phenomenon is called alert fatigue. Maintenance One of the core challenges facing CDSS is difficulty in incorporating the extensive quantity of clinical research being published on an ongoing basis. In a given year, tens of thousands of clinical trials are published. Currently, each one of these studies must be manually read, evaluated for scientific legitimacy, and incorporated into the CDSS in an accurate way. In 2004, it was stated that the process of gathering clinical data and medical knowledge and putting them into a form that computers can manipulate to assist in clinical decision-support is "still in its infancy". Nevertheless, it is more feasible for a business to do this centrally, even if incompletely, than for each doctor to try to keep up with all the research being published. In addition to being laborious, integration of new data can sometimes be difficult to quantify or incorporate into the existing decision support schema, particularly in instances where different clinical papers may appear conflicting. Properly resolving these sorts of discrepancies is often the subject of clinical papers itself (see meta-analysis), which often take months to complete. Evaluation In order for a CDSS to offer value, it must demonstrably improve clinical workflow or outcome. Evaluation of CDSS quantifies its value to improve a system's quality and measure its effectiveness. Because different CDSSs serve different purposes, no generic metric applies to all such systems; however, attributes such as consistency (with and with experts) often apply across a wide spectrum of systems. The evaluation benchmark for a CDSS depends on the system's goal: for e