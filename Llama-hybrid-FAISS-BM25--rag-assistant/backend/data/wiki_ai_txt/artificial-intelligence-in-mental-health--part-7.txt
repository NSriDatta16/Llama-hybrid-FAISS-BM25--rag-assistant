 models and employing health professionals is important. Lack of emotional understanding: Unlike human therapists, AI systems do not possess lived experiences or emotional awareness that make them limited. These limitations have prompted debate about the role of AI in addressing emotionally complex mental health needs. Some experts argue that AI cannot substitute for human-centered therapy, particularly in cases requiring deep emotional engagement. Risk of psychosis: ChatGPT usage has driven some users to experience delusions. The realism of the interaction can leave a user believing that a real person is chatting with them, fueling cognitive dissonance. Some ChatGPT conversations endorsed conspiracies and mystical beliefs, and in some cases lead to suicide. Delusions and psychosis induced by AI usage has been referred to as chatbot psychosis. Ethical issues AI in mental health is progressing with personalized care to incorporate voice, speech and biometric data. But to prevent algorithmic bias, models need to be culturally inclusive too. Ethical issues, practical uses and bias in generative models need to be addressed to promote fair and reliable mental healthcare. Although significant progress is still required, the integration of AI in mental health underscores the need for legal and regulatory frameworks to guide its development and implementation. Achieving a balance between human interaction and AI in healthcare is challenging, as there is a risk that increased automation may lead to a more mechanized approach, potentially diminishing the human touch that has traditionally characterized the field. Furthermore, granting patients a feeling of security and safety is a priority considering AI's reliance on individual data to perform and respond to inputs. Some experts caution that efforts to increase accessibility through automation may unintentionally affect aspects of the patient experience, such as trust or perceived support. To avoid veering in the wrong direction, more research should continue to develop a deeper understanding of where the incorporation of AI produces advantages and disadvantages. Data privacy and confidentiality are one of the most common security threats to medical data. Chatbots are known to be used as virtual assistants for patients but the sensitive data they collect may not be protected because the US law does not consider them as medical devices. Pharmaceutical companies use this loophole to access sensitive information and use it for their own purpose which results, in a lack of trust in chatbots and patients can hesitate in providing information essential to their treatment. Conversational Artificial Intelligence stores and remembers every conversation with a patient with complete accuracy, smartphones also collect data from search history and track app activity. If such private information is leaked it could further increase the stigma around mental health. The danger of cybercrimes and the government's unprotected access to our data, all raise serious concerns about data security. Additionally, a lack of clarity and openness with AI models can lead to a loss of trust from the patient for their medical advisors or doctors as the regular person is unaware of how they reach conclusions into giving certain medical advice. Access to such information is necessary to build trust. However, many of these models act like "black boxes", providing very little insight into how they work. AI specialists have thus highlighted ethical standards, diverse data and the correct usage of AI tools in mental healthcare. Bias and discrimination Artificial intelligence has shown promise in transforming mental health care through tools that support diagnosis, symptom tracking, and personalized interventions. However, significant concerns remain about the ways these systems may inadvertently reinforce existing disparities in care. Because AI models rely heavily on training data, they are particularly vulnerable to bias