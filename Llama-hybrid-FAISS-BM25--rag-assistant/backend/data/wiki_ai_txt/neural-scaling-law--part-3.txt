 o p t ( C ) 0.4 ∝ C 0.28 {\displaystyle D_{opt}(C)\propto N_{opt}(C)^{0.4}\propto C^{0.28}} . This exponent corresponds to the ≈ 0.5 {\displaystyle \approx 0.5} from the Chinchilla scaling paper. The scaling law of L = L 0 + ( C 0 / C ) 0.048 {\displaystyle L=L_{0}+(C_{0}/C)^{0.048}} was confirmed during the training of GPT-3 (Figure 3.1 ). Chinchilla scaling (Hoffmann, et al, 2022) One particular scaling law ("Chinchilla scaling") states that, for a large language model (LLM) autoregressively trained for one epoch, with a cosine learning rate schedule, we have: { C = C 0 N D L = A N α + B D β + L 0 {\displaystyle {\begin{cases}C=C_{0}ND\\L={\frac {A}{N^{\alpha }}}+{\frac {B}{D^{\beta }}}+L_{0}\end{cases}}} where the variables are C {\displaystyle C} is the cost of training the model, in FLOPS. N {\displaystyle N} is the number of parameters in the model. D {\displaystyle D} is the number of tokens in the training set. L {\displaystyle L} is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset. L 0 {\displaystyle L_{0}} represents the loss of an ideal generative process on the test data A N α {\displaystyle {\frac {A}{N^{\alpha }}}} captures the fact that a Transformer language model with N {\displaystyle N} parameters underperforms the ideal generative process B D β {\displaystyle {\frac {B}{D^{\beta }}}} captures the fact that the model trained on D {\displaystyle D} tokens underperforms the ideal generative process and the statistical parameters are C 0 = 6 {\displaystyle C_{0}=6} , meaning that it costs 6 FLOPs per parameter to train on one token. This is estimated by Kaplan et al. Note that training cost is much higher than inference cost, as training entails both forward and backward passes, whereas inference costs 1 to 2 FLOPs per parameter to infer on one token. α = 0.34 , β = 0.28 , A = 406.4 , B = 410.7 , L 0 = 1.69 {\displaystyle \alpha =0.34,\beta =0.28,A=406.4,B=410.7,L_{0}=1.69} . Although Besiroglu et al. claims that the statistical estimation is slightly off, and should be α = 0.35 , β = 0.37 , A = 482.01 , B = 2085.43 , L 0 = 1.82 {\displaystyle \alpha =0.35,\beta =0.37,A=482.01,B=2085.43,L_{0}=1.82} . The statistical laws were fitted over experimental data with N ∈ [ 7 × 10 7 , 1.6 × 10 10 ] , D ∈ [ 5 × 10 9 , 5 × 10 11 ] , C ∈ [ 10 18 , 10 24 ] {\displaystyle N\in [7\times 10^{7},1.6\times 10^{10}],D\in [5\times 10^{9},5\times 10^{11}],C\in [10^{18},10^{24}]} . Since there are 4 variables related by 2 equations, imposing 1 additional constraint and 1 additional optimization objective allows us to solve for all four variables. In particular, for any fixed C {\displaystyle C} , we can uniquely solve for all 4 variables that minimizes L {\displaystyle L} . This provides us with the optimal D o p t ( C ) , N o p t ( C ) {\displaystyle D_{opt}(C),N_{opt}(C)} for any fixed C {\displaystyle C} : N o p t ( C ) = G ( C 6 ) a , D o p t ( C ) = G − 1 ( C 6 ) b , where G = ( α A β B ) 1 α + β , a = β α + β , and b = α α + β . {\displaystyle N_{opt}(C)=G\left({\frac {C}{6}}\right)^{a},\quad D_{opt}(C)=G^{-1}\left({\frac {C}{6}}\right)^{b},\quad {\text{ where }}\quad G=\left({\frac {\alpha A}{\beta B}}\right)^{\frac {1}{\alpha +\beta }},\quad a={\frac {\beta }{\alpha +\beta }}{\text{, and }}b={\frac {\alpha }{\alpha +\beta }}{\text{. }}} Plugging in the numerical values, we obtain the "Chinchilla efficient" model size and training dataset size, as well as the test loss achievable: { N o p t ( C ) = 0.6 C 0.45 D o p t ( C ) = 0.3 C 0.55 L o p t ( C ) = 1070 C − 0.154 + 1.7 {\displaystyle {\begin{cases}N_{opt}(C)=0.6\;C^{0.45}\\D_{opt}(C)=0.3\;C^{0.55}\\L_{opt}(C)=1070\;C^{-0.154}+1.7\end{cases}}} Similarly, we may find the optimal training dataset size and training compute budget for any fixed model parameter size, and so on. There are other estimates for "Chinchilla efficient" model size and training dataset size. The above is based on a statistical model of L =