\sum _{i=1}^{m}\sum _{j=1}^{m}k(y_{i},y_{j})-{\frac {2}{nm}}\sum _{i=1}^{n}\sum _{j=1}^{m}k(x_{i},y_{j})\end{aligned}}} to obtain a two-sample test of the null hypothesis that both samples stem from the same distribution (i.e. P = Q {\displaystyle P=Q} ) against the broad alternative P ≠ Q {\displaystyle P\neq Q} . Density estimation via kernel embeddings Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution P X ∗ {\displaystyle P_{X}^{*}} . This can be done by solving the following optimization problem max P X H ( P X ) {\displaystyle \max _{P_{X}}H(P_{X})} subject to ‖ μ ^ X − μ X [ P X ] ‖ H ≤ ε {\displaystyle \|{\widehat {\mu }}_{X}-\mu _{X}[P_{X}]\|_{\mathcal {H}}\leq \varepsilon } where the maximization is done over the entire space of distributions on Ω . {\displaystyle \Omega .} Here, μ X [ P X ] {\displaystyle \mu _{X}[P_{X}]} is the kernel embedding of the proposed density P X {\displaystyle P_{X}} and H {\displaystyle H} is an entropy-like quantity (e.g. Entropy, KL divergence, Bregman divergence). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of M candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families. Measuring dependence of random variables A measure of the statistical dependence between random variables X {\displaystyle X} and Y {\displaystyle Y} (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert–Schmidt Independence Criterion HSIC ( X , Y ) = ‖ C X Y − μ X ⊗ μ Y ‖ H ⊗ H 2 {\displaystyle {\text{HSIC}}(X,Y)=\left\|{\mathcal {C}}_{XY}-\mu _{X}\otimes \mu _{Y}\right\|_{{\mathcal {H}}\otimes {\mathcal {H}}}^{2}} and can be used as a principled replacement for mutual information, Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent), and can be used to measure dependence between different types of data (e.g. images and text captions). Given n i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in O ( n ( d f 2 + d g 2 ) ) {\displaystyle O(n(d_{f}^{2}+d_{g}^{2}))} time, where the Gram matrices of the two datasets are approximated using A A T , B B T {\displaystyle \mathbf {A} \mathbf {A} ^{T},\mathbf {B} \mathbf {B} ^{T}} with A ∈ R n × d f , B ∈ R n × d g {\displaystyle \mathbf {A} \in \mathbb {R} ^{n\times d_{f}},\mathbf {B} \in \mathbb {R} ^{n\times d_{g}}} . The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: feature selection (BAHSIC ), clustering (CLUHSIC ), and dimensionality reduction (MUHSIC ). HSIC can be extended to measure the dependence of multiple random variables. The question of when HSIC captures independence in this case has recently been studied: for more than two variables on R d {\display