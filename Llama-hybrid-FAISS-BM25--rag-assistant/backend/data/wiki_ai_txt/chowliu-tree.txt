In probability theory and statistics Chow–Liu tree is an efficient method for constructing a second-order product approximation of a joint probability distribution, first described in a paper by Chow & Liu (1968). The goals of such a decomposition, as with such Bayesian networks in general, may be either data compression or inference. The Chow–Liu representation The Chow–Liu method describes a joint probability distribution P ( X 1 , X 2 , … , X n ) {\displaystyle P(X_{1},X_{2},\ldots ,X_{n})} as a product of second-order conditional and marginal distributions. For example, the six-dimensional distribution P ( X 1 , X 2 , X 3 , X 4 , X 5 , X 6 ) {\displaystyle P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})} might be approximated as P ′ ( X 1 , X 2 , X 3 , X 4 , X 5 , X 6 ) = P ( X 6 | X 5 ) P ( X 5 | X 2 ) P ( X 4 | X 2 ) P ( X 3 | X 2 ) P ( X 2 | X 1 ) P ( X 1 ) {\displaystyle P^{\prime }(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})} where each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure. The Chow–Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation. In general, unless there are no third-order or higher-order interactions, the Chow–Liu approximation is indeed an approximation, and cannot capture the complete structure of the original distribution. Pearl (1988) provides a modern analysis of the Chow–Liu tree as a Bayesian network. The Chow–Liu algorithm Chow and Liu show how to select second-order terms for the product approximation so that, among all such second-order approximations (first-order dependency trees), the constructed approximation P ′ {\displaystyle P^{\prime }} has the minimum Kullback–Leibler divergence to the actual distribution P {\displaystyle P} , and is thus the closest approximation in the classical information-theoretic sense. The Kullback–Leibler divergence between a second-order product approximation and the actual distribution is shown to be D ( P ∥ P ′ ) = − ∑ I ( X i ; X j ( i ) ) + ∑ H ( X i ) − H ( X 1 , X 2 , … , X n ) {\displaystyle D(P\parallel P^{\prime })=-\sum I(X_{i};X_{j(i)})+\sum H(X_{i})-H(X_{1},X_{2},\ldots ,X_{n})} where I ( X i ; X j ( i ) ) {\displaystyle I(X_{i};X_{j(i)})} is the mutual information between variable X i {\displaystyle X_{i}} and its parent X j ( i ) {\displaystyle X_{j(i)}} and H ( X 1 , X 2 , … , X n ) {\displaystyle H(X_{1},X_{2},\ldots ,X_{n})} is the joint entropy of variable set { X 1 , X 2 , … , X n } {\displaystyle \{X_{1},X_{2},\ldots ,X_{n}\}} . Since the terms ∑ H ( X i ) {\displaystyle \sum H(X_{i})} and H ( X 1 , X 2 , … , X n ) {\displaystyle H(X_{1},X_{2},\ldots ,X_{n})} are independent of the dependency ordering in the tree, only the sum of the pairwise mutual informations, ∑ I ( X i ; X j ( i ) ) {\displaystyle \sum I(X_{i};X_{j(i)})} , determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the maximum-weight tree. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions. Chow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum mutual information pair to 