), where the aforementioned equation is adjusted to remove a whole row in the vector matrix, and not only random weights P ( c ) {\displaystyle P(c)} – the probability c {\displaystyle c} to keep a row in the weight matrix w j {\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout w ^ j {\displaystyle {\hat {\mathbf {w} }}_{j}} – diluted row in the weight matrix Because dropout removes a whole row from the vector matrix, the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero, whether by setting the weights to zero, by “removing the node”, or by some other means, does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator, then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor, perhaps even an analog neuromorphic processor, then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. Google's patent Although there have been examples of randomly removing connections between neurons in a neural network to improve models, this technique was first introduced with the name dropout by Geoffrey Hinton, et al. in 2012. Google currently holds the patent for the dropout technique. See also AlexNet Convolutional neural network § Dropout Notes == References ==