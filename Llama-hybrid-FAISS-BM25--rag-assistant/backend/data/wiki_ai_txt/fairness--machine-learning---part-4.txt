l a,b\in A} P ( R = 1 | Y = 0 , A = a ) = P ( R = 1 | Y = 0 , A = b ) ∀ a , b ∈ A {\displaystyle P(R=1\ |\ Y=0,A=a)=P(R=1\ |\ Y=0,A=b)\quad \forall a,b\in A} A possible relaxation of the given definitions is to allow the value for the difference between rates to be a positive number lower than a given slack ϵ > 0 {\textstyle \epsilon >0} , rather than equal to zero. In some fields separation (separation coefficient) in a confusion matrix is a measure of the distance (at a given level of the probability score) between the predicted cumulative percent negative and predicted cumulative percent positive. The greater this separation coefficient is at a given score value, the more effective the model is at differentiating between the set of positives and negatives at a particular probability cut-off. According to Mayes: "It is often observed in the credit industry that the selection of validation measures depends on the modeling approach. For example, if modeling procedure is parametric or semi-parametric, the two-sample K-S test is often used. If the model is derived by heuristic or iterative search methods, the measure of model performance is usually divergence. A third option is the coefficient of separation...The coefficient of separation, compared to the other two methods, seems to be most reasonable as a measure for model performance because it reflects the separation pattern of a model." Sufficiency We say the random variables ( R , A , Y ) {\textstyle (R,A,Y)} satisfy sufficiency if the sensitive characteristics A {\textstyle A} are statistically independent of the target value Y {\textstyle Y} given the prediction R {\textstyle R} , and we write Y ⊥ A | R . {\displaystyle Y\bot A\ |\ R.} We can also express this notion with the following formula: P ( Y = q | R = r , A = a ) = P ( Y = q | R = r , A = b ) ∀ q ∈ Y r ∈ R ∀ a , b ∈ A {\displaystyle P(Y=q\ |\ R=r,A=a)=P(Y=q\ |\ R=r,A=b)\quad \forall q\in Y\quad r\in R\quad \forall a,b\in A} This means that the probability of actually being in each of the groups is equal for two individuals with different sensitive characteristics given that they were predicted to belong to the same group. Relationships between definitions Finally, we sum up some of the main results that relate the three definitions given above: Assuming Y {\textstyle Y} is binary, if A {\textstyle A} and Y {\textstyle Y} are not statistically independent, and R {\textstyle R} and Y {\textstyle Y} are not statistically independent either, then independence and separation cannot both hold except for rhetorical cases. If ( R , A , Y ) {\textstyle (R,A,Y)} as a joint distribution has positive probability for all its possible values and A {\textstyle A} and Y {\textstyle Y} are not statistically independent, then separation and sufficiency cannot both hold except for rhetorical cases. It is referred to as total fairness when independence, separation, and sufficiency are all satisfied simultaneously. However, total fairness is not possible to achieve except in specific rhetorical cases. Mathematical formulation of group fairness definitions Preliminary definitions Most statistical measures of fairness rely on different metrics, so we will start by defining them. When working with a binary classifier, both the predicted and the actual classes can take two values: positive and negative. Now let us start explaining the different possible relations between predicted and actual outcome: True positive (TP): The case where both the predicted and the actual outcome are in a positive class. True negative (TN): The case where both the predicted outcome and the actual outcome are assigned to the negative class. False positive (FP): A case predicted to befall into a positive class assigned in the actual outcome is to the negative one. False negative (FN): A case predicted to be in the negative class with an actual outcome is in the positive one. These relations can be easily represented with a confusion matrix, a table that descri