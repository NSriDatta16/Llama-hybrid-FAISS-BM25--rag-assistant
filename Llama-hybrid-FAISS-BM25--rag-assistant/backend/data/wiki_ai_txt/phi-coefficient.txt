In statistics, the phi coefficient, or mean square contingency coefficient, denoted by φ or rφ, is a measure of association for two binary variables. In machine learning, it is known as the Matthews correlation coefficient (MCC) and used as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. Introduced by Karl Pearson, and also known as the Yule phi coefficient from its introduction by Udny Yule in 1912 this measure is similar to the Pearson correlation coefficient in its interpretation. In meteorology, the phi coefficient, or its square (the latter aligning with M. H. Doolittle's original proposition from 1885), is referred to as the Doolittle Skill Score or the Doolittle Measure of Association. Definition A Pearson correlation coefficient estimated for two binary variables will return the phi coefficient. Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal. If we have a 2×2 table for two random variables x and y where n11, n10, n01, n00, are non-negative counts of numbers of observations that sum to n, the total number of observations. The phi coefficient that describes the association of x and y is φ = n 11 n 00 − n 10 n 01 n 1 ∙ n 0 ∙ n ∙ 0 n ∙ 1 . {\displaystyle \varphi ={\frac {n_{11}n_{00}-n_{10}n_{01}}{\sqrt {n_{1\bullet }n_{0\bullet }n_{\bullet 0}n_{\bullet 1}}}}.} Phi is related to the point-biserial correlation coefficient and Cohen's d and estimates the extent of the relationship between two variables (2×2). The phi coefficient can also be expressed using only n {\displaystyle n} , n 11 {\displaystyle n_{11}} , n 1 ∙ {\displaystyle n_{1\bullet }} , and n ∙ 1 {\displaystyle n_{\bullet 1}} , as φ = n n 11 − n 1 ∙ n ∙ 1 n 1 ∙ n ∙ 1 ( n − n 1 ∙ ) ( n − n ∙ 1 ) . {\displaystyle \varphi ={\frac {nn_{11}-n_{1\bullet }n_{\bullet 1}}{\sqrt {n_{1\bullet }n_{\bullet 1}(n-n_{1\bullet })(n-n_{\bullet 1})}}}.} Maximum values Although computationally the Pearson correlation coefficient reduces to the phi coefficient in the 2×2 case, they are not in general the same. The Pearson correlation coefficient ranges from −1 to +1, where ±1 indicates perfect agreement or disagreement, and 0 indicates no relationship. The phi coefficient has a maximum value that is determined by the distribution of the two variables if one or both variables can take on more than two values. See Davenport and El-Sanhury (1991) for a thorough discussion. Machine learning The MCC is defined identically to phi coefficient, introduced by Karl Pearson, also known as the Yule phi coefficient from its introduction by Udny Yule in 1912. Despite these antecedents which predate Matthews's use by several decades, the term MCC is widely used in the field of bioinformatics and machine learning. The coefficient accounts for true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. However, if MCC equals neither −1, 0, or +1, it is not a reliable indicator of how similar a predictor is to random guessing because MCC is dependent on the dataset. MCC is closely related to the chi-square statistic for a 2×2 contingency table | MCC | = χ 2 n {\displaystyle |{\text{MCC}}|={\sqrt {\frac {\chi ^{2}}{n}}}} where n is the total number of observations. While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the be