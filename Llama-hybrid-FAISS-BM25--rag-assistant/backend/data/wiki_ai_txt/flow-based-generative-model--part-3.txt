 where m θ {\displaystyle m_{\theta }} is any neural network with weights θ {\displaystyle \theta } . f θ − 1 {\displaystyle f_{\theta }^{-1}} is just z 1 = x 1 , z 2 = x 2 − m θ ( x 1 ) {\displaystyle z_{1}=x_{1},z_{2}=x_{2}-m_{\theta }(x_{1})} , and the Jacobian is just 1, that is, the flow is volume-preserving. When n = 1 {\displaystyle n=1} , this is seen as a curvy shearing along the x 2 {\displaystyle x_{2}} direction. Real Non-Volume Preserving (Real NVP) The Real Non-Volume Preserving model generalizes NICE model by: x = [ x 1 x 2 ] = f θ ( z ) = [ z 1 e s θ ( z 1 ) ⊙ z 2 ] + [ 0 m θ ( z 1 ) ] {\displaystyle x={\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}}=f_{\theta }(z)={\begin{bmatrix}z_{1}\\e^{s_{\theta }(z_{1})}\odot z_{2}\end{bmatrix}}+{\begin{bmatrix}0\\m_{\theta }(z_{1})\end{bmatrix}}} Its inverse is z 1 = x 1 , z 2 = e − s θ ( x 1 ) ⊙ ( x 2 − m θ ( x 1 ) ) {\displaystyle z_{1}=x_{1},z_{2}=e^{-s_{\theta }(x_{1})}\odot (x_{2}-m_{\theta }(x_{1}))} , and its Jacobian is ∏ i = 1 n e s θ ( z 1 , ) {\displaystyle \prod _{i=1}^{n}e^{s_{\theta }(z_{1,})}} . The NICE model is recovered by setting s θ = 0 {\displaystyle s_{\theta }=0} . Since the Real NVP map keeps the first and second halves of the vector x {\displaystyle x} separate, it's usually required to add a permutation ( x 1 , x 2 ) ↦ ( x 2 , x 1 ) {\displaystyle (x_{1},x_{2})\mapsto (x_{2},x_{1})} after every Real NVP layer. Generative Flow (Glow) In generative flow model, each layer has 3 parts: channel-wise affine transform y c i j = s c ( x c i j + b c ) {\displaystyle y_{cij}=s_{c}(x_{cij}+b_{c})} with Jacobian ∏ c s c H W {\displaystyle \prod _{c}s_{c}^{HW}} . invertible 1x1 convolution z c i j = ∑ c ′ K c c ′ y c i j {\displaystyle z_{cij}=\sum _{c'}K_{cc'}y_{cij}} with Jacobian det ( K ) H W {\displaystyle \det(K)^{HW}} . Here K {\displaystyle K} is any invertible matrix. Real NVP, with Jacobian as described in Real NVP. The idea of using the invertible 1x1 convolution is to permute all layers in general, instead of merely permuting the first and second half, as in Real NVP. Masked Autoregressive Flow (MAF) An autoregressive model of a distribution on R n {\displaystyle \mathbb {R} ^{n}} is defined as the following stochastic process: x 1 ∼ N ( μ 1 , σ 1 2 ) x 2 ∼ N ( μ 2 ( x 1 ) , σ 2 ( x 1 ) 2 ) ⋯ x n ∼ N ( μ n ( x 1 : n − 1 ) , σ n ( x 1 : n − 1 ) 2 ) {\displaystyle {\begin{aligned}x_{1}\sim &N(\mu _{1},\sigma _{1}^{2})\\x_{2}\sim &N(\mu _{2}(x_{1}),\sigma _{2}(x_{1})^{2})\\&\cdots \\x_{n}\sim &N(\mu _{n}(x_{1:n-1}),\sigma _{n}(x_{1:n-1})^{2})\\\end{aligned}}} where μ i : R i − 1 → R {\displaystyle \mu _{i}:\mathbb {R} ^{i-1}\to \mathbb {R} } and σ i : R i − 1 → ( 0 , ∞ ) {\displaystyle \sigma _{i}:\mathbb {R} ^{i-1}\to (0,\infty )} are fixed functions that define the autoregressive model. By the reparameterization trick, the autoregressive model is generalized to a normalizing flow: x 1 = μ 1 + σ 1 z 1 x 2 = μ 2 ( x 1 ) + σ 2 ( x 1 ) z 2 ⋯ x n = μ n ( x 1 : n − 1 ) + σ n ( x 1 : n − 1 ) z n {\displaystyle {\begin{aligned}x_{1}=&\mu _{1}+\sigma _{1}z_{1}\\x_{2}=&\mu _{2}(x_{1})+\sigma _{2}(x_{1})z_{2}\\&\cdots \\x_{n}=&\mu _{n}(x_{1:n-1})+\sigma _{n}(x_{1:n-1})z_{n}\\\end{aligned}}} The autoregressive model is recovered by setting z ∼ N ( 0 , I n ) {\displaystyle z\sim N(0,I_{n})} . The forward mapping is slow (because it's sequential), but the backward mapping is fast (because it's parallel). The Jacobian matrix is lower-diagonal, so the Jacobian is σ 1 σ 2 ( x 1 ) ⋯ σ n ( x 1 : n − 1 ) {\displaystyle \sigma _{1}\sigma _{2}(x_{1})\cdots \sigma _{n}(x_{1:n-1})} . Reversing the two maps f θ {\displaystyle f_{\theta }} and f θ − 1 {\displaystyle f_{\theta }^{-1}} of MAF results in Inverse Autoregressive Flow (IAF), which has fast forward mapping and slow backward mapping. Continuous Normalizing Flow (CNF) Instead of constructing flow by function composition, another approach is to formulate the flow as a continuous-time dynamic. Let z 0 {\displaystyle z_{0}}