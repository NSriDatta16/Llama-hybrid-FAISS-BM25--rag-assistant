Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods. While Monte Carlo methods only adjust their estimates once the outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the outcome is known. This is a form of bootstrapping, as illustrated with the following example: Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives. Temporal difference methods are related to the temporal difference model of animal learning. Mathematical formulation The tabular TD(0) method is one of the simplest TD methods. It is a special case of more general stochastic approximation methods. It estimates the state value function of a finite-state Markov decision process (MDP) under a policy π {\displaystyle \pi } . Let V π {\displaystyle V^{\pi }} denote the state value function of the MDP with states ( S t ) t ∈ N {\displaystyle (S_{t})_{t\in \mathbb {N} }} , rewards ( R t ) t ∈ N {\displaystyle (R_{t})_{t\in \mathbb {N} }} and discount rate γ {\displaystyle \gamma } under the policy π {\displaystyle \pi } : V π ( s ) = E a ∼ π { ∑ t = 0 ∞ γ t R t + 1 | S 0 = s } . {\displaystyle V^{\pi }(s)=E_{a\sim \pi }\left\{\sum _{t=0}^{\infty }\gamma ^{t}R_{t+1}{\Bigg |}S_{0}=s\right\}.} We drop the action from the notation for convenience. V π {\displaystyle V^{\pi }} satisfies the Hamilton-Jacobi-Bellman Equation: V π ( s ) = E π { R 1 + γ V π ( S 1 ) | S 0 = s } , {\displaystyle V^{\pi }(s)=E_{\pi }\{R_{1}+\gamma V^{\pi }(S_{1})|S_{0}=s\},} so R 1 + γ V π ( S 1 ) {\displaystyle R_{1}+\gamma V^{\pi }(S_{1})} is an unbiased estimate for V π ( s ) {\displaystyle V^{\pi }(s)} . This observation motivates the following algorithm for estimating V π {\displaystyle V^{\pi }} . The algorithm starts by initializing a table V ( s ) {\displaystyle V(s)} arbitrarily, with one value for each state of the MDP. A positive learning rate α {\displaystyle \alpha } is chosen. We then repeatedly evaluate the policy π {\displaystyle \pi } , obtain a reward r {\displaystyle r} and update the value function for the current state using the rule: V ( S t ) ← ( 1 − α ) V ( S t ) + α ⏟ learning rate [ R t + 1 + γ V ( S t + 1 ) ⏞ The TD target ] {\displaystyle V(S_{t})\leftarrow (1-\alpha )V(S_{t})+\underbrace {\alpha } _{\text{learning rate}}[\overbrace {R_{t+1}+\gamma V(S_{t+1})} ^{\text{The TD target}}]} where S t {\displaystyle S_{t}} and S t + 1 {\displaystyle S_{t+1}} are the current and next states, respectively. The value R t + 1 + γ V ( S t + 1 ) {\displaystyle R_{t+1}+\gamma V(S_{t+1})} is known as the TD target, and R t + 1 + γ V ( S t + 1 ) − V ( S t ) {\displaystyle R_{t+1}+\gamma V(S_{t+1})-V(S_{t})} is known as the TD error. TD-Lambda TD-Lambda is a learning algorithm invented by Richard S. Sutton based on earlier work on temporal difference learning by Arthur Samuel. This algorithm was famously applied by Gerald Tesauro to create TD-Gammon, a program that learned to play the game of backgammon at the level of expert human players. The lambda ( λ {\displaystyle \lambda } ) parameter refers to the trace decay parameter, with 0 ⩽ λ ⩽ 1 {\displaystyle 0\leqslant \lambda \leqslant 1} . Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when λ {\displaystyle \lambda } is higher, with