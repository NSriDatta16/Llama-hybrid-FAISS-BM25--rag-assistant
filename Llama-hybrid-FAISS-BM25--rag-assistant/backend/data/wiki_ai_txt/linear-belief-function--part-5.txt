}}\\{{\bar {\Sigma }}_{22}-{\bar {\Sigma }}_{21}({\bar {\Sigma }}_{11})^{-1}{\bar {\Sigma }}_{12}}\\\end{array}}\\\end{array}}\right]} Reverse sweepings are similar to those of forward ones, except for a sign difference for some multiplications. However, forward and reverse sweepings are opposite operations. It can be easily shown that applying the fully reverse sweeping to M ( X → ) {\displaystyle M({\vec {X}})} will recover the initial moment matrix M(X). It can also be proved that applying a partial reverse sweeping on X to the matrix M ( X → , Y ) {\displaystyle M({\vec {X}},Y)} will recover the moment matrix M(X,Y). As a matter of fact, Liu proves that a moment matrix will be recovered through a reverse sweeping after a forward sweeping on the same set of variables. It can be also recovered through a forward sweeping after a reverse sweeping. Intuitively, a partial forward sweeping factorizes a joint into a marginal and a conditional, whereas a partial reverse sweeping multiplies them into a joint. Combination According to Dempster’s rule, the combination of belief functions may be expressed as the intersection of focal elements and the multiplication of probability density functions. Liping Liu applies the rule to linear belief functions in particular and obtains a formula of combination in terms of density functions. Later he proves a claim by Arthur P. Dempster and reexpresses the formula as the sum of two fully swept matrices. Mathematically, assume M 1 ( X → ) = ( μ ¯ 1 Σ ¯ 1 ) {\displaystyle M_{1}({\vec {X}})=\left({\begin{array}{*{20}c}{{\bar {\mu }}_{1}}\\{{\bar {\Sigma }}_{1}}\\\end{array}}\right)} and M 2 ( X → ) = ( μ ¯ 2 Σ ¯ 2 ) {\displaystyle M_{2}({\vec {X}})=\left({\begin{array}{*{20}c}{{\bar {\mu }}_{2}}\\{{\bar {\Sigma }}_{2}}\\\end{array}}\right)} are two LBFs for the same vector of variables X. Then their combination is a fully swept matrix: M ( X → ) = ( μ ¯ 1 + μ ¯ 2 Σ ¯ 1 + Σ ¯ 2 ) {\displaystyle M({\vec {X}})=\left({\begin{array}{*{20}c}{{\bar {\mu }}_{1}+{\bar {\mu }}_{2}}\\{{\bar {\Sigma }}_{1}+{\bar {\Sigma }}_{2}}\\\end{array}}\right)} This above equation is often used for multiplying two normal distributions. Here we use it to define the combination of two linear belief functions, which include normal distributions as a special case. Also, note that a vacuous linear belief function (0 swept matrix) is the neutral element for combination. When applying the equation, we need to consider two special cases. First, if two matrices to be combined have different dimensions, then one or both matrices must be vacuously extended, i.e., assuming ignorance on the variables that are no present in each matrix. For example, if M1(X,Y) and M2(X,Z) are to be combined, we will first extend them into M 1 ( X , Y , Z → ) {\displaystyle M_{1}(X,Y,{\vec {Z}})} and M 2 ( X , Y → , Z ) {\displaystyle M_{2}(X,{\vec {Y}},Z)} respectively such that M 1 ( X , Y , Z → ) {\displaystyle M_{1}(X,Y,{\vec {Z}})} is ignorant about Z and M 2 ( X , Y → , Z ) {\displaystyle M_{2}(X,{\vec {Y}},Z)} is ignorant about Y. The vacuous extension was initially proposed by Kong for discrete belief functions. Second, if a variable has zero variance, it will not permit a sweeping operation. In this case, we can pretend the variance to be an extremely small number, say ε, and perform the desired sweeping and combination. We can then apply a reverse sweeping to the combined matrix on the same variable and let ε approaches 0. Since zero variance means complete certainty about a variable, this ε-procedure will vanish ε terms in the final result. In general, to combine two linear belief functions, their moment matrices must be fully swept. However, one may combine a fully swept matrix with a partially swept one directly if the variables of the former matrix have been all swept on in the later. We can use the linear regression model — Y = XA + b + E — to illustrate the property. As we mentioned, the regression model may be considered as the combina