"shortcut learning." Critics of claims for LLM understanding argue that high benchmark scores can be misleading. When tests created to test people for language comprehension are used to test LLMs, they sometimes result in false positives caused by spurious correlations within text data. Models have shown examples of shortcut learning, which is when a system makes unrelated correlations within data instead of using human-like understanding. One such experiment conducted in 2019 tested Google's BERT LLM using the argument reasoning comprehension task. BERT was prompted to choose between 2 statements, and find the one most consistent with an argument. Below is an example of one of these prompts: Researchers found that specific words such as "not" hint the model towards the correct answer, allowing near-perfect scores when included but resulting in random selection when hint words were removed. This problem, and the known difficulties defining intelligence, causes some to argue all benchmarks that find understanding in LLMs are flawed, that they all allow shortcuts to fake understanding. See also Chinese room Criticism of artificial neural networks Criticism of deep learning Generative AI Mark V. Shaney, an early chatbot that used a very simple three-word Markov chain algorithm to generate Markov text Autocomplete References Works cited Lindholm, A.; WahlstrÃ¶m, N.; Lindsten, F.; SchÃ¶n, T. B. (2022). Machine Learning: A First Course for Engineers and Scientists. Cambridge University Press. ISBN 978-1-108-84360-7. Weller, Adrian (July 13, 2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ (video). Alan Turing Institute. Keynote by Emily Bender. The presentation was followed by a panel discussion. Further reading Bogost, Ian (December 7, 2022). "ChatGPT Is Dumber Than You Think: Treat it like a toy, not a tool". The Atlantic. Retrieved 2024-01-17. Chomsky, Noam (March 8, 2023). "The False Promise of ChatGPT". The New York Times. Retrieved 2024-01-17. Glenberg, Arthur; Jones, Cameron Robert (April 6, 2023). "It takes a body to understand the world â€“ why ChatGPT and other language AIs don't know what they're saying". The Conversation. Retrieved 2024-01-17. McQuillan, D. (2022). Resisting AI: An Anti-fascist Approach to Artificial Intelligence. Bristol University Press. ISBN 978-1-5292-1350-8. Thompson, E. (2022). Escape from Model Land: How Mathematical Models Can Lead Us Astray and What We Can Do about It. Basic Books. ISBN 978-1-5416-0098-0. Zhong, Qihuang; Ding, Liang; Liu, Juhua; Du, Bo; Tao, Dacheng (2023). "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT". arXiv:2302.10198 [cs.CL]. External links "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ" at Wikimedia Commons