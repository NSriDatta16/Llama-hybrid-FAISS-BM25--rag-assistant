lly hindered by concerns about mismanagement of data collected, especially in the United States. The use of large language models for healthcare consultations introduces particular privacy risks, such as increased exposure of sensitive health information during consultations that may be collected for model retraining. A 2024 study of 846 Chinese users found that while 77.3% expressed willingness to use LLM-based healthcare services, privacy awareness varied significantly by demographics and cultural context. The research revealed a "privacy paradox" where users who claimed greater privacy knowledge and concern actually showed higher acceptance of information sharing, potentially due to better understanding of legitimate uses such as academic research and service improvement. Privacy expectations for LLMs vary significantly across cultural contexts. Research in China has shown that users may have different privacy norms compared to Western populations, with factors such as age, education level, and medical background influencing acceptance of data sharing. Younger and more educated users tend to be more privacy-conscious, while those with medical backgrounds show greater acceptance of health data sharing for legitimate medical purposes. Technological unemployment A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic, or fulfill beneficence. According to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years. However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction. Outputs can be incorrect or incomplete and diagnosis and recommendations harm people. Bias and discrimination Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care, i.e. violating the ethical principle of social justice or non-maleficence. A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping. There can be unintended bias in algorithms that can exacerbate social and healthcare inequities. Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. For instance, if populations are less represented in healthcare data it is likely to create bias in AI tools that lead to incorrect assumptions of a demographic and impact the ability to provide appropriate care. White males are overly represented in medical data sets. Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations. Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients. In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influen