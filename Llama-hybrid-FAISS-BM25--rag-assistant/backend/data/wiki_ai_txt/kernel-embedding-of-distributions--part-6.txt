yle Y} from the joint density (including the prior distribution on Y {\displaystyle Y} ) Q ( X ) = ∫ Ω P ( X ∣ Y ) d π ( Y ) {\displaystyle Q(X)=\int _{\Omega }P(X\mid Y)\,\mathrm {d} \pi (Y)} The analog of this rule in the kernel embedding framework states that μ X π , {\displaystyle \mu _{X}^{\pi },} the RKHS embedding of Q ( X ) {\displaystyle Q(X)} , can be computed via μ X π = E [ C X ∣ Y φ ( Y ) ] = C X ∣ Y E [ φ ( Y ) ] = C X ∣ Y μ Y π {\displaystyle \mu _{X}^{\pi }=\mathbb {E} [{\mathcal {C}}_{X\mid Y}\varphi (Y)]={\mathcal {C}}_{X\mid Y}\mathbb {E} [\varphi (Y)]={\mathcal {C}}_{X\mid Y}\mu _{Y}^{\pi }} where μ Y π {\displaystyle \mu _{Y}^{\pi }} is the kernel embedding of π ( Y ) . {\displaystyle \pi (Y).} In practical implementations, the kernel sum rule takes the following form μ ^ X π = C ^ X ∣ Y μ ^ Y π = Υ ( G + λ I ) − 1 G ~ α {\displaystyle {\widehat {\mu }}_{X}^{\pi }={\widehat {\mathcal {C}}}_{X\mid Y}{\widehat {\mu }}_{Y}^{\pi }={\boldsymbol {\Upsilon }}(\mathbf {G} +\lambda \mathbf {I} )^{-1}{\widetilde {\mathbf {G} }}{\boldsymbol {\alpha }}} where μ Y π = ∑ i = 1 n ~ α i φ ( y ~ i ) {\displaystyle \mu _{Y}^{\pi }=\sum _{i=1}^{\widetilde {n}}\alpha _{i}\varphi ({\widetilde {y}}_{i})} is the empirical kernel embedding of the prior distribution, α = ( α 1 , … , α n ~ ) T , {\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{\widetilde {n}})^{T},} Υ = ( φ ( x 1 ) , … , φ ( x n ) ) {\displaystyle {\boldsymbol {\Upsilon }}=\left(\varphi (x_{1}),\ldots ,\varphi (x_{n})\right)} , and G , G ~ {\displaystyle \mathbf {G} ,{\widetilde {\mathbf {G} }}} are Gram matrices with entries G i j = k ( y i , y j ) , G ~ i j = k ( y i , y ~ j ) {\displaystyle \mathbf {G} _{ij}=k(y_{i},y_{j}),{\widetilde {\mathbf {G} }}_{ij}=k(y_{i},{\widetilde {y}}_{j})} respectively. Kernel chain rule In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions Q ( X , Y ) = P ( X ∣ Y ) π ( Y ) {\displaystyle Q(X,Y)=P(X\mid Y)\pi (Y)} The analog of this rule in the kernel embedding framework states that C X Y π , {\displaystyle {\mathcal {C}}_{XY}^{\pi },} the joint embedding of Q ( X , Y ) , {\displaystyle Q(X,Y),} can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with π ( Y ) {\displaystyle \pi (Y)} C X Y π = C X ∣ Y C Y Y π {\displaystyle {\mathcal {C}}_{XY}^{\pi }={\mathcal {C}}_{X\mid Y}{\mathcal {C}}_{YY}^{\pi }} where C X Y π = E [ φ ( X ) ⊗ φ ( Y ) ] , {\displaystyle {\mathcal {C}}_{XY}^{\pi }=\mathbb {E} [\varphi (X)\otimes \varphi (Y)],} C Y Y π = E [ φ ( Y ) ⊗ φ ( Y ) ] . {\displaystyle {\mathcal {C}}_{YY}^{\pi }=\mathbb {E} [\varphi (Y)\otimes \varphi (Y)].} In practical implementations, the kernel chain rule takes the following form C ^ X Y π = C ^ X ∣ Y C ^ Y Y π = Υ ( G + λ I ) − 1 G ~ diag ⁡ ( α ) Φ ~ T {\displaystyle {\widehat {\mathcal {C}}}_{XY}^{\pi }={\widehat {\mathcal {C}}}_{X\mid Y}{\widehat {\mathcal {C}}}_{YY}^{\pi }={\boldsymbol {\Upsilon }}(\mathbf {G} +\lambda \mathbf {I} )^{-1}{\widetilde {\mathbf {G} }}\operatorname {diag} ({\boldsymbol {\alpha }}){\boldsymbol {\widetilde {\Phi }}}^{T}} Kernel Bayes' rule In probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as Q ( Y ∣ x ) = P ( x ∣ Y ) π ( Y ) Q ( x ) {\displaystyle Q(Y\mid x)={\frac {P(x\mid Y)\pi (Y)}{Q(x)}}} where Q ( x ) = ∫ Ω P ( x ∣ y ) d π ( y ) {\displaystyle Q(x)=\int _{\Omega }P(x\mid y)\,\mathrm {d} \pi (y)} The analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution μ Y ∣ x π = C Y ∣ X π φ ( x ) = C Y X π ( C X X π ) − 1 φ ( x ) {\displaystyle \mu _{Y\mid x}^{\pi }={\mathcal {C}}_{Y\mid X}^{\pi }\varphi (x)={\mathcal {C}}_{YX}^{\pi }\left({\mathcal {C}}_{XX}^{\pi }\right)^{-1}\varphi (x)} wher