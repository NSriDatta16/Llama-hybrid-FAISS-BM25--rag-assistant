 (CPD) was introduced by Myronenko and Song. The algorithm takes a probabilistic approach to aligning point sets, similar to the GMM KC method. Unlike earlier approaches to non-rigid registration which assume a thin plate spline transformation model, CPD is agnostic with regard to the transformation model used. The point set M {\displaystyle {\mathcal {M}}} represents the Gaussian mixture model (GMM) centroids. When the two point sets are optimally aligned, the correspondence is the maximum of the GMM posterior probability for a given data point. To preserve the topological structure of the point sets, the GMM centroids are forced to move coherently as a group. The expectation maximization algorithm is used to optimize the cost function. Let there be M points in M {\displaystyle {\mathcal {M}}} and N points in S {\displaystyle {\mathcal {S}}} . The GMM probability density function for a point s is: where, in D dimensions, p ( s | i ) {\displaystyle p(s|i)} is the Gaussian distribution centered on point m i ∈ M {\displaystyle m_{i}\in {\mathcal {M}}} . p ( s | i ) = 1 ( 2 π σ 2 ) D / 2 exp ⁡ ( − ‖ s − m i ‖ 2 2 σ 2 ) {\displaystyle p(s|i)={\frac {1}{(2\pi \sigma ^{2})^{D/2}}}\exp {\left(-{\frac {\lVert s-m_{i}\rVert ^{2}}{2\sigma ^{2}}}\right)}} The membership probabilities P ( i ) = 1 M {\displaystyle P(i)={\frac {1}{M}}} is equal for all GMM components. The weight of the uniform distribution is denoted as w ∈ [ 0 , 1 ] {\displaystyle w\in [0,1]} . The mixture model is then: The GMM centroids are re-parametrized by a set of parameters θ {\displaystyle \theta } estimated by maximizing the likelihood. This is equivalent to minimizing the negative log-likelihood function: where it is assumed that the data is independent and identically distributed. The correspondence probability between two points m i {\displaystyle m_{i}} and s j {\displaystyle s_{j}} is defined as the posterior probability of the GMM centroid given the data point: P ( i | s j ) = P ( i ) p ( s j | i ) p ( s j ) {\displaystyle P(i|s_{j})={\frac {P(i)p(s_{j}|i)}{p(s_{j})}}} The expectation maximization (EM) algorithm is used to find θ {\displaystyle \theta } and σ 2 {\displaystyle \sigma ^{2}} . The EM algorithm consists of two steps. First, in the E-step or estimation step, it guesses the values of parameters ("old" parameter values) and then uses Bayes' theorem to compute the posterior probability distributions P old ( i , s j ) {\displaystyle P^{\text{old}}(i,s_{j})} of mixture components. Second, in the M-step or maximization step, the "new" parameter values are then found by minimizing the expectation of the complete negative log-likelihood function, i.e. the cost function: Ignoring constants independent of θ {\displaystyle \theta } and σ {\displaystyle \sigma } , Equation (cpd.4) can be expressed thus: where N P = ∑ j = 0 N ∑ i = 0 M P old ( i | s j ) ≤ N {\displaystyle N_{\mathbf {P} }=\sum _{j=0}^{N}\sum _{i=0}^{M}P^{\text{old}}(i|s_{j})\leq N} with N = N P {\displaystyle N=N_{\mathbf {P} }} only if w = 0 {\displaystyle w=0} . The posterior probabilities of GMM components computed using previous parameter values P old {\displaystyle P^{\text{old}}} is: Minimizing the cost function in Equation (cpd.5) necessarily decreases the negative log-likelihood function E in Equation (cpd.3) unless it is already at a local minimum. Thus, the algorithm can be expressed using the following pseudocode, where the point sets M {\displaystyle {\mathcal {M}}} and S {\displaystyle {\mathcal {S}}} are represented as M × D {\displaystyle M\times D} and N × D {\displaystyle N\times D} matrices M {\displaystyle \mathbf {M} } and S {\displaystyle \mathbf {S} } respectively: algorithm CPD ( M , S ) {\displaystyle ({\mathcal {M}},{\mathcal {S}})} θ := θ0 initialize 0 ≤ w ≤ 1 σ 2 := 1 D N M ∑ j = 1 N ∑ i = 1 M ‖ s j − m i ‖ 2 {\displaystyle \sigma ^{2}:={\frac {1}{DNM}}\sum _{j=1}^{N}\sum _{i=1}^{M}\lVert s_{j}-m_{i}\rVert ^{2}} while not registered: // E-step, compute P for i ∊ [1,