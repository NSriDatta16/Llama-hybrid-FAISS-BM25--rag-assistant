nt for each loss. A self-driving car's objective function would be more complex. Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a "fitness function" influencing how many descendants each agent is allowed to leave. The mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm. However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware. Agent function An intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen. A percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn). The agent function, often denoted as f, maps the agent's entire history of percepts to an action. Mathematically, this can be represented as f : P ∗ → A , {\displaystyle f\colon P^{*}\rightarrow A,} where: P ∗ {\displaystyle {\boldsymbol {P^{*}}}} represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts. A {\displaystyle {\boldsymbol {A}}} represents the set of all possible actions the agent can take. f {\displaystyle {\boldsymbol {f}}} is the agent function that maps a percept sequence to an action. It's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function). The agent function is a theoretical description. The agent program is the actual code that runs on the agent. The agent program takes the current percept as input and produces an action as output. The agent function can incorporate a wide range of decision-making approaches, including: Calculating the utility (desirability) of different actions. Using logical rules and deduction. Employing fuzzy logic. Other methods. Classes of intelligent agents Russell and Norvig's classification Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability: Simple reflex agents Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: "if condition, then action". This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops. A home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent. Model-based reflex agents A model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about "how the world works" is referred to as a model of the world, hence the name "model-based agent". A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent. An agent may also use models t