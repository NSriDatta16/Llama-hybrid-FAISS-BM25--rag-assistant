{pmatrix}}={\begin{pmatrix}x_{11}&\cdots &x_{1p}\\x_{21}&\cdots &x_{2p}\\\vdots &\ddots &\vdots \\x_{n1}&\cdots &x_{np}\end{pmatrix}},\quad {\boldsymbol {\beta }}={\begin{pmatrix}\beta _{1}\\\vdots \\\beta _{p}\end{pmatrix}},\quad {\boldsymbol {\varepsilon }}={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\\vdots \\\varepsilon _{n}\end{pmatrix}}.} The matrix X is known as the design matrix and encodes all known information about the independent variables. The variables ε i {\displaystyle \varepsilon _{i}} are random variables, which in standard linear regression are distributed according to a standard normal distribution; they express the influence of any unknown factors on the outcome. This makes it possible to find optimal coefficients through the method of least squares using simple matrix operations. In particular, the optimal coefficients β ^ {\displaystyle {\boldsymbol {\hat {\beta }}}} as estimated by least squares can be written as follows: β ^ = ( X T X ) − 1 X T y . {\displaystyle {\boldsymbol {\hat {\beta }}}=(X^{\mathrm {T} }X)^{-1}X^{\mathrm {T} }\mathbf {y} .} The matrix ( X T X ) − 1 X T {\displaystyle (X^{\mathrm {T} }X)^{-1}X^{\mathrm {T} }} is known as the Moore–Penrose pseudoinverse of X. The use of the matrix inverse in this formula requires that X is of full rank, i.e. there is not perfect multicollinearity among different explanatory variables (i.e. no explanatory variable can be perfectly predicted from the others). In such cases, the singular value decomposition can be used to compute the pseudoinverse. Preprocessing of explanatory variables When a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as basis functions. An example is polynomial regression, which uses a linear predictor function to fit an arbitrary degree polynomial relationship (up to a given order) between two sets of data points (i.e. a single real-valued explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable. Mathematically, the form looks like this: y i = β 0 + β 1 x i + β 2 x i 2 + ⋯ + β p x i p . {\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i}+\beta _{2}x_{i}^{2}+\cdots +\beta _{p}x_{i}^{p}.} In this case, for each data point i, a set of explanatory variables is created as follows: ( x i 1 = x i , x i 2 = x i 2 , … , x i p = x i p ) {\displaystyle (x_{i1}=x_{i},\quad x_{i2}=x_{i}^{2},\quad \ldots ,\quad x_{ip}=x_{i}^{p})} and then standard linear regression is run. The basis functions in this example would be ϕ ( x ) = ( ϕ 1 ( x ) , ϕ 2 ( x ) , … , ϕ p ( x ) ) = ( x , x 2 , … , x p ) . {\displaystyle {\boldsymbol {\phi }}(x)=(\phi _{1}(x),\phi _{2}(x),\ldots ,\phi _{p}(x))=(x,x^{2},\ldots ,x^{p}).} This example shows that a linear predictor function can actually be much more powerful than it first appears: It only really needs to be linear in the coefficients. All sorts of non-linear functions of the explanatory variables can be fit by the model. There is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a K-dimensional output value is likely to be treated as K separate scalar-output basis functions). An example of this is radial basis functions (RBF's), which compute some transformed version of the distance to some fixed point: ϕ ( x ; c ) = ϕ ( | | x − c | | ) = ϕ ( ( x 1 − c 1 ) 2 + … + ( x K − c K ) 2 ) {\displaystyle \phi (\mathbf {x} ;\mathbf {c} )=\phi (||\mathbf {x} -\mathbf {c} ||)=\phi ({\sqrt {(x_{1}-c_{1})^{2}+\ldots +(x_{K}-c_{K})^{2}}})} An example is the Gaussian RBF, which has the same functional form as the normal distribution: ϕ ( x ; c ) = e − b | | x − c | | 2 {\displaystyle \phi (\mathbf {x} ;\mathbf {c} )=e^{-b||\mathbf {x} -\mathbf {c} ||^{2}}} which drops off rapidly as the distance from c increases. A possible usage of RBF'