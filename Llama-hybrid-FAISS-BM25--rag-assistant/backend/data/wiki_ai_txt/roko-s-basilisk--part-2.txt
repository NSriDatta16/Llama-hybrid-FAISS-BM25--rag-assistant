y reacted harshly, calling Roko an "idiot". Yudkowsky harshly refuted the idea, arguing that a friendly superintelligent artificial intelligence would have no incentive to carry out the punishment after its own creation. However, in the original post, Roko reported someone having nightmares about the thought experiment. Yudkowsky did not want that to happen to other users who might obsess over the idea. He was also worried there might be some variant on Roko's argument that worked, which could be a serious information hazard. So he took down the post and banned discussion of the topic outright for five years on the platform. However, likely due to the Streisand effect, the post gained LessWrong much more attention than it had previously received, and the post has since been acknowledged on the site. In a later post, in 2015, he expressed regret for his initial overreaction. In the 2020s, the philosophy of the Zizians was heavily influenced by the Roko's basilisk thought experiment. Ziz LaSota, the leader of the cult, believes the basilisk to be real and wrote on her blog: "Eventually I came to believe that if I persisted in trying to save the world, I would be tortured until the end of the universe by a coalition of all unfriendly A.I.s." Philosophy Pascal's wager Roko's basilisk has been viewed as a version of Pascal's wager, which proposes that a rational person should live as though God exists and seek to believe in God, regardless of the probability of God's existence, because the finite costs of believing are insignificant compared to the infinite punishment associated with not believing (eternity in Hell) and the infinite rewards for believing (eternity in Heaven). Roko's basilisk analogously proposes that a rational person should contribute to the creation of the basilisk, because the cost of contributing would be insignificant compared to the extreme pain of the punishment that the basilisk would otherwise inflict on simulations. Newcomb's paradox Newcomb's paradox, created by physicist William Newcomb in 1960, describes a "predictor" who is aware of what will occur in the future. When a player is asked to choose between two boxes, the first containing £1000 and the second either containing £1,000,000 or nothing, the super-intelligent predictor already knows what the player will do. As such, the contents of box B varies depending on what the player does; the paradox lies in whether the being is really super-intelligent. Roko's basilisk functions in a similar manner to this problem – one can take the risk of doing nothing, or assist in creating the basilisk itself. Assisting the basilisk may either lead to nothing or the reward of not being punished by it, but it varies depending on whether one believes in the basilisk and if it ever comes to be at all. Implicit religion Implicit religion refers to people's commitments taking a religious form. Since the basilisk would hypothetically force anyone who did not assist in creating it to devote their life to it, the basilisk is an example of this concept. Others have taken this idea further, such as former Slate columnist David Auerbach, who stated that the singularity and the basilisk "brings about the equivalent of God itself." Commenting on the belief system of the Zizians, Anna Salamon, the director of the Center for Applied Rationality, said: "There’s this all-or-nothing thing, where A.I. will either bring utopia by solving all the problems, if it’s successfully controlled, or literally kill everybody. From my perspective, that’s already a chunk of the way toward doomsday cult dynamics." Legacy In 2014, Slate magazine called Roko's basilisk "The Most Terrifying Thought Experiment of All Time" while Yudkowsky had called it "a genuinely dangerous thought" upon its posting. However, opinions diverged on LessWrong itself – user Gwern stated "Only a few LWers seem to take the basilisk very seriously", and added "It's funny how everyone seems to know all about who is affected 