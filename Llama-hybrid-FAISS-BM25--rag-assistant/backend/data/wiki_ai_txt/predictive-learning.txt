Predictive learning is a machine learning (ML) technique where an artificial intelligence model is fed new data to develop an understanding of its environment, capabilities, and limitations. This technique finds application in many areas, including neuroscience, business, robotics, and computer vision. This concept was developed and expanded by French computer scientist Yann LeCun in 1988 during his career at Bell Labs, where he trained models to detect handwriting so that financial companies could automate check processing. The mathematical foundation for predictive learning dates back to the 17th century, where British insurance company Lloyd's used predictive analytics to make a profit. Starting out as a mathematical concept, this method expanded the possibilities of artificial intelligence. Predictive learning is an attempt to learn with a minimum of pre-existing mental structure. It was inspired by Jean Piaget's account of children constructing knowledge of the world through interaction. Gary Drescher's book Made-up Minds was crucial to the development of this concept. The idea that predictions and unconscious inference are used by the brain to construct a model of the world, in which it can identify causes of percepts, goes back even further to Hermann von Helmholtz's iteration of this study. These ideas were further developed by the field of predictive coding. Another related predictive learning theory is Jeff Hawkins' memory-prediction framework, which is laid out in his book On Intelligence. Mathematical procedures Training process Similar to ML, predictive learning aims to extrapolate the value of an unknown dependent variable Y {\displaystyle Y} , given independent input data X = ( x 1 , x 2 , … , x n ) {\displaystyle X=(x_{1},x_{2},\dots ,x_{n})} . A set of attributes can be classified into categorical data (discrete factors such as race, sex, or affiliation) or numerical data (continuous values such as temperature, annual income, or speed). Every set of input values is fed into a neural network to predict a value y {\displaystyle y} . In order to predict the output accurately, the weights of the neural network (which represent how much each predictor variable affects the outcome) must be incrementally adjusted via backpropagation to produce estimates closer to the actual data. Once an ML model is given enough adjustments through training to predict values closer to the ground truth, it should be able to correctly predict outputs of new data with little error. Maximizing accuracy In order to ensure maximum accuracy for a predictive learning model, the predicted values y ^ = F ( x ) {\displaystyle {\hat {y}}=F(x)} must not exceed a certain error threshold when compared to actual values y {\displaystyle y} by the risk formula: R ( F ) = E x y L ( y , F ( x ) ) {\displaystyle R(F)=E_{xy}L(y,F(x))} , where L {\displaystyle L} is the loss function, y {\displaystyle y} is the ground truth, and F ( x ) {\displaystyle F(x)} is the predicted data. This error function is used to make incremental adjustments to the model's weights to eventually reach a well-trained prediction of: F ∗ ( x ) = argmin F ( x ) E x y {\displaystyle F^{*}(x)={\underset {F(x)}{\operatorname {argmin} }}\,E_{xy}} L ( y , F ( x ) ) {\displaystyle L(y,F(x))} Once the error is negligible or considered small enough after training, the model is said to have converged. Ensemble learning In some cases, using a singular machine learning approach is not enough to create an accurate estimate for certain data. Ensemble learning is the combination of several ML algorithms to create a stronger model. Each model is represented by the function F ( x ) = a 0 + ∑ m = 1 M a m f m ( x ) {\displaystyle F(x)=a_{0}+\sum _{m=1}^{M}a_{m}f_{m}(x)} , where M {\displaystyle M} is the number of ensemble models, a 0 {\displaystyle a_{0}} is the bias, a m {\displaystyle a_{m}} is the weight corresponding to each m {\displaystyle m} -th variable, and f m ( x ) {\displaystyle f_{m}(