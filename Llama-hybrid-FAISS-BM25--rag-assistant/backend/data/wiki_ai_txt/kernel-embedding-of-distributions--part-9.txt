style \mathbb {R} ^{d}} : the characteristic property of the individual kernels remains an equivalent condition. on general domains: the characteristic property of the kernel components is necessary but not sufficient. Kernel belief propagation Belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given n samples of random variables represented by nodes in a Markov random field, the incoming message to node t from node u can be expressed as m u t ( ⋅ ) = ∑ i = 1 n β u t i φ ( x t i ) {\displaystyle m_{ut}(\cdot )=\sum _{i=1}^{n}\beta _{ut}^{i}\varphi (x_{t}^{i})} if it assumed to lie in the RKHS. The kernel belief propagation update message from t to node s is then given by m ^ t s = ( ⊙ u ∈ N ( t ) ∖ s K t β u t ) T ( K s + λ I ) − 1 Υ s T φ ( x s ) {\displaystyle {\widehat {m}}_{ts}=\left(\odot _{u\in N(t)\backslash s}\mathbf {K} _{t}{\boldsymbol {\beta }}_{ut}\right)^{T}(\mathbf {K} _{s}+\lambda \mathbf {I} )^{-1}{\boldsymbol {\Upsilon }}_{s}^{T}\varphi (x_{s})} where ⊙ {\displaystyle \odot } denotes the element-wise vector product, N ( t ) ∖ s {\displaystyle N(t)\backslash s} is the set of nodes connected to t excluding node s, β u t = ( β u t 1 , … , β u t n ) {\displaystyle {\boldsymbol {\beta }}_{ut}=\left(\beta _{ut}^{1},\dots ,\beta _{ut}^{n}\right)} , K t , K s {\displaystyle \mathbf {K} _{t},\mathbf {K} _{s}} are the Gram matrices of the samples from variables X t , X s {\displaystyle X_{t},X_{s}} , respectively, and Υ s = ( φ ( x s 1 ) , … , φ ( x s n ) ) {\displaystyle {\boldsymbol {\Upsilon }}_{s}=\left(\varphi (x_{s}^{1}),\dots ,\varphi (x_{s}^{n})\right)} is the feature matrix for the samples from X s {\displaystyle X_{s}} . Thus, if the incoming messages to node t are linear combinations of feature mapped samples from X t {\displaystyle X_{t}} , then the outgoing message from this node is also a linear combination of feature mapped samples from X s {\displaystyle X_{s}} . This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled. Nonparametric filtering in hidden Markov models In the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states P ( S t ∣ S t − 1 ) {\displaystyle P(S^{t}\mid S^{t-1})} and the emission probabilities P ( O t ∣ S t ) {\displaystyle P(O^{t}\mid S^{t})} for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible. One common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state s t {\displaystyle s^{t}} at time step t given a history of previous observations h t = ( o 1 , … , o t ) {\displaystyle h^{t}=(o^{1},\dots ,o^{t})} from the system. In filtering, a belief state P ( S t + 1 ∣ h t + 1 ) {\displaystyle P(S^{t+1}\mid h^{t+1})} is recursively maintained via a prediction step (where updates P ( S t + 1 ∣ h t ) = E [ P ( S t + 1 ∣ S t ) ∣ h t ] {\displaystyle P(S^{t+1}\mid h^{t})=\mathbb {E} [P(S^{t+1}\mid S^{t})\mid h^{t}]} are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates P ( S t + 1 ∣ h t , o t + 1 ) ∝ P ( o t + 1 ∣ S t + 1 ) P ( S t + 1 ∣ h t ) {\displaystyle P(S^{t+1}\mid h^{t},o^{t+1})\propto P(o^{t+1}\mid S^{t+1})P(S^{t+1}\mid 