, A → B ⟺ P ( A → B ) = 1 ⟺ P ( A ∧ B ∨ ¬ A ) = 1 ⟺ P ( A ∧ B ) + P ( ¬ A ) = 1 ⟺ P ( A ∧ B ) = P ( A ) ⟺ P ( A ) ⋅ P ( B | A ) = P ( A ) ⟺ P ( B | A ) = 1 {\displaystyle {\begin{aligned}A\to B&\iff P(A\to B)=1\\&\iff P(A\land B\lor \neg A)=1\\&\iff P(A\land B)+P(\neg A)=1\\&\iff P(A\land B)=P(A)\\&\iff P(A)\cdot P(B|A)=P(A)\\&\iff P(B|A)=1\end{aligned}}} Bayesian hypothesis testing Bayes' theorem may be used to estimate the probability of a hypothesis or theory H, given some facts F. The posterior probability of H is then P ( H | F ) = P ( H ) P ( F | H ) P ( F ) {\displaystyle P(H|F)={\frac {P(H)P(F|H)}{P(F)}}} or in terms of information, P ( H | F ) = 2 − ( L ( H ) + L ( F | H ) − L ( F ) ) {\displaystyle P(H|F)=2^{-(L(H)+L(F|H)-L(F))}} By assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is L ( F | H ) . {\displaystyle L(F|H).} L ( H ) + L ( F | H ) {\displaystyle L(H)+L(F|H)} represents the amount of information needed to represent the facts F, if H is true. L ( F ) {\displaystyle L(F)} is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true. This is the evidence that the hypothesis H is true. If L ( F ) {\displaystyle L(F)} is estimated from encoding length then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory. If a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability P ( F ) {\displaystyle P(F)} . Set of hypothesis Probabilities may be calculated from the extended form of Bayes' theorem. Given all mutually exclusive hypothesis H i {\displaystyle H_{i}} which give evidence, such that, L ( H i ) + L ( F | H i ) < L ( F ) {\displaystyle L(H_{i})+L(F|H_{i})<L(F)} and also the hypothesis R, that none of the hypothesis is true, then, P ( H i | F ) = P ( H i ) P ( F | H i ) P ( F | R ) + ∑ j P ( H j ) P ( F | H j ) P ( R | F ) = P ( F | R ) P ( F | R ) + ∑ j P ( H j ) P ( F | H j ) {\displaystyle {\begin{aligned}P(H_{i}|F)&={\frac {P(H_{i})P(F|H_{i})}{P(F|R)+\sum _{j}{P(H_{j})P(F|H_{j})}}}\\[8pt]P(R|F)&={\frac {P(F|R)}{P(F|R)+\sum _{j}{P(H_{j})P(F|H_{j})}}}\end{aligned}}} In terms of information, P ( H i | F ) = 2 − ( L ( H i ) + L ( F | H i ) ) 2 − L ( F | R ) + ∑ j 2 − ( L ( H j ) + L ( F | H j ) ) P ( R | F ) = 2 − L ( F | R ) 2 − L ( F | R ) + ∑ j 2 − ( L ( H j ) + L ( F | H j ) ) {\displaystyle {\begin{aligned}P(H_{i}|F)&={\frac {2^{-(L(H_{i})+L(F|H_{i}))}}{2^{-L(F|R)}+\sum _{j}2^{-(L(H_{j})+L(F|H_{j}))}}}\\[8pt]P(R|F)&={\frac {2^{-L(F|R)}}{2^{-L(F|R)}+\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\end{aligned}}} In most situations it is a good approximation to assume that F {\displaystyle F} is independent of R {\displaystyle R} , which means P ( F | R ) = P ( F ) {\displaystyle P(F|R)=P(F)} giving, P ( H i | F ) ≈ 2 − ( L ( H i ) + L ( F | H i ) ) 2 − L ( F ) + ∑ j 2 − ( L ( H j ) + L ( F | H j ) ) P ( R | F ) ≈ 2 − L ( F ) 2 − L ( F ) + ∑ j 2 − ( L ( H j ) + L ( F | H j ) ) {\displaystyle {\begin{aligned}P(H_{i}|F)&\approx {\frac {2^{-(L(H_{i})+L(F|H_{i}))}}{2^{-L(F)}+\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\\[8pt]P(R|F)&\approx {\frac {2^{-L(F)}}{2^{-L(F)}+\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\end{aligned}}} Boolean inductive inference Abductive inference starts with a set of facts F which is a statement (Boolean expression). Abductive reasoning is of the form, A theory T implies the statement F. As the theory T is simpler than F, abduction says that there is a probability that the theory T is implied by F. The theory T, also called an explanation of the condition F, is an answer to t