 training phase. In the evaluation phase, answers of to the testing phase were evaluated in a supervised an unsupervised framework. The unsupervised evaluation for WSI considered two types of evaluation V Measure (Rosenberg and Hirschberg, 2007), and paired F-Score (Artiles et al., 2009). This evaluation follows the supervised evaluation of SemEval-2007 WSI task (Agirre and Soroa, 2007) Senseval and SemEval tasks overview The tables below reflects the workshop growth from Senseval to SemEval and gives an overview of which area of computational semantics was evaluated throughout the Senseval/SemEval workshops. The Multilingual WSD task was introduced for the SemEval-2013 workshop. The task is aimed at evaluating Word Sense Disambiguation systems in a multilingual scenario using BabelNet as its sense inventory. Unlike similar task like crosslingual WSD or the multilingual lexical substitution task, where no fixed sense inventory is specified, Multilingual WSD uses the BabelNet as its sense inventory. Prior to the development of BabelNet, a bilingual lexical sample WSD evaluation task was carried out in SemEval-2007 on Chinese-English bitexts. The Cross-lingual WSD task was introduced in the SemEval-2007 evaluation workshop and re-proposed in the SemEval-2013 workshop . To facilitate the ease of integrating WSD systems into other Natural Language Processing (NLP) applications, such as Machine Translation and multilingual Information Retrieval, the cross-lingual WSD evaluation task was introduced a language-independent and knowledge-lean approach to WSD. The task is an unsupervised Word Sense Disambiguation task for English nouns by means of parallel corpora. It follows the lexical-sample variant of the Classic WSD task, restricted to only 20 polysemous nouns. It is worth noting that the SemEval-2014 have only two tasks that were multilingual/crosslingual, i.e. (i) the L2 Writing Assistant task, which is a crosslingual WSD task that includes English, Spanish, German, French and Dutch and (ii) the Multilingual Semantic Textual Similarity task that evaluates systems on English and Spanish texts. Areas of evaluation The major tasks in semantic evaluation include the following areas of natural language processing. This list is expected to grow as the field progresses. The following table shows the areas of studies that were involved in Senseval-1 through SemEval-2014 (S refers to Senseval and SE refers to SemEval, e.g. S1 refers to Senseval-1 and SE07 refers to SemEval2007): Type of Semantic Annotations SemEval tasks have created many types of semantic annotations, each type with various schema. In SemEval-2015, the organizers have decided to group tasks together into several tracks. These tracks are by the type of semantic annotations that the task hope to achieve. Here lists the type of semantic annotations involved in the SemEval workshops: Learning Semantic Relations Question and Answering Semantic Parsing Semantic Taxonomy Sentiment Analysis Text Similarity Time and Space Word Sense Disambiguation and Induction A task and its track allocation is flexible; a task might develop into its own track, e.g. the taxonomy evaluation task in SemEval-2015 was under the Learning Semantic Relations track and in SemEval-2016, there is a dedicated track for Semantic Taxonomy with a new Semantic Taxonomy Enrichment task. See also List of computer science awards Computational semantics Natural language processing Word sense Word sense disambiguation Different variants of WSD evaluations Semantic analysis (computational) References External links Special Interest Group on the Lexicon (SIGLEX) of the Association for Computational Linguistics (ACL) Semeval-2010 – Semantic Evaluation Workshop (endorsed by SIGLEX) Senseval - international organization devoted to the evaluation of Word Sense Disambiguation Systems (endorsed by SIGLEX) SemEval Portal on the Wiki of the Association for Computational Linguistics Senseval / SemEval tasks: Senseval-1 – the 