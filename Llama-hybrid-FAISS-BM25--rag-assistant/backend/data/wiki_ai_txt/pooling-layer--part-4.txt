T, MPNN). The Self-attention pooling layer can then be formalised as follows: X ′ = ( X ⊙ y ) i {\displaystyle \mathbf {X} '=(\mathbf {X} \odot \mathbf {y} )_{\mathbf {i} }} A ′ = A i , i {\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} ,\mathbf {i} }} where i = top k ( y ) {\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores, ⊙ {\displaystyle \odot } denotes element-wise matrix multiplication. The self-attention pooling layer can be seen as an extension of the top-k pooling layer. Differently from top-k pooling, the self-attention scores computed in self-attention pooling account both for the graph features and the graph topology. History In early 20th century, neuroanatomists noticed a certain motif where multiple neurons synapse to the same neuron. This was given a functional explanation as "local pooling", which makes vision translation-invariant. (Hartline, 1940) gave supporting evidence for the theory by electrophysiological experiments on the receptive fields of retinal ganglion cells. The Hubel and Wiesel experiments showed that the vision system in cats is similar to a convolutional neural network, with some cells summing over inputs from the lower layer. See (Westheimer, 1965) for citations to these early literature. During the 1970s, to explain the effects of depth perception, some such as (Julesz and Chang, 1976) proposed that the vision system implements a disparity-selective mechanism by global pooling, where the outputs from matching pairs of retinal regions in the two eyes are pooled in higher order cells. See for citations to these early literature. In artificial neural networks, max pooling was used in 1990 for speech processing (1-dimensional convolution), and for image processing, was first used in the Cresceptron of 1992. See also Convolutional neural network Subsampling Image scaling Feature extraction Region of interest Graph neural network == References ==