N applied once per channel group. Suppose at a layer l {\displaystyle l} , there are channels 1 , 2 , … , C {\displaystyle 1,2,\dots ,C} , then it is partitioned into groups g 1 , g 2 , … , g G {\displaystyle g_{1},g_{2},\dots ,g_{G}} . Then, LayerNorm is applied to each group. Instance normalization Instance normalization (InstanceNorm), or contrast normalization, is a technique first developed for neural style transfer, and is also only used for CNNs. It can be understood as the LayerNorm for CNN applied once per channel, or equivalently, as group normalization where each group consists of a single channel: μ c ( l ) = 1 H W ∑ h = 1 H ∑ w = 1 W x h , w , c ( l ) ( σ c ( l ) ) 2 = 1 H W ∑ h = 1 H ∑ w = 1 W ( x h , w , c ( l ) − μ c ( l ) ) 2 x ^ h , w , c ( l ) = x ^ h , w , c ( l ) − μ c ( l ) ( σ c ( l ) ) 2 + ϵ y h , w , c ( l ) = γ c ( l ) x ^ h , w , c ( l ) + β c ( l ) {\displaystyle {\begin{aligned}\mu _{c}^{(l)}&={\frac {1}{HW}}\sum _{h=1}^{H}\sum _{w=1}^{W}x_{h,w,c}^{(l)}\\(\sigma _{c}^{(l)})^{2}&={\frac {1}{HW}}\sum _{h=1}^{H}\sum _{w=1}^{W}(x_{h,w,c}^{(l)}-\mu _{c}^{(l)})^{2}\\{\hat {x}}_{h,w,c}^{(l)}&={\frac {{\hat {x}}_{h,w,c}^{(l)}-\mu _{c}^{(l)}}{\sqrt {(\sigma _{c}^{(l)})^{2}+\epsilon }}}\\y_{h,w,c}^{(l)}&=\gamma _{c}^{(l)}{\hat {x}}_{h,w,c}^{(l)}+\beta _{c}^{(l)}\end{aligned}}} Adaptive instance normalization Adaptive instance normalization (AdaIN) is a variant of instance normalization, designed specifically for neural style transfer with CNNs, rather than just CNNs in general. In the AdaIN method of style transfer, we take a CNN and two input images, one for content and one for style. Each image is processed through the same CNN, and at a certain layer l {\displaystyle l} , AdaIn is applied. Let x ( l ) , content {\displaystyle x^{(l),{\text{ content}}}} be the activation in the content image, and x ( l ) , style {\displaystyle x^{(l),{\text{ style}}}} be the activation in the style image. Then, AdaIn first computes the mean and variance of the activations of the content image x ′ ( l ) {\displaystyle x'^{(l)}} , then uses those as the γ , β {\displaystyle \gamma ,\beta } for InstanceNorm on x ( l ) , content {\displaystyle x^{(l),{\text{ content}}}} . Note that x ( l ) , style {\displaystyle x^{(l),{\text{ style}}}} itself remains unchanged. Explicitly, we have: y h , w , c ( l ) , content = σ c ( l ) , style ( x h , w , c ( l ) , content − μ c ( l ) , content ( σ c ( l ) , content ) 2 + ϵ ) + μ c ( l ) , style {\displaystyle {\begin{aligned}y_{h,w,c}^{(l),{\text{ content}}}&=\sigma _{c}^{(l),{\text{ style}}}\left({\frac {x_{h,w,c}^{(l),{\text{ content}}}-\mu _{c}^{(l),{\text{ content}}}}{\sqrt {(\sigma _{c}^{(l),{\text{ content}}})^{2}+\epsilon }}}\right)+\mu _{c}^{(l),{\text{ style}}}\end{aligned}}} Transformers Some normalization methods were designed for use in transformers. The original 2017 transformer used the "post-LN" configuration for its LayerNorms. It was difficult to train, and required careful hyperparameter tuning and a "warm-up" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence. FixNorm and ScaleNorm both normalize activation vectors in a transformer. The FixNorm method divides the output vectors from a transformer by their L2 norms, then multiplies by a learned parameter g {\displaystyle g} . The ScaleNorm replaces all LayerNorms inside a transformer by division with L2 norm, then multiplying by a learned parameter g ′ {\displaystyle g'} (shared by all ScaleNorm modules of a transformer). Query-Key normalization (QKNorm) normalizes query and key vectors to have unit L2 norm. In nGPT, many vectors are normalized to have unit L2 norm: hidden state vectors, input and output embedding vectors, weight matrix columns, and query and key vectors. Miscellaneous Gradient normalization (GradNorm) normalizes gradient vectors during backpropagation. 