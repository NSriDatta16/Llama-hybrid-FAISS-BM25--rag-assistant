ngle Market. They are specifically referred to in the EU's AI Act (which came into force in 2024), which "is widely seen as a clear indication of the EU legislator's intention that the exception covers AI data collection", a view that was also endorsed in a 2024 German court decision. Unlike the TDM exception for scientific research, the more general exception covering commercial AI only applies if the copyright holder has not opted out. In order to facilitate the opt-out to the TDM exception, the EU's AI Act of 2024 requires providers of "general-purpose" AI models to implement a policy to comply with EU law (including the TDM exception opt-out) and to publish a detailed summary of training content according to a template provided by the AI Office. These provisions will come into force in August 2025, with further clarification on exactly what will be required to providers of general-purpose AI models expected to come from a Code of Practice to be released in advance of this. UK Unlike the EU, the United Kingdom prohibits data mining for commercial purposes but has proposed this should be changed to support the development of AI: "For text and data mining, we plan to introduce a new copyright and database exception which allows TDM for any purpose. Rights holders will still have safeguards to protect their content, including a requirement for lawful access." India Indian copyright law provides fair use exceptions for scientific research, but lacks specific provisions for commercial AI training models. Unlike the EU and UK, India has not established TDM provisions that explicitly address commercial AI systems. This regulatory uncertainty became apparent in 2024 when Asian News International (ANI) sued OpenAI for using its content to train AI models without authorization. While OpenAI offered an opt-out policy that ANI used in October 2024 to block AI scrapers, ANI claimed this measure was ineffective since their content remained available through content syndication. The case also highlighted jurisdictional challenges, as OpenAI argued it was not subject to Indian law because its servers and training operations were located outside the country. Copyright infringing AI outputs In some cases, deep learning models may replicate items in their training set when generating output. This behaviour is generally considered an undesired overfitting of a model by AI developers, and has in previous generations of AI been considered a manageable problem. Memorization is the emergent phenomenon of LLMs to repeat long strings of training data, and it is no longer related to overfitting. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. This is potentially a security risk and a copyright risk, for both users and providers. As of August 2023, major consumer LLMs have attempted to mitigate these problems, but researchers have still been able to prompt leakage of copyrighted material. Under U.S. law, to prove that an AI output infringes a copyright, a plaintiff must show the copyrighted work was "actually copied", meaning that the AI generates output which is "substantially similar" to their work, and that the AI had access to their work. In the course of learning to statistically model the data on which they are trained, deep generative AI models may learn to imitate the distinct style of particular authors in the training set. Since fictional characters enjoy some copyright protection in the U.S. and other jurisdictions, an AI may also produce infringing content in the form of novel works which incorporate fictional characters. A generative image model such as Stable Diffusion is able to model the stylistic characteristics of an artist like Pablo Picasso (including his particular brush strokes, use of colour, perspective, and so on), and a user can engineer a prompt such as "an astronaut riding a horse, by Picasso" t