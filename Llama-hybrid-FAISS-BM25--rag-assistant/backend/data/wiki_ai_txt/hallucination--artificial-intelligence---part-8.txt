vailable to validate LLM-generated responses (or the responses are already based on external data as in RAG), model uncertainty estimation techniques from machine learning may be applied to detect hallucinations. Another proposal includes a two-phase framework that detects hallucinations in LLM-generated content via unsupervised screening and LLM validation. According to Luo et al., the previous methods fall into knowledge- and retrieval-based approaches, which ground LLM responses in factual data using external knowledge sources, such as path grounding. Luo et al. also mention training or reference guiding for language models, involving strategies like employing control codes or contrastive learning to guide the generation process to differentiate between correct and hallucinated content. Another category is evaluation and mitigation focused on specific hallucination types, such as employing methods to evaluate quantity entity in summarization and methods to detect and mitigate self-contradictory statements. Nvidia Guardrails, launched in 2023, can be configured to hard-code certain responses via script instead of leaving them to the LLM. Furthermore, numerous tools like SelfCheckGPT, the Trustworthy Language Model, and Aimon have emerged to aid in the detection of hallucination in offline experimentation and real-time production scenarios. Evaluating multiple possible replies before answering a query by assigning confidence scores to each could mitigate the problem. However, this approach would multiply computational costs. Active learning would further increase these costs. In high-stakes domains such as chip design, supply chain logistics, and medical diagnostics, the added costs are operationally necessary and therefore economically viable. In chatbots, however, customers tend to prefer rapid, overconfident answers over cautious, uncertainty-aware ones. See also Notes == References ==