ymplecticity. Partial differential equations A number of probabilistic numerical methods have also been proposed for partial differential equations. As with ordinary differential equations, the approaches can broadly be divided into those based on randomisation, generally of some underlying finite-element mesh and those based on Gaussian process regression. Probabilistic numerical PDE solvers based on Gaussian process regression recover classical methods on linear PDEs for certain priors, in particular methods of mean weighted residuals, which include Galerkin methods, finite element methods, as well as spectral methods. History and related fields The interplay between numerical analysis and probability is touched upon by a number of other areas of mathematics, including average-case analysis of numerical methods, information-based complexity, game theory, and statistical decision theory. Precursors to what is now being called "probabilistic numerics" can be found as early as the late 19th and early 20th century. The origins of probabilistic numerics can be traced to a discussion of probabilistic approaches to polynomial interpolation by Henri Poincaré in his Calcul des Probabilités. In modern terminology, Poincaré considered a Gaussian prior distribution on a function f : R → R {\displaystyle f\colon \mathbb {R} \to \mathbb {R} } , expressed as a formal power series with random coefficients, and asked for "probable values" of f ( x ) {\displaystyle f(x)} given this prior and n ∈ N {\displaystyle n\in \mathbb {N} } observations f ( a i ) = B i {\displaystyle f(a_{i})=B_{i}} for i = 1 , … , n {\displaystyle i=1,\dots ,n} . A later seminal contribution to the interplay of numerical analysis and probability was provided by Albert Suldin in the context of univariate quadrature. The statistical problem considered by Suldin was the approximation of the definite integral ∫ u ( t ) d t {\displaystyle \textstyle \int u(t)\,\mathrm {d} t} of a function u : [ a , b ] → R {\displaystyle u\colon [a,b]\to \mathbb {R} } , under a Brownian motion prior on u {\displaystyle u} , given access to pointwise evaluation of u {\displaystyle u} at nodes t 1 , … , t n ∈ [ a , b ] {\displaystyle t_{1},\dots ,t_{n}\in [a,b]} . Suldin showed that, for given quadrature nodes, the quadrature rule with minimal mean squared error is the trapezoidal rule; furthermore, this minimal error is proportional to the sum of cubes of the inter-node spacings. As a result, one can see the trapezoidal rule with equally-spaced nodes as statistically optimal in some sense — an early example of the average-case analysis of a numerical method. Suldin's point of view was later extended by Mike Larkin. Note that Suldin's Brownian motion prior on the integrand u {\displaystyle u} is a Gaussian measure and that the operations of integration and of point wise evaluation of u {\displaystyle u} are both linear maps. Thus, the definite integral ∫ u ( t ) d t {\displaystyle \textstyle \int u(t)\,\mathrm {d} t} is a real-valued Gaussian random variable. In particular, after conditioning on the observed pointwise values of u {\displaystyle u} , it follows a normal distribution with mean equal to the trapezoidal rule and variance equal to 1 12 ∑ i = 2 n ( t i − t i − 1 ) 3 {\displaystyle \textstyle {\frac {1}{12}}\sum _{i=2}^{n}(t_{i}-t_{i-1})^{3}} . This viewpoint is very close to that of Bayesian quadrature, seeing the output of a quadrature method not just as a point estimate but as a probability distribution in its own right. As noted by Houman Owhadi and collaborators, interplays between numerical approximation and statistical inference can also be traced back to Palasti and Renyi, Sard, Kimeldorf and Wahba (on the correspondence between Bayesian estimation and spline smoothing/interpolation) and Larkin (on the correspondence between Gaussian process regression and numerical approximation). Although the approach of modelling a perfectly known function as a sample from a random proce