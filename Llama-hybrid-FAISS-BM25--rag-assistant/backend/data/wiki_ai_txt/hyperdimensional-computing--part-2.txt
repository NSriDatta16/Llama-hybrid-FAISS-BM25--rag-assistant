r is "nearly orthogonal" to SHAPE and CIRCLE. The components are recoverable from the vector (e.g., answer the question "is the shape a circle?"). Addition creates a vector that combines concepts. For example, adding "SHAPE is CIRCLE" to "COLOR is RED," creates a vector that represents a red circle. Permutation rearranges the vector elements. For example, permuting a three-dimensional vector with values labeled x, y and z, can interchange x to y, y to z, and z to x. Events represented by hypervectors A and B can be added, forming one vector, but that would sacrifice the event sequence. Combining addition with permutation preserves the order; the event sequence can be retrieved by reversing the operations. Bundling combines a set of elements in H as function ⊕ : H ×H → H. The input is two points in H and the output is a third point that is similar to both. History Vector symbolic architectures (VSA) provided a systematic approach to high-dimensional symbol representations to support operations such as establishing relationships. Early examples include holographic reduced representations, binary spatter codes, and matrix binding of additive terms. HD computing advanced these models, particularly emphasizing hardware efficiency. In 2015, Eric Weiss showed how to fully represent an image as a hypervector. A vector could contain information about all the objects in the image, including properties such as color, position, and size. In 2023, Abbas Rahimi et al., used HDC with neural networks to solve Raven's progressive matrices. In 2023, Mike Heddes et Al. under the supervision of Professors Givargis, Nicolau and Veidenbaum created a hyper-dimensional computing library that is built on top of PyTorch. Applications Image recognition HDC algorithms can replicate tasks long completed by deep neural networks, such as classifying images. Classifying an annotated set of handwritten digits uses an algorithm to analyze the features of each image, yielding a hypervector per image. The algorithm then adds the hypervectors for all labeled images of e.g., zero, to create a prototypical hypervector for the concept of zero and repeats this for the other digits. Classifying an unlabeled image involves creating a hypervector for it and comparing it to the reference hypervectors. This comparison identifies the digit that the new image most resembles. Given labeled example set S = { ( x i , y i ) } i = 1 N , where x i ∈ X and y i ∈ { c i } i = 1 K {\displaystyle S=\{(x_{i},y_{i})\}_{i=1}^{N},\ {\scriptstyle {\text{where}}}\ x_{i}\in X\ {\scriptstyle {\text{and}}}\ y_{i}\in \{c_{i}\}_{i=1}^{K}} is the class of a particular xi. Given query xq ∈ X the most similar prototype can be found with k ∗ = k ∈ 1 , . . . , K a r g m a x ρ ( ϕ ( x q ) , ϕ ( c k ) ) {\displaystyle k^{*}=_{k\in 1,...,K}^{argmax}\ \rho (\phi (x_{q}),\phi (c_{k}))} . The similarity metric ρ is typically the dot-product. Reasoning Hypervectors can also be used for reasoning. Raven's progressive matrices presents images of objects in a grid. One position in the grid is blank. The test is to choose from candidate images the one that best fits. A dictionary of hypervectors represents individual objects. Each hypervector represents an object concept with its attributes. For each test image a neural network generates a binary hypervector (values are +1 or −1) that is as close as possible to some set of dictionary hypervectors. The generated hypervector thus describes all the objects and their attributes in the image. Another algorithm creates probability distributions for the number of objects in each image and their characteristics. These probability distributions describe the likely characteristics of both the context and candidate images. They too are transformed into hypervectors, then algebra predicts the most likely candidate image to fill the slot. This approach achieved 88% accuracy on one problem set, beating neural network–only solutions that were 61% accurate. For 3-by-3 grids, t