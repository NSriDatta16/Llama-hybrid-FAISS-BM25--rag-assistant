 λ = 1 {\displaystyle \lambda =1} producing parallel learning to Monte Carlo RL algorithms. In neuroscience The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward. Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Subsequently, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning. The relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research. It has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning. See also PVLV Q-learning Rescorla–Wagner model State–action–reward–state–action (SARSA) Notes Works cited Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement Learning: An Introduction (2nd ed.). Cambridge, MA: MIT Press. Tesauro, Gerald (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343. S2CID 6023746. Further reading Meyn, S. P. (2007). Control Techniques for Complex Networks. Cambridge University Press. ISBN 978-0521884419. See final chapter and appendix. Sutton, R. S.; Barto, A. G. (1990). "Time Derivative Models of Pavlovian Reinforcement" (PDF). Learning and Computational Neuroscience: Foundations of Adaptive Networks: 497–537. Archived from the original (PDF) on 2017-03-30. Retrieved 2017-03-29. External links Connect Four TDGravity Applet Archived 2012-07-24 at the Wayback Machine (+ mobile phone version) – self-learned using TD-Leaf method (combination of TD-Lambda with shallow tree search) Self Learning Meta-Tic-Tac-Toe Archived 2014-03-19 at the Wayback Machine Example web app showing how temporal difference learning can be used to learn state evaluation constants for a minimax AI playing a simple board game. Reinforcement Learning Problem, document explaining how temporal difference learning can be used to speed up Q-learning TD-Simulator Temporal difference simulator for classical conditioning