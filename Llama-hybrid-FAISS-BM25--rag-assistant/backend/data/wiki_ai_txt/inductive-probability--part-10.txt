 − n + ∑ j : I ( K j ) < n 2 − I ( K j ) . {\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\frac {2^{-I(K_{i})}}{2^{-n}+\sum _{j:I(K_{j})<n}2^{-I(K_{j})}}}.} The probability that the source is random, or unpredictable is, P ( random ⁡ ( s ) | T n ( s ) = x ) = 2 − n 2 − n + ∑ j : I ( K j ) < n 2 − I ( K j ) . {\displaystyle P(\operatorname {random} (s)|T_{n}(s)=x)={\frac {2^{-n}}{2^{-n}+\sum _{j:I(K_{j})<n}2^{-I(K_{j})}}}.} A model for inductive inference A model of how worlds are constructed is used in determining the probabilities of theories, A random bit string is selected. A condition is constructed from the bit string. A world is constructed that is consistent with the condition. If w is the bit string then the world is created such that R ( w ) {\displaystyle R(w)} is true. An intelligent agent has some facts about the word, represented by the bit string c, which gives the condition, C = R ( c ) {\displaystyle C=R(c)} The set of bit strings identical with any condition x is E ( x ) {\displaystyle E(x)} . ∀ x , E ( x ) = { w : R ( w ) ≡ x } {\displaystyle \forall x,E(x)=\{w:R(w)\equiv x\}} A theory is a simpler condition that explains (or implies) C. The set of all such theories is called T, T ( C ) = { t : t → C } {\displaystyle T(C)=\{t:t\to C\}} Applying Bayes' theorem extended form of Bayes' theorem may be applied P ( A i | B ) = P ( B | A i ) P ( A i ) ∑ j P ( B | A j ) P ( A j ) , {\displaystyle P(A_{i}|B)={\frac {P(B|A_{i})\,P(A_{i})}{\sum _{j}P(B|A_{j})\,P(A_{j})}},} where, B = E ( C ) {\displaystyle B=E(C)} A i = E ( t ) {\displaystyle A_{i}=E(t)} To apply Bayes' theorem the following must hold: A i {\displaystyle A_{i}} is a partition of the event space. For T ( C ) {\displaystyle T(C)} to be a partition, no bit string n may belong to two theories. To prove this assume they can and derive a contradiction, ( N ∈ T ) ∧ ( N ∈ M ) ∧ ( N ≠ M ) ∧ ( n ∈ E ( N ) ∧ n ∈ E ( M ) ) {\displaystyle (N\in T)\land (N\in M)\land (N\neq M)\land (n\in E(N)\land n\in E(M))} ⟹ ( N ≠ M ) ∧ R ( n ) ≡ N ∧ R ( n ) ≡ M {\displaystyle \implies (N\neq M)\land R(n)\equiv N\land R(n)\equiv M} ⟹ ⊥ {\displaystyle \implies \bot } Secondly prove that T includes all outcomes consistent with the condition. As all theories consistent with C are included then R ( w ) {\displaystyle R(w)} must be in this set. So Bayes theorem may be applied as specified giving, ∀ t ∈ T ( C ) , P ( E ( t ) | E ( C ) ) = P ( E ( t ) ) ⋅ P ( E ( C ) | E ( t ) ) ∑ j ∈ T ( C ) P ( E ( j ) ) ⋅ P ( E ( C ) | E ( j ) ) {\displaystyle \forall t\in T(C),P(E(t)|E(C))={\frac {P(E(t))\cdot P(E(C)|E(t))}{\sum _{j\in T(C)}P(E(j))\cdot P(E(C)|E(j))}}} Using the implication and condition probability law, the definition of T ( C ) {\displaystyle T(C)} implies, ∀ t ∈ T ( C ) , P ( E ( C ) | E ( t ) ) = 1 {\displaystyle \forall t\in T(C),P(E(C)|E(t))=1} The probability of each theory in T is given by, ∀ t ∈ T ( C ) , P ( E ( t ) ) = ∑ n : R ( n ) ≡ t 2 − L ( n ) {\displaystyle \forall t\in T(C),P(E(t))=\sum _{n:R(n)\equiv t}2^{-L(n)}} so, ∀ t ∈ T ( C ) , P ( E ( t ) | E ( C ) ) = ∑ n : R ( n ) ≡ t 2 − L ( n ) ∑ j ∈ T ( C ) ∑ m : R ( m ) ≡ j 2 − L ( m ) {\displaystyle \forall t\in T(C),P(E(t)|E(C))={\frac {\sum _{n:R(n)\equiv t}2^{-L(n)}}{\sum _{j\in T(C)}\sum _{m:R(m)\equiv j}2^{-L(m)}}}} Finally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy, ∀ t ∈ T ( C ) , P ( E ( t ) | E ( C ) ) = P ( t | C ) {\displaystyle \forall t\in T(C),P(E(t)|E(C))=P(t|C)} giving ∀ t ∈ T ( C ) , P ( t | C ) = ∑ n : R ( n ) ≡ t 2 − L ( n ) ∑ j ∈ T ( C ) ∑ m : R ( m ) ≡ j 2 − L ( m ) {\displaystyle \forall t\in T(C),P(t|C)={\frac {\sum _{n:R(n)\equiv t}2^{-L(n)}}{\sum _{j\in T(C)}\sum _{m:R(m)\equiv j}2^{-L(m)}}}} This is the probability of the theory t after observing that the condition C holds. Removing theories without predictive power Theories that are less probable than the condition C have no predictive power. Separate them out gi