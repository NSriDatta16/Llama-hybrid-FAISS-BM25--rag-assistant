machine learning problem: where H {\displaystyle {\mathcal {H}}} is a vector valued reproducing kernel Hilbert space with functions f : X → Y T {\displaystyle f:{\mathcal {X}}\rightarrow {\mathcal {Y}}^{T}} having components f t : X → Y {\displaystyle f_{t}:{\mathcal {X}}\rightarrow {\mathcal {Y}}} . The reproducing kernel for the space H {\displaystyle {\mathcal {H}}} of functions f : X → R T {\displaystyle f:{\mathcal {X}}\rightarrow \mathbb {R} ^{T}} is a symmetric matrix-valued function Γ : X × X → R T × T {\displaystyle \Gamma :{\mathcal {X}}\times {\mathcal {X}}\rightarrow \mathbb {R} ^{T\times T}} , such that Γ ( ⋅ , x ) c ∈ H {\displaystyle \Gamma (\cdot ,x)c\in {\mathcal {H}}} and the following reproducing property holds: The reproducing kernel gives rise to a representer theorem showing that any solution to equation 1 has the form: Separable kernels The form of the kernel Γ induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a separable kernel, which factors into separate kernels on the input space X and on the tasks { 1 , . . . , T } {\displaystyle \{1,...,T\}} . In this case the kernel relating scalar components f t {\displaystyle f_{t}} and f s {\displaystyle f_{s}} is given by γ ( ( x i , t ) , ( x j , s ) ) = k ( x i , x j ) k T ( s , t ) = k ( x i , x j ) A s , t {\textstyle \gamma ((x_{i},t),(x_{j},s))=k(x_{i},x_{j})k_{T}(s,t)=k(x_{i},x_{j})A_{s,t}} . For vector valued functions f ∈ H {\displaystyle f\in {\mathcal {H}}} we can write Γ ( x i , x j ) = k ( x i , x j ) A {\displaystyle \Gamma (x_{i},x_{j})=k(x_{i},x_{j})A} , where k is a scalar reproducing kernel, and A is a symmetric positive semi-definite T × T {\displaystyle T\times T} matrix. Henceforth denote S + T = { PSD matrices } ⊂ R T × T {\displaystyle S_{+}^{T}=\{{\text{PSD matrices}}\}\subset \mathbb {R} ^{T\times T}} . This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by A. Methods for non-separable kernels Γ is a current field of research. For the separable case, the representation theorem is reduced to f ( x ) = ∑ i = 1 N k ( x , x i ) A c i {\textstyle f(x)=\sum _{i=1}^{N}k(x,x_{i})Ac_{i}} . The model output on the training data is then KCA , where K is the n × n {\displaystyle n\times n} empirical kernel matrix with entries K i , j = k ( x i , x j ) {\textstyle K_{i,j}=k(x_{i},x_{j})} , and C is the n × T {\displaystyle n\times T} matrix of rows c i {\displaystyle c_{i}} . With the separable kernel, equation 1 can be rewritten as where V is a (weighted) average of L applied entry-wise to Y and KCA. (The weight is zero if Y i t {\displaystyle Y_{i}^{t}} is a missing observation). Note the second term in P can be derived as follows: ‖ f ‖ H 2 = ⟨ ∑ i = 1 n k ( ⋅ , x i ) A c i , ∑ j = 1 n k ( ⋅ , x j ) A c j ⟩ H = ∑ i , j = 1 n ⟨ k ( ⋅ , x i ) A c i , k ( ⋅ , x j ) A c j ⟩ H (bilinearity) = ∑ i , j = 1 n ⟨ k ( x i , x j ) A c i , c j ⟩ R T (reproducing property) = ∑ i , j = 1 n k ( x i , x j ) c i ⊤ A c j = t r ( K C A C ⊤ ) {\displaystyle {\begin{aligned}\|f\|_{\mathcal {H}}^{2}&=\left\langle \sum _{i=1}^{n}k(\cdot ,x_{i})Ac_{i},\sum _{j=1}^{n}k(\cdot ,x_{j})Ac_{j}\right\rangle _{\mathcal {H}}\\&=\sum _{i,j=1}^{n}\langle k(\cdot ,x_{i})Ac_{i},k(\cdot ,x_{j})Ac_{j}\rangle _{\mathcal {H}}&{\text{(bilinearity)}}\\&=\sum _{i,j=1}^{n}\langle k(x_{i},x_{j})Ac_{i},c_{j}\rangle _{\mathbb {R} ^{T}}&{\text{(reproducing property)}}\\&=\sum _{i,j=1}^{n}k(x_{i},x_{j})c_{i}^{\top }Ac_{j}=tr(KCAC^{\top })\end{aligned}}} Known task structure Task structure representations There are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping. Task structure examples Via the regularizer formulation, one can represent a variety 