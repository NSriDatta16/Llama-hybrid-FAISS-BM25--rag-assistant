ing is useless in this case, since each data read is random and is not reused. hfreduce: Library for asynchronous communication, originally designed to replace Nvidia Collective Communication Library (NCCL). It is mainly used for allreduce, especially of gradients during backpropagation. It is asynchronously run on the CPU to avoid blocking kernels on the GPU. It uses two-tree broadcast like NCCL. hfai.nn: Software library of commonly used operators for neural network training, similar to torch.nn in PyTorch. HaiScale Distributed Data Parallel (DDP): Parallel training library that implements various forms of parallelism such as Data Parallelism (DP), Pipeline Parallelism (PP), Tensor Parallelism (TP), Experts Parallelism (EP), Fully Sharded Data Parallel (FSDP) and Zero Redundancy Optimizer (ZeRO). It is similar to PyTorch DDP, which uses NCCL on the backend. HAI Platform: Various applications such as task scheduling, fault handling, and disaster recovery. As of 2022, Fire-Flyer 2 had 5,000 PCIe A100 GPUs in 625 nodes, each containing 8 GPUs. It later incorporated NVLinks and NCCL to train larger models that required model parallelism. Development and release history The first DeepSeek models were essentially the same as Llama, which were dense decoder-only transformers. Later models incorporated the multi-head latent attention (MLA), Mixture of Experts (MoE), and KV caching. A decoder-only transformer consists of multiple identical decoder layers. Each of these layers features two main components: an attention layer and a feedforward network (FFN) layer. V2 replaced the standard multi-head attention mechanism (MHA) with multi-head latent attention (MLA). This introduces compressed latent vectors to reduce KV (keyâ€“value) cache size, and thus memory usage. A standard MoE Transformer generally use the sparsely-gated MoE layers in the FFN layers. In such an MoE layer, there are several FFN modules in parallel ("routed experts") and a small classifier ("gate") to compute a score for all these modules upon each token. Only the highest-scoring modules are activated. Starting with DeepSeekMoE, DeepSeek adopted a variant that adds "shared experts", which are always activated. Overview of models DeepSeek's models are "open weight", which provides less freedom for modification than true open source software. DeepSeek Coder DeepSeek Coder is a series of eight models, four pretrained (Base) and four instruction-finetuned (Instruct). All have 16K context lengths. The model was made source-available under the DeepSeek License, which includes "open and responsible downstream usage" restrictions. The training program was: Pretraining: 1.8T tokens (87% source code, 10% code-related English (GitHub markdown and Stack Exchange), and 3% code-unrelated Chinese). Long-context pretraining: 200B tokens. This extends the context length from 4K to 16K. This produced the Base models. Supervised finetuning (SFT): 2B tokens of instruction data. This produced the Instruct models. They were trained on clusters of A100 and H800 Nvidia GPUs, connected by InfiniBand, NVLink, NVSwitch. DeepSeek-LLM The DeepSeek-LLM series was released in November 2023. It has 7B and 67B parameters in both Base and Chat forms. DeepSeek's accompanying paper claimed benchmark results higher than Llama 2 and most open-source LLMs at the time. The model code is under the source-available DeepSeek License. The architecture was essentially the same as the Llama series. They used the pre-norm decoder-only Transformer with RMSNorm as the normalization, SwiGLU in the feedforward layers, rotary positional embedding (RoPE), and grouped-query attention (GQA). Both had vocabulary size 102,400 (byte-level BPE) and context length of 4096. They trained on 2 trillion tokens of English and Chinese text obtained by deduplicating the Common Crawl. The Chat versions of the two Base models was released concurrently, obtained by training Base by supervised finetuning (SFT) followed by direct policy op