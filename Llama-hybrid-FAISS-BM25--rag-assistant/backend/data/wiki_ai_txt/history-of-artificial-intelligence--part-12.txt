st databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required. In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut―the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. New directions in the 1980s Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as "connectionism", robotics, "soft" computing and reinforcement learning. Nils Nilsson called these approaches "sub-symbolic". Revival of neural networks: "connectionism" In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a "Hopfield net") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically. Geoffrey Hinton proved a similar result about a device called a "Boltzmann machine". (Hopfield and Hinton would eventually receive the 2024 Nobel prize for this work.) In 1986, Hinton and David Rumelhart popularized a method for training neural networks called "backpropagation". These three developments helped to revive the exploration of artificial neural networks. Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened "connectionism" and there was a considerable debate between advocates of symbolic AI and the "connectionists". Hinton called symbols the "luminous aether of AI"―that is, an unworkable and misleading model of intelligence. This was a direct attack on the principles that inspired the cognitive revolution. Neural networks started to advance state of the art in some specialist areas such as protein structure prediction. Following pioneering work from Terry Sejnowski, cascading multilayer perceptrons such as PhD and PsiPred reached near-theoretical maximum accuracy in predicting secondary structure. In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks. Robotics and embodied reason Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body—it needs to perceive, move, survive, and deal with the world. Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence "from the bottom up". A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.) In his 1990 paper "Elephants Don't Play Chess", ro