eighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) suggest the (maximum and minimum, respectively) Hausdorff metrics for bags A {\displaystyle A} and B {\displaystyle B} : H ( A , B ) = max { max A min B ‖ a − b ‖ , max B min A ‖ a − b ‖ } {\displaystyle H(A,B)=\max \left\{\max _{A}\min _{B}\|a-b\|,\max _{B}\min _{A}\|a-b\|\right\}} h 1 ( A , B ) = min A min B ‖ a − b ‖ {\displaystyle h_{1}(A,B)=\min _{A}\min _{B}\|a-b\|} They define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting. Generalizations So far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case. One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if X {\displaystyle {\mathcal {X}}} is the space of features and Y {\displaystyle {\mathcal {Y}}} is the space of labels, an MIML concept is a map c : N X → 2 Y {\displaystyle c:\mathbb {N} ^{\mathcal {X}}\rightarrow 2^{\mathcal {Y}}} . Zhou and Zhang (2006) propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem. Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the "prime instance", which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem. See also Supervised learning Multi-label classification References Further reading Recent reviews of the MIL literature include: Amores (2013), which provides an extensive review and comparative study of the different paradigms, Foulds & Frank (2010), which provides a thorough review of the different assumptions used by different paradigms in the literature. Dietterich, Thomas G; Lathrop, Richard H; Lozano-Pérez, Tomás (1997). "Solving the multiple instance problem with axis-parallel rectangles". Artificial Intelligence. 89 (1–2): 31–71. doi:10.1016/S0004-3702(96)00034-3. Herrera, Francisco; Ventura, Sebastián; Bello, Rafael; Cornelis, Chris; Zafra, Amelia; Sánchez-Tarragó, Dánel; Vluymans, Sarah (2016). Multiple Instance Learning. doi:10.1007/978-3-319-47759-6. ISBN 978-3-319-47758-9. S2CID 24047205. Amores, Jaume (2013). "Multiple instance classification: Review, taxonomy and comparative study". Artificial Intelligence. 201: 81–105. doi:10.1016/j.artint.2013.06.003. Foulds, James; Frank, Eibe (2010). "A review of multi-instance learning assumptions". The Knowledge Engineering Review. 25: 1–25. CiteSeerX 10.1.1.148.2333. doi:10.1017/S026988890999035X. S2CID 8601873. Keeler, James D.; Rumelhart, David E.; Leow, Wee-Kheng (1990). "Integrated segmentation and recognition of hand-printed numerals". Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems (NIPS 3). Morgan Kaufmann Publishers. pp. 557–563. ISBN 978-1-55860-184-0. Li, Hong-Dong; Menon, Rajasree; Omenn, Gilbert S; Guan, Yuanfang (2014). "The emerging era of ge