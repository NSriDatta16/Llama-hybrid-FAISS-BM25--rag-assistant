 model In practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size N = 1000 {\displaystyle N=1000} , first estimate a model Î¸ ~ {\displaystyle {\tilde {\theta }}} using N h = 500 {\displaystyle N_{h}=500} samples from weighted case control sampling, then collect another N h = 500 {\displaystyle N_{h}=500} samples using local case-control sampling. Larger or smaller sample size It is possible to control the sample size by multiplying the acceptance probability with a constant c {\displaystyle c} . For a larger sample size, pick c > 1 {\displaystyle c>1} and adjust the acceptance probability to min ( c a ( x i , y i ) , 1 ) {\displaystyle \min(ca(x_{i},y_{i}),1)} . For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling. Properties The algorithm has the following properties. When the pilot is consistent, the estimates using the samples from local case-control sampling is consistent even under model misspecification. If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with c > 1 {\displaystyle c>1} , the factor 2 is improved to 1 + 1 c {\displaystyle 1+{\frac {1}{c}}} . == References ==