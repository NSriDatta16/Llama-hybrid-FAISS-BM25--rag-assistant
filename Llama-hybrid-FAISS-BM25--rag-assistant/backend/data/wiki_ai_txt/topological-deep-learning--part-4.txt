 messages from neighboring cells are aggregated within each neighborhood. The function ⨁ {\displaystyle \bigoplus } aggregates these messages, allowing information to be exchanged effectively between adjacent cells within the same neighborhood. Third, Equation 3 outlines the process of combining messages from different neighborhoods. The function ⨂ {\displaystyle \bigotimes } aggregates messages across various neighborhoods, facilitating communication between cells that may not be directly connected but share common neighborhood relationships. Fourth, Equation 4 specifies how the aggregated messages influence the state of a cell in the next layer. Here, the function β {\displaystyle \beta } updates the state of cell x {\displaystyle x} based on its current state h x ( l ) {\displaystyle \mathbf {h} _{x}^{(l)}} and the aggregated message m x {\displaystyle m_{x}} obtained from neighboring cells. Non-message passing topological neural networks While the majority of TNNs follow the message passing paradigm from graph learning, several models have been suggested that do not follow this approach. For instance, Maggs et al. leverage geometric information from embedded simplicial complexes, i.e., simplicial complexes with high-dimensional features attached to their vertices.This offers interpretability and geometric consistency without relying on message passing. Furthermore, in a contrastive loss-based method was suggested to learn the simplicial representation. Learning on topological descriptors Motivated by the modular nature of deep neural networks, initial work in TDL drew inspiration from topological data analysis, and aimed to make the resulting descriptors amenable to integration into deep-learning models. This led to work defining new layers for deep neural networks. Pioneering work by Hofer et al., for instance, introduced a layer that permitted topological descriptors like persistence diagrams or persistence barcodes to be integrated into a deep neural network. This was achieved by means of end-to-end-trainable projection functions, permitting topological features to be used to solve shape classification tasks, for instance. Follow-up work expanded more on the theoretical properties of such descriptors and integrated them into the field of representation learning. Other such topological layers include layers based on extended persistent homology descriptors, persistence landscapes, or coordinate functions. In parallel, persistent homology also found applications in graph-learning tasks. Noteworthy examples include new algorithms for learning task-specific filtration functions for graph classification or node classification tasks. Applications TDL is rapidly finding new applications across different domains, including data compression, enhancing the expressivity and predictive performance of graph neural networks, action recognition, and trajectory prediction. == References ==