l achieve low loss on a test set of Python text. Suppose the model has parameter count N {\displaystyle N} , and after being finetuned on D F {\displaystyle D_{F}} Python tokens, it achieves some loss L {\displaystyle L} . We say that its "transferred token count" is D T {\displaystyle D_{T}} , if another model with the same N {\displaystyle N} achieves the same L {\displaystyle L} after training on D F + D T {\displaystyle D_{F}+D_{T}} Python tokens. They found D T = 1.9 e 4 ( D F ) .18 ( N ) .38 {\displaystyle D_{T}=1.9e4\left(D_{F}\right)^{.18}(N)^{.38}} for pretraining on English text, and D T = 2.1 e 5 ( D F ) .096 ( N ) .38 {\displaystyle D_{T}=2.1e5\left(D_{F}\right)^{.096}(N)^{.38}} for pretraining on English and non-Python code. Precision Kumar et al. study scaling laws for numerical precision in the training of language models. They train a family of language models with weights, activations, and KV cache in varying numerical precision in both integer and floating-point type to measure the effects on loss as a function of precision. For training, their scaling law accounts for lower precision by wrapping the effects of precision into an overall "effective parameter count" that governs loss scaling, using the parameterization N ↦ N eff ( P ) = N ( 1 − e − P / γ ) {\displaystyle N\mapsto N_{\text{eff}}(P)=N(1-e^{-P/\gamma })} . This illustrates how training in lower precision degrades performance by reducing the true capacity of the model in a manner that varies exponentially with bits. For inference, they find that extreme overtraining of language models past Chinchilla-optimality can lead to models being more sensitive to quantization, a standard technique for efficient deep learning. This is demonstrated by observing that the degradation in loss due to weight quantization increases as an approximate power law in the token/parameter ratio D / N {\displaystyle D/N} seen during pretraining, so that models pretrained on extreme token budgets can perform worse in terms of validation loss than those trained on more modest token budgets if post-training quantization is applied. Other work examining the effects of overtraining include Sardana et al. and Gadre et al. Densing laws Xiao et al. considered the parameter efficiency ("density") of models over time. The idea is that over time, researchers would discover models that use their parameters more efficiently, in that models with the same performance can have fewer parameters. A model can have an actual parameter count N {\displaystyle N} , defined as the actual number of parameters in the model, and an "effective" parameter count N ^ {\displaystyle {\hat {N}}} , defined as how many parameters it would have taken a previous well-known model to reach he same performance on some benchmarks, such as MMLU. N ^ {\displaystyle {\hat {N}}} is not measured directly, but rather by measuring the actual model performance S {\displaystyle S} , then plugging it back to a previously fitted scaling law, such as the Chinchilla scaling law, to obtain what N ^ {\displaystyle {\hat {N}}} would be required to reach that performance S {\displaystyle S} , according to that previously fitted scaling laws. A densing law states that ln ⁡ ( N ^ N ) m a x = A t + B {\displaystyle \ln \left({\frac {\hat {N}}{N}}\right)_{max}=At+B} , where t {\displaystyle t} is real-world time, measured in days. See also == References ==