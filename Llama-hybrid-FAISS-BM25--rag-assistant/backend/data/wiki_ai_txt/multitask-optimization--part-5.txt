of task structures easily. Letting A † = γ I T + ( γ − λ ) 1 T 1 1 ⊤ {\textstyle A^{\dagger }=\gamma I_{T}+(\gamma -\lambda ){\frac {1}{T}}\mathbf {1} \mathbf {1} ^{\top }} (where I T {\displaystyle I_{T}} is the TxT identity matrix, and 1 1 ⊤ {\textstyle \mathbf {1} \mathbf {1} ^{\top }} is the TxT matrix of ones) is equivalent to letting Γ control the variance ∑ t | | f t − f ¯ | | H k {\textstyle \sum _{t}||f_{t}-{\bar {f}}||_{{\mathcal {H}}_{k}}} of tasks from their mean 1 T ∑ t f t {\textstyle {\frac {1}{T}}\sum _{t}f_{t}} . For example, blood levels of some biomarker may be taken on T patients at n t {\displaystyle n_{t}} time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients. Letting A † = α I T + ( α − λ ) M {\displaystyle A^{\dagger }=\alpha I_{T}+(\alpha -\lambda )M} , where M t , s = 1 | G r | I ( t , s ∈ G r ) {\displaystyle M_{t,s}={\frac {1}{|G_{r}|}}\mathbb {I} (t,s\in G_{r})} is equivalent to letting α {\displaystyle \alpha } control the variance measured with respect to a group mean: ∑ r ∑ t ∈ G r | | f t − 1 | G r | ∑ s ∈ G r ) f s | | {\displaystyle \sum _{r}\sum _{t\in G_{r}}||f_{t}-{\frac {1}{|G_{r}|}}\sum _{s\in G_{r})}f_{s}||} . (Here | G r | {\displaystyle |G_{r}|} the cardinality of group r, and I {\displaystyle \mathbb {I} } is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group. Letting A † = δ I T + ( δ − λ ) L {\displaystyle A^{\dagger }=\delta I_{T}+(\delta -\lambda )L} , where L = D − M {\displaystyle L=D-M} is the Laplacian for the graph with adjacency matrix M giving pairwise similarities of tasks. This is equivalent to giving a larger penalty to the distance separating tasks t and s when they are more similar (according to the weight M t , s {\displaystyle M_{t,s}} ,) i.e. δ {\displaystyle \delta } regularizes ∑ t , s | | f t − f s | | H k 2 M t , s {\displaystyle \sum _{t,s}||f_{t}-f_{s}||_{{\mathcal {H}}_{k}}^{2}M_{t,s}} . All of the above choices of A also induce the additional regularization term λ ∑ t | | f | | H k 2 {\textstyle \lambda \sum _{t}||f||_{{\mathcal {H}}_{k}}^{2}} which penalizes complexity in f more broadly. Learning tasks together with their structure Learning problem P can be generalized to admit learning task matrix A as follows: Choice of F : S + T → R + {\displaystyle F:S_{+}^{T}\rightarrow \mathbb {R} _{+}} must be designed to learn matrices A of a given type. See "Special cases" below. Optimization of Q Restricting to the case of convex losses and coercive penalties Ciliberto et al. have shown that although Q is not convex jointly in C and A, a related problem is jointly convex. Specifically on the convex set C = { ( C , A ) ∈ R n × T × S + T | R a n g e ( C ⊤ K C ) ⊆ R a n g e ( A ) } {\displaystyle {\mathcal {C}}=\{(C,A)\in \mathbb {R} ^{n\times T}\times S_{+}^{T}|Range(C^{\top }KC)\subseteq Range(A)\}} , the equivalent problem is convex with the same minimum value. And if ( C R , A R ) {\displaystyle (C_{R},A_{R})} is a minimizer for R then ( C R A R † , A R ) {\displaystyle (C_{R}A_{R}^{\dagger },A_{R})} is a minimizer for Q. R may be solved by a barrier method on a closed set by introducing the following perturbation: The perturbation via the barrier δ 2 t r ( A † ) {\displaystyle \delta ^{2}tr(A^{\dagger })} forces the objective functions to be equal to + ∞ {\displaystyle +\infty } on the boundary of R n × T × S + T {\displaystyle R^{n\times T}\times S_{+}^{T}} . S can be solved with a block coordinate descent method, alternating in C and A. This results in a sequence of minimizers ( C m , A m ) {\displaystyle (C_{m},A_{m})} in S that converges to the solution in R as δ m → 0 {\displaystyle \delta _{m}\rightarrow 0} , and hence gives the solution to Q. Special cases Spectral penalties -