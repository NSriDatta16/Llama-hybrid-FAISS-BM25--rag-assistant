mples multiple continuations until the end, starting at each reasoning step y i {\displaystyle y_{i}} , and set the reward at that step to be either # (correct answers) # (total answers) {\displaystyle {\frac {\#{\text{(correct answers)}}}{\#{\text{(total answers)}}}}} in the case of "soft estimation", or { 1 if one of the answers is correct 0 else {\displaystyle {\begin{cases}1&{\text{if one of the answers is correct}}\\0&{\text{else}}\end{cases}}} in the case of "hard estimation". This creates process rewards from an ORM, which is often easier or cheaper to construct. A PRM can then be trained on these labels. Some work has tried a fully MCTS approach. One can also use an ORM to implicitly construct a PRM, similar to direct preference optimization. Guided sampling A trained ORM can be used to pick the best response. The policy generates several responses, and the ORM selects the best one. This implements a simple form of test-time compute scaling ("best-of-N"). A trained PRM can guide reasoning by a greedy tree search: the policy proposes several next steps, the PRM picks one, and the process repeats. This mirrors using an ORM to pick a whole response. Beam search performs better than greedy search. Lookahead search is another tree search method. The policy proposes several next steps, then makes a short rollout for each. If a solution is found during rollout, the search stops early. Otherwise, the PRM scores each rollout, and the step with the highest score is chosen. Self-consistency can be combined with an ORM. The model generates multiple answers, and the answers are clustered so that each cluster has the same final answer. The ORM scores each answer, scores in each cluster are summed, and the answer from the highest-scoring cluster is returned. Benchmarks Reasoning models generally achieve higher scores than non-reasoning models on many benchmarks, particularly on tasks requiring multi-step reasoning. The Humanity's Last Exam (HLE) benchmark evaluates expert-level reasoning across mathematics, humanities, and natural sciences, revealing significant performance gaps between models. Current state-of-the-art reasoning models achieve relatively low scores on HLE, indicating substantial room for improvement. For example, the full reasoning model o3 achieved 26.6%, while the lighter o3-mini-high (on text-only questions) achieved 13%. On the American Invitational Mathematics Examination (AIME), a challenging mathematics competition, non-reasoning models typically solve fewer than 30% of problems. In contrast, models employing reasoning methods achieve success rates between 50% and 80%. While OpenAI's o1 maintained or slightly improved its accuracy from reported 2024 results to 2025 AIME results, o3-mini-high achieved 80% accuracy at significantly lower cost, approximately 12 times cheaper. Some minority or independent benchmarks exclude reasoning models due to their longer response times and higher inference costs, including benchmarks for online complex event detection in cyber-physical systems, general inference-time compute evaluation, Verilog engineering tasks, and network security assessments. Models See also Automated reasoning Reflection (artificial intelligence) Large language model References External links Fortes, Armando (2025-01-27). "atfortes/Awesome-LLM-Reasoning". GitHub. Retrieved 2025-01-27. Huang, Jie; Chang, Kevin Chen-Chuan (2023-05-26). "Towards Reasoning in Large Language Models: A Survey". arXiv:2212.10403 [cs.CL]. Besta, Maciej; Barth, Julia; Schreiber, Eric; Kubicek, Ales; Catarino, Afonso; Gerstenberger, Robert; Nyczyk, Piotr; Iff, Patrick; Li, Yueling (2025-01-23). "Reasoning Language Models: A Blueprint". arXiv:2501.11223 [cs.AI].