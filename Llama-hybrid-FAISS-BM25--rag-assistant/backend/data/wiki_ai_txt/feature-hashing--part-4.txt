 there exist pairs of different tokens with the same hash: t ≠ t ′ , ϕ ( t ) = ϕ ( t ′ ) = v {\displaystyle t\neq t',\phi (t)=\phi (t')=v} . A machine learning model trained on feature-hashed words would then have difficulty distinguishing t {\displaystyle t} and t ′ {\displaystyle t'} , essentially because v {\displaystyle v} is polysemic. If t ′ {\displaystyle t'} is rare, then performance degradation is small, as the model could always just ignore the rare case, and pretend all v {\displaystyle v} means t {\displaystyle t} . However, if both are common, then the degradation can be serious. To handle this, one can train supervised hashing functions that avoids mapping common tokens to the same feature vectors. Applications and practical performance Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function. Weinberger et al. (2009) applied their version of feature hashing to multi-task learning, and in particular, spam filtering, where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up. Chen et al. (2015) combined the idea of feature hashing and sparse matrix to construct "virtual matrices": large matrices with small storage requirements. The idea is to treat a matrix M ∈ R n × n {\displaystyle M\in \mathbb {R} ^{n\times n}} as a dictionary, with keys in n × n {\displaystyle n\times n} , and values in R {\displaystyle \mathbb {R} } . Then, as usual in hashed dictionaries, one can use a hash function h : N × N → m {\displaystyle h:\mathbb {N} \times \mathbb {N} \to m} , and thus represent a matrix as a vector in R m {\displaystyle \mathbb {R} ^{m}} , no matter how big n {\displaystyle n} is. With virtual matrices, they constructed HashedNets, which are large neural networks taking only small amounts of storage. Implementations Implementations of the hashing trick are present in: Apache Mahout Gensim scikit-learn sofia-ml Vowpal Wabbit Apache Spark R TensorFlow Dask-ML See also Bloom filter – Data structure for approximate set membership Count–min sketch – Probabilistic data structure in computer science Heaps' law – Heuristic for distinct words in a document Locality-sensitive hashing – Algorithmic technique using hashing MinHash – Data mining technique References External links Hashing Representations for Machine Learning on John Langford's website What is the "hashing trick"? - MetaOptimize Q+A