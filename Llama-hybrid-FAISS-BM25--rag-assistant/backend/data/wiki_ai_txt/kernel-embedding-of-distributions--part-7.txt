e from the chain rule: C Y X π = ( C X ∣ Y C Y Y π ) T . {\displaystyle {\mathcal {C}}_{YX}^{\pi }=\left({\mathcal {C}}_{X\mid Y}{\mathcal {C}}_{YY}^{\pi }\right)^{T}.} In practical implementations, the kernel Bayes' rule takes the following form μ ^ Y ∣ x π = C ^ Y X π ( ( C ^ X X ) 2 + λ ~ I ) − 1 C ^ X X π φ ( x ) = Φ ~ Λ T ( ( D K ) 2 + λ ~ I ) − 1 K D K x {\displaystyle {\widehat {\mu }}_{Y\mid x}^{\pi }={\widehat {\mathcal {C}}}_{YX}^{\pi }\left(\left({\widehat {\mathcal {C}}}_{XX}\right)^{2}+{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widehat {\mathcal {C}}}_{XX}^{\pi }\varphi (x)={\widetilde {\boldsymbol {\Phi }}}{\boldsymbol {\Lambda }}^{T}\left((\mathbf {D} \mathbf {K} )^{2}+{\widetilde {\lambda }}\mathbf {I} \right)^{-1}\mathbf {K} \mathbf {D} \mathbf {K} _{x}} where Λ = ( G + λ ~ I ) − 1 G ~ diag ⁡ ( α ) , D = diag ⁡ ( ( G + λ ~ I ) − 1 G ~ α ) . {\displaystyle {\boldsymbol {\Lambda }}=\left(\mathbf {G} +{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widetilde {\mathbf {G} }}\operatorname {diag} ({\boldsymbol {\alpha }}),\qquad \mathbf {D} =\operatorname {diag} \left(\left(\mathbf {G} +{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widetilde {\mathbf {G} }}{\boldsymbol {\alpha }}\right).} Two regularization parameters are used in this framework: λ {\displaystyle \lambda } for the estimation of C ^ Y X π , C ^ X X π = Υ D Υ T {\displaystyle {\widehat {\mathcal {C}}}_{YX}^{\pi },{\widehat {\mathcal {C}}}_{XX}^{\pi }={\boldsymbol {\Upsilon }}\mathbf {D} {\boldsymbol {\Upsilon }}^{T}} and λ ~ {\displaystyle {\widetilde {\lambda }}} for the estimation of the final conditional embedding operator C ^ Y ∣ X π = C ^ Y X π ( ( C ^ X X π ) 2 + λ ~ I ) − 1 C ^ X X π . {\displaystyle {\widehat {\mathcal {C}}}_{Y\mid X}^{\pi }={\widehat {\mathcal {C}}}_{YX}^{\pi }\left(\left({\widehat {\mathcal {C}}}_{XX}^{\pi }\right)^{2}+{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widehat {\mathcal {C}}}_{XX}^{\pi }.} The latter regularization is done on square of C ^ X X π {\displaystyle {\widehat {\mathcal {C}}}_{XX}^{\pi }} because D {\displaystyle D} may not be positive definite. Applications Measuring distance between distributions The maximum mean discrepancy (MMD) is a distance-measure between distributions P ( X ) {\displaystyle P(X)} and Q ( Y ) {\displaystyle Q(Y)} which is defined as the distance between their embeddings in the RKHS MMD ( P , Q ) = ‖ μ X − μ Y ‖ H . {\displaystyle {\text{MMD}}(P,Q)=\left\|\mu _{X}-\mu _{Y}\right\|_{\mathcal {H}}.} While most distance-measures between distributions such as the widely used Kullback–Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies, the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the maximum mean discrepancy refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions MMD ( P , Q ) = sup ‖ f ‖ H ≤ 1 ( E [ f ( X ) ] − E [ f ( Y ) ] ) , {\displaystyle {\text{MMD}}(P,Q)=\sup _{\|f\|_{\mathcal {H}}\leq 1}\left(\mathbb {E} [f(X)]-\mathbb {E} [f(Y)]\right),} a form of integral probability metric. Kernel two-sample test Given n training examples from P ( X ) {\displaystyle P(X)} and m samples from Q ( Y ) {\displaystyle Q(Y)} , one can formulate a test statistic based on the empirical estimate of the MMD MMD ^ ( P , Q ) = ‖ 1 n ∑ i = 1 n φ ( x i ) − 1 m ∑ i = 1 m φ ( y i ) ‖ H 2 = 1 n 2 ∑ i = 1 n ∑ j = 1 n k ( x i , x j ) + 1 m 2 ∑ i = 1 m ∑ j = 1 m k ( y i , y j ) − 2 n m ∑ i = 1 n ∑ j = 1 m k ( x i , y j ) {\displaystyle {\begin{aligned}{\widehat {\text{MMD}}}(P,Q)&=\left\|{\frac {1}{n}}\sum _{i=1}^{n}\varphi (x_{i})-{\frac {1}{m}}\sum _{i=1}^{m}\varphi (y_{i})\right\|_{\mathcal {H}}^{2}\\[5pt]&={\frac {1}{n^{2}}}\sum _{i=1}^{n}\sum _{j=1}^{n}k(x_{i},x_{j})+{\frac {1}{m^{2}}}