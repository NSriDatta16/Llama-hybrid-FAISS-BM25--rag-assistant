ith respect to the feature map A k {\displaystyle ^{k}} as ∂ y C ∂ A k ( i , j ) {\displaystyle {\frac {\partial {y^{C}}}{\partial {A^{k}}(i,j)}}} highlighting the importance of a certain class discrimination decision process of the logit. These gradients are global-average-pooled over each element of the feature map (hence, highlighting the "importance" of the elements of a feature map k for a target class C): α k C = 1 u v ∑ i ∑ j ∂ y C ∂ A k ( i , j ) {\displaystyle \alpha _{k}^{C}={\frac {1}{uv}}\sum _{i}\sum _{j}{\frac {\partial {y^{C}}}{\partial {A^{k}}(i,j)}}} So, to account for the total number of feature maps, each of them is multiplied by its weight (via dot-product) and element-wise summation is done: ∑ k α k C A k {\displaystyle \sum _{k}\alpha _{k}^{C}A^{k}} It can be observed that, due to the intrinsic nature of the gradient operation, some elements of the weighted feature map will have negative value, so, since only elements that have increased the logit of the predicted class are of interest, a ReLU activation function is applied: L G r a d − C A M C ( x , y ) = R e L U ( ∑ k α k C A k ( x , y ) ) {\displaystyle L_{Grad-CAM}^{C}(x,y)=ReLU(\sum _{k}\alpha _{k}^{C}A^{k}(x,y))} Lastly, the output heatmap image dimensions are upsampled to the original image size to match the input dimensions. Advantages and drawbacks Grad-CAM addresses the most important CAM limitations. It makes CAM free from the GAP layer need, generalizing its behavior and enabling visual explanation at intermediate layers. However, Grad-CAM focuses on the most discriminative region when contributing to classification. If multiple similar objects are present, Grad-CAM often highlights only one of them, or part of one, providing also coarser maps and lower localization accuracy. Moreover, Grad-CAM retrieves backwards information (the gradients), without taking into consideration how the activation flowed forward during prediction (unless combined with the guided backpropagation technique), resulting in a certain probability of missing patterns highlighted in the forward signal. On top of that, Grad-CAM heat maps are low-resolution when choosing a very deep layer. Lastly, false emphasis in the heatmap may be present when large gradients are computed for low activation values. Grad-CAM assumes that gradient implies importance, ignoring the activation features value. Grad-CAM and CAM comparison Fine-tuned versions Several methods have refined Grad-CAM to improve clarity and flexibility. Guided Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM enhance aspects such as localization accuracy, gradient independence, and multi-layer visualization. These techniques build directly on the principles of CAM and Grad-CAM. Guided Grad-CAM Guided Grad-CAM fuses the coarse, class‐discriminative localization of Grad-CAM with the high‐resolution details of guided backpropagation. Grad-CAM heatmap is first computed for the target class, and it is upsampled to the input size. Then, a Guided Backpropagation saliency map for the same class is computed. A final element‐wise product of the two results in the Guided Grad-CAM visualization map. The result is a high-resolution, class-specific saliency map that highlights exactly which pixels contribute most to the network's decision. Grad-CAM++ Grad-CAM++ introduces a more refined way of computing the weights for each feature map, bypassing the global average of the gradients approach provided by Grad-CAM. This approach aims to improve the visual effect when multiple target instances are present in a single image. Specifically, Grad-CAM++ employs pixel-wise gradients (via higher-order gradients), to compute the importance of a specific pixel for a prediction, lighting up multiple object instances in the same image. These improvements allow for a more sensible and detailed output heatmap. The associated mathematical framework is defined by the following localization map: L G r a d − C A M + + C ( x , y ) = ∑ k w k C A k ( x , y