 the AI age: 1. Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback. The Bill was formally published in October 2022 by the Office of Science and Technology Policy (OSTP), a U.S. government office that advises the President on science and technology policy matters. 2023-2024 In July 2023, the Biden–Harris Administration secured voluntary commitments from seven companies – Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI – to manage the risks associated with AI. The companies committed to ensure AI products undergo both internal and external security testing before public release; to share information on the management of AI risks with the industry, governments, civil society, and academia; to prioritize cybersecurity and protect proprietary AI system components; to develop mechanisms to inform users when content is AI-generated, such as watermarking; to publicly report on their AI systems' capabilities, limitations, and areas of use; to prioritize research on societal risks posed by AI, including bias, discrimination, and privacy concerns; and to develop AI systems to address societal challenges, ranging from cancer prevention to climate change mitigation. In September 2023, eight additional companies – Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI – subscribed to these voluntary commitments. In January 2023, the National Institute of Standards and Technology (NIST) released the Artificial Intelligence Risk Management Framework (AI RMF 1.0), providing voluntary guidance for organizations to identify, assess, and manage risks associated with AI systems. The Biden administration, in October 2023 signaled that they would release an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies. On October 30, 2023, President Biden released this Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The Executive Order addresses a variety of issues, such as focusing on standards for critical infrastructure, AI-enhanced cybersecurity, and federally funded biological synthesis projects. The Executive Order provides the authority to various agencies and departments of the US government, including the Energy and Defense departments, to apply existing consumer protection laws to AI development. The Executive Order builds on the Administration’s earlier agreements with AI companies to instate new initiatives to "red-team" or stress-test AI dual-use foundation models, especially those that have the potential to pose security risks, with data and results shared with the federal government. The Executive Order also recognizes AI's social challenges, and calls for companies building AI dual-use foundation models to be wary of these societal problems. For example, the Executive Order states that AI should not “worsen job quality”, and should not “cause labor-force disruptions”. Additionally, Biden’s Executive Order mandates that AI must “advance equity and civil rights”, and cannot disadvantage marginalized groups. It also called for foundation models to include "watermarks" to help the public discern between human and AI-generated content, which has raised controversy and criticism from deepfake detection researchers. 2025 In January 2025, President Trump repealed the Biden executive order. This action reflects President Trump's preference for deregulating AI in support of innovation over safeguarding risks. In early 2025, Congress began advanced bipartisan legislation targeting AI-generated deepfakes, including the "TAKE IT DOWN Act," which would prohibit nonconsensual disclosure of AI-generated "intimate imagery", requiring all platforms to remove such content. Additionally, lawmakers also reintroduced the CREATE AI Act to codify the National AI Research R