n. Others Instead of initializing all weights with random values on the order of O ( 1 / n ) {\displaystyle O(1/{\sqrt {n}})} , sparse initialization initialized only a small subset of the weights with larger random values, and the other weights zero, so that the total variance is still on the order of O ( 1 ) {\displaystyle O(1)} . Random walk initialization was designed for MLP so that during backpropagation, the L2 norm of gradient at each layer performs an unbiased random walk as one moves from the last layer to the first. Looks linear initialization was designed to allow the neural network to behave like a deep linear network at initialization, since W R e L U ( x ) − W R e L U ( − x ) = W x {\displaystyle W\;\mathrm {ReLU} (x)-W\;\mathrm {ReLU} (-x)=Wx} . It initializes a matrix W {\displaystyle W} of shape R n 2 × m {\displaystyle \mathbb {R} ^{{\frac {n}{2}}\times m}} by any method, such as orthogonal initialization, then let the R n × m {\displaystyle \mathbb {R} ^{n\times m}} weight matrix to be the concatenation of W , − W {\displaystyle W,-W} . Miscellaneous For hyperbolic tangent activation function, a particular scaling is sometimes used: 1.7159 tanh ⁡ ( 2 x / 3 ) {\displaystyle 1.7159\tanh(2x/3)} . This was sometimes called "LeCun's tanh". It was designed so that it maps the interval [ − 1 , + 1 ] {\displaystyle [-1,+1]} to itself, thus ensuring that the overall gain is around 1 in "normal operating conditions", and that | f ″ ( x ) | {\displaystyle |f''(x)|} is at maximum when x = − 1 , + 1 {\displaystyle x=-1,+1} , which improves convergence at the end of training. In self-normalizing neural networks, the SELU activation function S E L U ( x ) = λ { x if x > 0 α e x − α if x ≤ 0 {\displaystyle \mathrm {SELU} (x)=\lambda {\begin{cases}x&{\text{if }}x>0\\\alpha e^{x}-\alpha &{\text{if }}x\leq 0\end{cases}}} with parameters λ ≈ 1.0507 , α ≈ 1.6733 {\displaystyle \lambda \approx 1.0507,\alpha \approx 1.6733} makes it such that the mean and variance of the output of each layer has ( 0 , 1 ) {\displaystyle (0,1)} as an attracting fixed-point. This makes initialization less important, though they recommend initializing weights randomly with variance 1 / n l − 1 {\displaystyle 1/n_{l-1}} . History Random weight initialization was used since Frank Rosenblatt's perceptrons. An early work that described weight initialization specifically was (LeCun et al., 1998). Before the 2010s era of deep learning, it was common to initialize models by "generative pre-training" using an unsupervised learning algorithm that is not backpropagation, as it was difficult to directly train deep neural networks by backpropagation. For example, a deep belief network was trained by using contrastive divergence layer by layer, starting from the bottom. (Martens, 2010) proposed Hessian-free Optimization, a quasi-Newton method to directly train deep networks. The work generated considerable excitement that initializing networks without pre-training phase was possible. However, a 2013 paper demonstrated that with well-chosen hyperparameters, momentum gradient descent with weight initialization was sufficient for training neural networks, without needing either quasi-Newton method or generative pre-training, a combination that is still in use as of 2024. Since then, the impact of initialization on tuning the variance has become less important, with methods developed to automatically tune variance, like batch normalization tuning the variance of the forward pass, and momentum-based optimizers tuning the variance of the backward pass. There is a tension between using careful weight initialization to decrease the need for normalization, and using normalization to decrease the need for careful weight initialization, with each approach having its tradeoffs. For example, batch normalization causes training examples in the minibatch to become dependent, an undesirable trait, while weight initialization is architecture-dependent. See also Backpropagation N