 the mini-batches to ensure efficient and reliable training. However, in the inference stage, this dependence is not useful any more. Instead, the normalization step in this stage is computed with the population statistics such that the output could depend on the input in a deterministic manner. The population mean, E [ x ( k ) ] {\displaystyle E[x^{(k)}]} , and variance, Var ⁡ [ x ( k ) ] {\displaystyle \operatorname {Var} [x^{(k)}]} , are computed as: E [ x ( k ) ] = E B [ μ B ( k ) ] {\displaystyle E[x^{(k)}]=E_{B}[\mu _{B}^{(k)}]} , and Var ⁡ [ x ( k ) ] = m m − 1 E B [ ( σ B ( k ) ) 2 ] {\displaystyle \operatorname {Var} [x^{(k)}]={\frac {m}{m-1}}E_{B}[\left(\sigma _{B}^{(k)}\right)^{2}]} . The population statistics thus is a complete representation of the mini-batches. The BN transform in the inference step thus becomes y ( k ) = B N γ ( k ) , β ( k ) inf ( x ( k ) ) = γ ( k ) x ( k ) − E [ x ( k ) ] Var ⁡ [ x ( k ) ] + ϵ + β ( k ) {\displaystyle y^{(k)}=BN_{\gamma ^{(k)},\beta ^{(k)}}^{\text{inf}}(x^{(k)})=\gamma ^{(k)}{\frac {x^{(k)}-E[x^{(k)}]}{\sqrt {\operatorname {Var} [x^{(k)}]+\epsilon }}}+\beta ^{(k)}} , where y ( k ) {\displaystyle y^{(k)}} is passed on to next layers instead of x ( k ) {\displaystyle x^{(k)}} . Since the parameters are fixed in this transformation, the batch normalization procedure is essentially applying a linear transform to the activation function. Theory Although batch normalization has become popular due to its strong empirical performance, the working mechanism of the method is not yet well-understood. The explanation made in the original paper was that batch norm works by reducing internal covariate shift, but this has been challenged by more recent work. One experiment trained a VGG-16 network under 3 different training regimes: standard (no batch norm), batch norm, and batch norm with noise added to each layer during training. In the third model, the noise has non-zero mean and non-unit variance, i.e. it explicitly introduces covariate shift. Despite this, it showed similar accuracy to the second model, and both performed better than the first, suggesting that covariate shift is not the reason that batch norm improves performance. Using batch normalization causes the items in a batch to no longer be iid, which can lead to difficulties in training due to lower quality gradient estimation. Smoothness One alternative explanation is that the improvement with batch normalization is instead due to producing a smoother parameter space and smoother gradients, as formalized by a smaller Lipschitz constant. Consider two identical networks, one contains batch normalization layers and the other does not, the behaviors of these two networks are then compared. Denote the loss functions as L ^ {\displaystyle {\hat {L}}} and L {\displaystyle L} , respectively. Let the input to both networks be x {\displaystyle x} , and the output be y {\displaystyle y} , for which y = W x {\displaystyle y=Wx} , where W {\displaystyle W} is the layer weights. For the second network, y {\displaystyle y} additionally goes through a batch normalization layer. Denote the normalized activation as y ^ {\displaystyle {\hat {y}}} , which has zero mean and unit variance. Let the transformed activation be z = γ y ^ + β {\displaystyle z=\gamma {\hat {y}}+\beta } , and suppose γ {\displaystyle \gamma } and β {\displaystyle \beta } are constants. Finally, denote the standard deviation over a mini-batch y j ^ ∈ R m {\displaystyle {\hat {y_{j}}}\in \mathbb {R} ^{m}} as σ j {\displaystyle \sigma _{j}} . First, it can be shown that the gradient magnitude of a batch normalized network, | | ▽ y i L ^ | | {\displaystyle ||\triangledown _{y_{i}}{\hat {L}}||} , is bounded, with the bound expressed as | | ▽ y i L ^ | | 2 ≤ γ 2 σ j 2 ( | | ▽ y i L | | 2 − 1 m ⟨ 1 , ▽ y i L ⟩ 2 − 1 m ⟨ ▽ y i L , y ^ j ⟩ 2 ) {\displaystyle ||\triangledown _{y_{i}}{\hat {L}}||^{2}\leq {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}{\Bigg (}||\triangledown _{y_{i}}L||^{2}